{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Kubernetes Fundamentals by School of Devops\n\n\nThis is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.com\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy\n\n\nVenkat", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-kubernetes-fundamentals-by-school-of-devops", 
            "text": "This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.com .", 
            "title": "Welcome to Kubernetes Fundamentals by School of Devops"
        }, 
        {
            "location": "/#team", 
            "text": "Gourav Shah  Vijayboopathy  Venkat", 
            "title": "Team"
        }, 
        {
            "location": "/2_kube-cluster-vagrant/", 
            "text": "Install VirtualBox and Vagrant\n\n\n\n\n\n\n\n\nTOOL\n\n\nVERSION\n\n\nLINK\n\n\n\n\n\n\n\n\n\n\nVirtualBox\n\n\n5.1.26\n\n\nhttps://www.virtualbox.org/wiki/Downloads\n\n\n\n\n\n\nVagrant\n\n\n1.9.7\n\n\nhttps://www.vagrantup.com/downloads.html\n\n\n\n\n\n\n\n\nImporting a VM Template\n\n\nIf you have already copied/downloaded the box file \nubuntu-xenial64.box\n, go to the directory which contains that file. If you do not have a box file, skip to next section.\n\n\nvagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list\n\n\n\n\n\nProvisioning Vagrant Nodes\n\n\nClone repo if not already\n\n\ngit clone https://github.com/schoolofdevops/lab-setup.git\n\n\n\n\n\n\nLaunch environments with Vagrant\n\n\ncd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up\n\n\n\n\n\nLogin to nodes\n\n\nOpen three different terminals to login to 3 nodes created with above command\n\n\nTerminal 1\n\n\nvagrant ssh kube-01\nsudo su\n\n\n\n\n\nTerminal 2\n\n\nvagrant ssh kube-02\nsudo su\n\n\n\n\nTerminal 3\n\n\nvagrant ssh kube-03\nsudo su\n\n\n\n\nOnce the environment is setup, follow \nInitialization of Master\n onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning VMs with Vagrant"
        }, 
        {
            "location": "/2_kube-cluster-vagrant/#install-virtualbox-and-vagrant", 
            "text": "TOOL  VERSION  LINK      VirtualBox  5.1.26  https://www.virtualbox.org/wiki/Downloads    Vagrant  1.9.7  https://www.vagrantup.com/downloads.html", 
            "title": "Install VirtualBox and Vagrant"
        }, 
        {
            "location": "/2_kube-cluster-vagrant/#importing-a-vm-template", 
            "text": "If you have already copied/downloaded the box file  ubuntu-xenial64.box , go to the directory which contains that file. If you do not have a box file, skip to next section.  vagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list", 
            "title": "Importing a VM Template"
        }, 
        {
            "location": "/2_kube-cluster-vagrant/#provisioning-vagrant-nodes", 
            "text": "Clone repo if not already  git clone https://github.com/schoolofdevops/lab-setup.git  Launch environments with Vagrant  cd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up  Login to nodes  Open three different terminals to login to 3 nodes created with above command  Terminal 1  vagrant ssh kube-01\nsudo su  Terminal 2  vagrant ssh kube-02\nsudo su  Terminal 3  vagrant ssh kube-03\nsudo su  Once the environment is setup, follow  Initialization of Master  onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning Vagrant Nodes"
        }, 
        {
            "location": "/3_install_kubernetes/", 
            "text": "Compatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicable for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup\n\n\n Skip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described here,\n\n\nCreate Kubernetes Repository\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat \nEOF \n /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01\n\n\nkubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\nCopy and paste it on all node.\n\n\nTroubleshooting Tips\n\n\nIf you lose  the join token, you could retrieve it using\n\n\nkubeadm token list\n\n\n\n\nOn successfully joining the master, you should see output similar to following,\n\n\nroot@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \n159.203.170.84:6443\n\n[discovery] Created cluster-info discovery client, requesting info from \nhttps://159.203.170.84:6443\n\n[discovery] Requesting info from \nhttps://159.203.170.84:6443\n again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \n159.203.170.84:6443\n\n[discovery] Successfully established connection with API Server \n159.203.170.84:6443\n\n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n\n\n\n\nSetup the admin client - Kubectl\n\n\nOn Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nInstalling CNI with Weave\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nWeave Net\n.\n\n\n\nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f \nhttps://cloud.weave.works/k8s/net?k8s-version=$kubever\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     \nnone\n    4m        v1.8.2\nkube-03   Ready     \nnone\n    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nPossible Issues\n\n\n\n\nNodes are node in Ready status\n\n\nkube-dns is crashing constantly\n\n\nSome of the systems services are not up\n\n\n\n\nMost of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,\n\n\nTroubleshooting Tips\n\n\nCheck events\n\n\nkubectl get events\n\n\n\n\nCheck Logs\n\n\nkubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs \npod\n -n kube-system\n\n\n\n\n\ne.g.\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....\n\n\n\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nTo access the Dashboard in th browser, run the below command\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\n\n\n\n\nSample output:\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None\n\n\n\n\nNow check for the node port, here it is 32756, and go to the browser,\n\n\nmasterip:32756\n\n\n\n\nThe Dashboard Looks like:\n\n\n\n\nCheck out the supporting code\n\n\nBefore we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.\n\n\ngit clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Setup Kubernetes Cluster"
        }, 
        {
            "location": "/3_install_kubernetes/#compatibility", 
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicable for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04  Xenial", 
            "title": "Compatibility"
        }, 
        {
            "location": "/3_install_kubernetes/#base-setup", 
            "text": "Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described here,", 
            "title": "Base Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#create-kubernetes-repository", 
            "text": "We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat  EOF   /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF", 
            "title": "Create Kubernetes Repository"
        }, 
        {
            "location": "/3_install_kubernetes/#installation-of-the-packages", 
            "text": "We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*", 
            "title": "Installation of the packages"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-sysctl-configs", 
            "text": "In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.", 
            "title": "Setup sysctl configs"
        }, 
        {
            "location": "/3_install_kubernetes/#initializing-master", 
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01  kubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16", 
            "title": "Initializing Master"
        }, 
        {
            "location": "/3_install_kubernetes/#initialization-of-the-nodes-previously-minions", 
            "text": "After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  Copy and paste it on all node.", 
            "title": "Initialization of the Nodes (Previously Minions)"
        }, 
        {
            "location": "/3_install_kubernetes/#troubleshooting-tips", 
            "text": "If you lose  the join token, you could retrieve it using  kubeadm token list  On successfully joining the master, you should see output similar to following,  root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server  159.203.170.84:6443 \n[discovery] Created cluster-info discovery client, requesting info from  https://159.203.170.84:6443 \n[discovery] Requesting info from  https://159.203.170.84:6443  again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server  159.203.170.84:6443 \n[discovery] Successfully established connection with API Server  159.203.170.84:6443 \n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-the-admin-client-kubectl", 
            "text": "On Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Setup the admin client - Kubectl"
        }, 
        {
            "location": "/3_install_kubernetes/#installing-cni-with-weave", 
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Weave Net .  \nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f  https://cloud.weave.works/k8s/net?k8s-version=$kubever", 
            "title": "Installing CNI with Weave"
        }, 
        {
            "location": "/3_install_kubernetes/#validating-the-setup", 
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready      none     4m        v1.8.2\nkube-03   Ready      none     4m        v1.8.2  Additional Status Commands  kubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.", 
            "title": "Validating the Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#possible-issues", 
            "text": "Nodes are node in Ready status  kube-dns is crashing constantly  Some of the systems services are not up   Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,  Troubleshooting Tips  Check events  kubectl get events  Check Logs  kubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs  pod  -n kube-system  e.g.  root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....", 
            "title": "Possible Issues"
        }, 
        {
            "location": "/3_install_kubernetes/#enable-kubernetes-dashboard", 
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  To access the Dashboard in th browser, run the below command  kubectl describe svc kubernetes-dashboard -n kube-system  Sample output:  kubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                    unset  80/TCP\nNodePort:                unset  32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None  Now check for the node port, here it is 32756, and go to the browser,  masterip:32756  The Dashboard Looks like:", 
            "title": "Enable Kubernetes Dashboard"
        }, 
        {
            "location": "/3_install_kubernetes/#check-out-the-supporting-code", 
            "text": "Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.  git clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Check out the supporting code"
        }, 
        {
            "location": "/4_configs/", 
            "text": "In this lesson we are going to cover the following topics\n\n\n\n\nSetting up monitors\n\n\nConfigs\n\n\nContext\n\n\nNamespaces\n\n\n\n\nSetup monitoring console for Kubernetes\n\n\nUnix \nscreen\n is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a \nscreenrc\n script as follows on kube-01 node (node where \nkubectl\n is configured)\n\n\nfile: k8s-code/monitoring/deploy.screenrc\n\n\nscreen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get deploy\nsplit\nfocus down\nscreen watch -n 1 kubectl get services\nsplit\nfocus down\nscreen watch -n 1 kubectl get replicasets\nfocus bottom\n\n\n\n\nOpen a dedicated terminal to run this utility.  Launch it using\n\n\nscreen -c monitoring/deploy.screenrc\n\n\n\n\n\nWorking with Unix Screen\n\n\nTo detach\n\n\n^a d  \n\n\n\n\nTo list existing screen sessions\n\n\nscreen -ls\n\n\n\n\nTo re attach\n\n\nscreen  -x \nsession_id\n\n\n\n\n\nTo delete a session\n\n\nscreen -X -S \nsession_id\n quit\n\n\n\n\ne.g.\n\n\nscreen -ls\nThere are screens on:\n    26484.pts-2.kube-01 (01/12/2018 06:47:41 AM)    (Detached)\n    18472.pts-2.kube-01 (01/12/2018 06:43:21 AM)    (Detached)\n2 Sockets in /var/run/screen/S-root.\n\nscreen -X -S 26 quit\n\n\n\n\nListing Configurations\n\n\nCheck current config\n\n\nkubectl config view\n\n\n\n\nYou could also examine the current configs in file \ncat ~/.kube/config\n\n\nCreating a namespace for instavote project\n\n\nNamespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.   \n\n\nLets create a namespace called \ninstavote\n  \n\n\nfile: k8s-code/projects/instavote/instavote-ns.yaml\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n\n\n\n\nTo create namespace\n\n\ncd k8s-code/projects/instavote\nkubectl get ns\nkubectl apply -f dev_instavote.yaml\nkubectl get ns\n\n\n\n\nAnd switch to it\n\n\nkubectl config set-context $(kubectl config current-context) --namespace=instavote", 
            "title": "Configuring Cluster"
        }, 
        {
            "location": "/4_configs/#setup-monitoring-console-for-kubernetes", 
            "text": "Unix  screen  is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a  screenrc  script as follows on kube-01 node (node where  kubectl  is configured)  file: k8s-code/monitoring/deploy.screenrc  screen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get deploy\nsplit\nfocus down\nscreen watch -n 1 kubectl get services\nsplit\nfocus down\nscreen watch -n 1 kubectl get replicasets\nfocus bottom  Open a dedicated terminal to run this utility.  Launch it using  screen -c monitoring/deploy.screenrc", 
            "title": "Setup monitoring console for Kubernetes"
        }, 
        {
            "location": "/4_configs/#working-with-unix-screen", 
            "text": "To detach  ^a d    To list existing screen sessions  screen -ls  To re attach  screen  -x  session_id   To delete a session  screen -X -S  session_id  quit  e.g.  screen -ls\nThere are screens on:\n    26484.pts-2.kube-01 (01/12/2018 06:47:41 AM)    (Detached)\n    18472.pts-2.kube-01 (01/12/2018 06:43:21 AM)    (Detached)\n2 Sockets in /var/run/screen/S-root.\n\nscreen -X -S 26 quit", 
            "title": "Working with Unix Screen"
        }, 
        {
            "location": "/4_configs/#listing-configurations", 
            "text": "Check current config  kubectl config view  You could also examine the current configs in file  cat ~/.kube/config", 
            "title": "Listing Configurations"
        }, 
        {
            "location": "/4_configs/#creating-a-namespace-for-instavote-project", 
            "text": "Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.     Lets create a namespace called  instavote     file: k8s-code/projects/instavote/instavote-ns.yaml  kind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote  To create namespace  cd k8s-code/projects/instavote\nkubectl get ns\nkubectl apply -f dev_instavote.yaml\nkubectl get ns  And switch to it  kubectl config set-context $(kubectl config current-context) --namespace=instavote", 
            "title": "Creating a namespace for instavote project"
        }, 
        {
            "location": "/5_deploying_pods/", 
            "text": "Deploying Pods\n\n\nLife of a pod\n\n\n\n\nPending : in progress\n\n\nRunning\n\n\nSucceeded : successfully exited\n\n\nFailed\n\n\nUnknown\n\n\n\n\nProbes\n\n\n\n\nlivenessProbe : Containers are Alive\n\n\nreadinessProbe : Ready to Serve Traffic\n\n\n\n\nResource Configs\n\n\nEach entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.\n\n\nAKMS\n =\n Resource Configs Specs\n\n\napiVersion: v1\nkind:\nmetadata:\nspec:\n\n\n\n\nSpec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/\n\n\nTo list supported version of apis\n\n\nkubectl api-versions\n\n\n\n\nCommon Configurations\n\n\nThroughout this tutorial, we would be deploying different components of  example voting application. Lets assume we are deploying it in a \ndev\n environment. Lets create the common specs for this app with the AKMS schema discussed above.\n\n\nfile: common.yml\n\n\napiVersion: v1\nkind:\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n\n\n\n\nLets now create the  Pod config by adding the kind and specs to above schema.\n\n\nFilename: k8s-code/pods/vote-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n\n\n\n\nUse this link to refer to pod spec\n\n\nLaunching and operating a Pod\n\n\nSyntax:\n\n\n kubectl apply -f FILE\n\n\n\n\nTo Launch pod using configs above,\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\n\nTo view pods\n\n\nkubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote\n\n\n\n\nTo get detailed info\n\n\nkubectl describe pods vote\n\n\n\n\n[Output:]\n\n\nName:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:    \nnone\n\nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:      \nnone\n\nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:    \nnone\n\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image \nschoolofdevops/vote:latest\n\n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image \nschoolofdevops/vote:latest\n\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945\n\n\n\n\nCommands to operate the pod\n\n\nkubectl exec -it vote ps sh\n\nkubectl exec -it vote  sh\n\nkubectl logs vote\n\n\n\n\n\nTroubleshooting Tip\n\n\nIf you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.\n\n\nkubectl get pod vote -o yaml\n\n\n\n\nLets learn by example. Update pod spec and change the image to something that does not exist.\n\n\nkubectl edit pod vote\n\n\n\n\nThis will open a editor. Go to the line which defines image  and change it to a tag that does not exist\n\n\ne.g.\n\n\nspec:\n  containers:\n  - image: schoolofdevops/vote:latst\n    imagePullPolicy: Always\n\n\n\n\nwhere tag \nlatst\n does not exist. As soon as you save this file, kubernetes will apply the change.\n\n\nNow check the status,\n\n\nkubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nvote      0/1       ImagePullBackOff   0          27m\n\n\n\n\nThe above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.\n\n\nObserve the \nstatus\n field.  \n\n\nkubectl get pod vote -o yaml\n\n\n\n\nNow the status field shows a detailed information, including what the exact error. Observe the following snippet...\n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nThis will help you to pinpoint to the exact cause and fix it quickly.\n\n\nNow that you  are done experimenting with pod, delete it with the following command,\n\n\nkubectl delete pod vote\n\nkubectl get pods\n\n\n\n\nAttach a Volume to the Pod\n\n\nLets create a pod for database and attach a volume to it. To achieve this we will need to\n\n\n\n\ncreate a \nvolumes\n definition\n\n\nattach volume to container using \nVolumeMounts\n property\n\n\n\n\nLocal host volumes are of two types:\n\n  * emptyDir\n  * hostPath\n\n\nWe will pick hostPath. \nRefer to this doc to read more about hostPath.\n\n\nFile: db-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /pgdata\n    type: DirectoryOrCreate\n\n\n\n\nTo create this pod,\n\n\nkubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events\n\n\n\n\nSelecting Node to run on\n\n\nkubectl get nodes --show-labels\n\nkubectl label nodes \nnode-name\n rack=1\n\nkubectl get nodes --show-labels\n\n\n\n\n\nUpdate pod definition with nodeSelector\n\n\nfile: vote-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    rack: '1'\n\n\n\n\nFor this change, pod needs to be re created.\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\nCreating Multi Container Pods\n\n\nfile: multi_container_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: schoolofdevops/nginx-synch\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n    - name: sync\n      image: schoolofdevops/synch\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n  volumes:\n  - name: data\n    emptyDir: {}\n\n\n\n\nTo create this pod\n\n\nkubectl apply -f multi_container_pod.yml\n\n\n\n\nCheck Status\n\n\nroot@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m\n\n\n\n\nChecking logs, logging in\n\n\nkubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync\n\n\n\n\n\nExercise\n\n\nCreate a pod definition for redis and deploy.\n\n\nReading List :\n\n\nManaging Volumes with Kubernetes\n\n\nNode Selectors, Affinity", 
            "title": "Launching Pods"
        }, 
        {
            "location": "/5_deploying_pods/#deploying-pods", 
            "text": "Life of a pod   Pending : in progress  Running  Succeeded : successfully exited  Failed  Unknown", 
            "title": "Deploying Pods"
        }, 
        {
            "location": "/5_deploying_pods/#probes", 
            "text": "livenessProbe : Containers are Alive  readinessProbe : Ready to Serve Traffic", 
            "title": "Probes"
        }, 
        {
            "location": "/5_deploying_pods/#resource-configs", 
            "text": "Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.  AKMS  =  Resource Configs Specs  apiVersion: v1\nkind:\nmetadata:\nspec:  Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/  To list supported version of apis  kubectl api-versions", 
            "title": "Resource Configs"
        }, 
        {
            "location": "/5_deploying_pods/#common-configurations", 
            "text": "Throughout this tutorial, we would be deploying different components of  example voting application. Lets assume we are deploying it in a  dev  environment. Lets create the common specs for this app with the AKMS schema discussed above.  file: common.yml  apiVersion: v1\nkind:\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:  Lets now create the  Pod config by adding the kind and specs to above schema.  Filename: k8s-code/pods/vote-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest  Use this link to refer to pod spec", 
            "title": "Common Configurations"
        }, 
        {
            "location": "/5_deploying_pods/#launching-and-operating-a-pod", 
            "text": "Syntax:   kubectl apply -f FILE  To Launch pod using configs above,  kubectl apply -f vote-pod.yaml  To view pods  kubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote  To get detailed info  kubectl describe pods vote  [Output:]  Name:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:     none \nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:       none \nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:     none \nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image  schoolofdevops/vote:latest \n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image  schoolofdevops/vote:latest \n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945  Commands to operate the pod  kubectl exec -it vote ps sh\n\nkubectl exec -it vote  sh\n\nkubectl logs vote", 
            "title": "Launching and operating a Pod"
        }, 
        {
            "location": "/5_deploying_pods/#troubleshooting-tip", 
            "text": "If you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.  kubectl get pod vote -o yaml  Lets learn by example. Update pod spec and change the image to something that does not exist.  kubectl edit pod vote  This will open a editor. Go to the line which defines image  and change it to a tag that does not exist  e.g.  spec:\n  containers:\n  - image: schoolofdevops/vote:latst\n    imagePullPolicy: Always  where tag  latst  does not exist. As soon as you save this file, kubernetes will apply the change.  Now check the status,  kubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nvote      0/1       ImagePullBackOff   0          27m  The above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.  Observe the  status  field.    kubectl get pod vote -o yaml  Now the status field shows a detailed information, including what the exact error. Observe the following snippet...  status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248  This will help you to pinpoint to the exact cause and fix it quickly.  Now that you  are done experimenting with pod, delete it with the following command,  kubectl delete pod vote\n\nkubectl get pods", 
            "title": "Troubleshooting Tip"
        }, 
        {
            "location": "/5_deploying_pods/#attach-a-volume-to-the-pod", 
            "text": "Lets create a pod for database and attach a volume to it. To achieve this we will need to   create a  volumes  definition  attach volume to container using  VolumeMounts  property   Local host volumes are of two types: \n  * emptyDir\n  * hostPath  We will pick hostPath.  Refer to this doc to read more about hostPath.  File: db-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /pgdata\n    type: DirectoryOrCreate  To create this pod,  kubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events", 
            "title": "Attach a Volume to the Pod"
        }, 
        {
            "location": "/5_deploying_pods/#selecting-node-to-run-on", 
            "text": "kubectl get nodes --show-labels\n\nkubectl label nodes  node-name  rack=1\n\nkubectl get nodes --show-labels  Update pod definition with nodeSelector  file: vote-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    rack: '1'  For this change, pod needs to be re created.  kubectl apply -f vote-pod.yaml", 
            "title": "Selecting Node to run on"
        }, 
        {
            "location": "/5_deploying_pods/#creating-multi-container-pods", 
            "text": "file: multi_container_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: schoolofdevops/nginx-synch\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n    - name: sync\n      image: schoolofdevops/synch\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n  volumes:\n  - name: data\n    emptyDir: {}  To create this pod  kubectl apply -f multi_container_pod.yml  Check Status  root@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m  Checking logs, logging in  kubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync", 
            "title": "Creating Multi Container Pods"
        }, 
        {
            "location": "/5_deploying_pods/#exercise", 
            "text": "Create a pod definition for redis and deploy.", 
            "title": "Exercise"
        }, 
        {
            "location": "/5_deploying_pods/#reading-list", 
            "text": "Managing Volumes with Kubernetes  Node Selectors, Affinity", 
            "title": "Reading List :"
        }, 
        {
            "location": "/6_kubernetes_deployment/", 
            "text": "Creating a Deployment\n\n\nA Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.\n\n\nDeployment has mainly two responsibilities,\n  * Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.\n  * Update Strategy: Define a release strategy and update the pods accordingly.\n\n\nTopics\n*  \n\n  * Rollout a Replicaset\n\n  * Deploy a new version : Creates a new replica set every time, moves pods from RS(n) to RS(n+1)\n\n  * Rollback to previous RS    \n\n  * Auto Scaling\n\n  * Pause Deployments  \n\n\nFile: vote-deploy.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: instavote\nspec:\n  replicas: 8\n  selector:\n    matchLabels:\n      tier: front\n      app: vote\n  revisionHistoryLimit: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  minReadySeconds: 20\n  paused: false\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n\n\nDeployment spec (deployment.spec) contains the following,\n\n\n\n\nreplicaset specs\n\n\nselectors  \n\n\nreplicas  \n\n\n\n\n\n\ndeployment spec\n\n\nstrategy\n\n\nrollingUpdate\n\n\nminReadySeconds\n\n\n\n\n\n\npod template\n\n\nmetadata, labels\n\n\ncontainer specs\n\n\n\n\n\n\n\n\nLets  create the Deployment\n\n\nkubectl apply -f vote_deploy.yaml --record\n\n\n\n\nNow that the deployment is created. To validate,\n\n\nkubectl get deployment\nkubectl get rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels\n\n\n\n\nSample Output\n\n\nkubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m\n\n\n\n\nScaling a deployment\n\n\nTo scale a deployment in Kubernetes:\n\n\nkubectl scale deployment/vote --replicas=5\n\n\n\n\nSample output:\n\n\nkubectl scale deployment/vote --replicas=5\ndeployment \nvote\n scaled", 
            "title": "Creating Deployments"
        }, 
        {
            "location": "/6_kubernetes_deployment/#creating-a-deployment", 
            "text": "A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.  Deployment has mainly two responsibilities,\n  * Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.\n  * Update Strategy: Define a release strategy and update the pods accordingly.  Topics *   \n  * Rollout a Replicaset \n  * Deploy a new version : Creates a new replica set every time, moves pods from RS(n) to RS(n+1) \n  * Rollback to previous RS     \n  * Auto Scaling \n  * Pause Deployments    File: vote-deploy.yaml  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: instavote\nspec:\n  replicas: 8\n  selector:\n    matchLabels:\n      tier: front\n      app: vote\n  revisionHistoryLimit: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  minReadySeconds: 20\n  paused: false\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP  Deployment spec (deployment.spec) contains the following,   replicaset specs  selectors    replicas      deployment spec  strategy  rollingUpdate  minReadySeconds    pod template  metadata, labels  container specs     Lets  create the Deployment  kubectl apply -f vote_deploy.yaml --record  Now that the deployment is created. To validate,  kubectl get deployment\nkubectl get rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels  Sample Output  kubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m", 
            "title": "Creating a Deployment"
        }, 
        {
            "location": "/6_kubernetes_deployment/#scaling-a-deployment", 
            "text": "To scale a deployment in Kubernetes:  kubectl scale deployment/vote --replicas=5  Sample output:  kubectl scale deployment/vote --replicas=5\ndeployment  vote  scaled", 
            "title": "Scaling a deployment"
        }, 
        {
            "location": "/7_exposing_app_with_service/", 
            "text": "Exposing Application with  a Service\n\n\nTypes of Services:\n\n  * ClusterIP\n  * NodePort\n  * LoadBalancer\n  * ExternalName\n\n\n\n\nkubectl get pods\nkubectl get svc\n\n\n\n\nSample Output:\n\n\nNAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m\n\n\n\n\nFilename: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: vote-svc\n  namespace: instavote\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n\n\n\n\nSave the file.\n\n\nNow to create a service:\n\n\nkubectl create -f vote_svc.yaml\nkubectl get svc\n\n\n\n\nNow to check which port the pod is connected\n\n\nkubectl describe service vote-svc\n\n\n\n\nCheck for the Nodeport here\n\n\nSample Output\n\n\nName:                   vote-svc\nNamespace:              instavote\nLabels:                 app=vote\nSelector:               app=vote\nType:                   NodePort\nIP:                     10.99.147.158\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 30308/TCP\nEndpoints:              10.40.0.2:80,10.40.0.3:80,10.40.0.4:80 + 1 more...\nSession Affinity:       None\nNo events.\n\n\n\n\nGo to browser and check hostip:NodePort\n\n\nHere the node port is 30308.\n\n\nSample output will be:", 
            "title": "Service Endpoints"
        }, 
        {
            "location": "/7_exposing_app_with_service/#exposing-application-with-a-service", 
            "text": "Types of Services: \n  * ClusterIP\n  * NodePort\n  * LoadBalancer\n  * ExternalName   kubectl get pods\nkubectl get svc  Sample Output:  NAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m  Filename: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: vote-svc\n  namespace: instavote\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort  Save the file.  Now to create a service:  kubectl create -f vote_svc.yaml\nkubectl get svc  Now to check which port the pod is connected  kubectl describe service vote-svc  Check for the Nodeport here  Sample Output  Name:                   vote-svc\nNamespace:              instavote\nLabels:                 app=vote\nSelector:               app=vote\nType:                   NodePort\nIP:                     10.99.147.158\nPort:                    unset  80/TCP\nNodePort:                unset  30308/TCP\nEndpoints:              10.40.0.2:80,10.40.0.3:80,10.40.0.4:80 + 1 more...\nSession Affinity:       None\nNo events.  Go to browser and check hostip:NodePort  Here the node port is 30308.  Sample output will be:", 
            "title": "Exposing Application with  a Service"
        }, 
        {
            "location": "/8_rollouts-and-rollbacks/", 
            "text": "Rolling updates with deployments\n\n\nUpdate the version of the image in vote_deploy.yaml\n\n\nFile: vote_deploy.yaml\n\n\n...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movies\n\n\n\n\n\nApply Changes and monitor the rollout\n\n\nkubectl apply -f vote-deploy.yaml\nkubectl rollout status deployment/vote\n\n\n\n\nRolling Back a Failed Update\n\n\nLets update the image to a tag which is non existent. We intentionally introduce this intentional error to fail fail the deployment.\n\n\nFile: vote_deploy.yaml\n\n\n...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movi\n\n\n\n\n\nDo a new rollout and monitor\n\n\nkubectl apply -f vote_deploy.yaml\nkubectl rollout status deployment/vote\n\n\n\n\nAlso watch the pod status which might look like\n\n\nvote-3040199436-sdq17   1/1       Running            0          9m\nvote-4086029260-0vjjb   0/1       ErrImagePull       0          16s\nvote-4086029260-zvgmd   0/1       ImagePullBackOff   0          15s\nvote-rc-fsdsd               1/1       Running            0          27m\nvote-rc-mcxs5               1/1       Running            0\n\n\n\n\nTo get the revision history and details  \n\n\nkubectl rollout history deployment/vote\nkubectl rollout history deployment/vote --revision=x\n[replace x with the latest revision]\n\n\n\n\n[Sample Output]\n\n\nroot@kube-01:~# kubectl rollout history deployment/vote\ndeployments \nvote\n\nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/vote --replicas=5\n3       \nnone\n\n6       \nnone\n\n7       \nnone\n\n\nroot@kube-01:~# kubectl rollout history deployment/vote --revision=7\ndeployments \nvote\n with revision #7\nPod Template:\n  Labels:   app=vote\n    env=dev\n    pod-template-hash=4086029260\n    role=ui\n    stack=voting\n    tier=front\n  Containers:\n   vote:\n    Image:  schoolofdevops/vote:movi\n    Port:   80/TCP\n    Environment:    \nnone\n\n    Mounts: \nnone\n\n  Volumes:  \nnone\n\n\n\n\n\nTo undo rollout,\n\n\nkubectl rollout undo deployment/vote\n\n\n\n\nor\n\n\nkubectl rollout undo deployment/vote --to-revision=1\nkubectl get rs\nkubectl describe deployment vote", 
            "title": "Rollouts and Rollbacks"
        }, 
        {
            "location": "/8_rollouts-and-rollbacks/#rolling-updates-with-deployments", 
            "text": "Update the version of the image in vote_deploy.yaml  File: vote_deploy.yaml  ...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movies  Apply Changes and monitor the rollout  kubectl apply -f vote-deploy.yaml\nkubectl rollout status deployment/vote", 
            "title": "Rolling updates with deployments"
        }, 
        {
            "location": "/8_rollouts-and-rollbacks/#rolling-back-a-failed-update", 
            "text": "Lets update the image to a tag which is non existent. We intentionally introduce this intentional error to fail fail the deployment.  File: vote_deploy.yaml  ...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movi  Do a new rollout and monitor  kubectl apply -f vote_deploy.yaml\nkubectl rollout status deployment/vote  Also watch the pod status which might look like  vote-3040199436-sdq17   1/1       Running            0          9m\nvote-4086029260-0vjjb   0/1       ErrImagePull       0          16s\nvote-4086029260-zvgmd   0/1       ImagePullBackOff   0          15s\nvote-rc-fsdsd               1/1       Running            0          27m\nvote-rc-mcxs5               1/1       Running            0  To get the revision history and details    kubectl rollout history deployment/vote\nkubectl rollout history deployment/vote --revision=x\n[replace x with the latest revision]  [Sample Output]  root@kube-01:~# kubectl rollout history deployment/vote\ndeployments  vote \nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/vote --replicas=5\n3        none \n6        none \n7        none \n\nroot@kube-01:~# kubectl rollout history deployment/vote --revision=7\ndeployments  vote  with revision #7\nPod Template:\n  Labels:   app=vote\n    env=dev\n    pod-template-hash=4086029260\n    role=ui\n    stack=voting\n    tier=front\n  Containers:\n   vote:\n    Image:  schoolofdevops/vote:movi\n    Port:   80/TCP\n    Environment:     none \n    Mounts:  none \n  Volumes:   none   To undo rollout,  kubectl rollout undo deployment/vote  or  kubectl rollout undo deployment/vote --to-revision=1\nkubectl get rs\nkubectl describe deployment vote", 
            "title": "Rolling Back a Failed Update"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/", 
            "text": "Configmap is one of the ways to provide configurations to your application.\n\n\nInjecting env variables with configmaps\n\n\nCreate our configmap for vote app\n\n\nfile:  projects/instavote/dev/vote-cm.yaml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: EMACS\n  OPTION_B: VI\n\n\n\n\nIn the above given configmap, we define two environment variables,\n  1. OPTION_A=EMACS\n  2. OPTION_B=VI\n\n\nIn order to use this configmap in the deployment, we need to reference it from the deployment file.\nCheck the deployment file for vote add for the following block.\n\n\nfile: \nvote-deploy.yaml\n\n\n...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      restartPolicy: Always\n\n\n\n\nSo when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.\n\n\n\n\nConfigmap as a configuration file\n\n\nIn the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.\n\n\nSyntax for consuming file as a configmap is as follows\n\n\n  kubectl create configmap --from-file \nCONF-FILE-PATH\n \nNAME-OF-CONFIGMAP\n\n\n\n\n\nWe have redis configuration as a file named \nprojects/instavote/config/redis.conf\n. We are going to convert this file into a configmap\n\n\nkubectl create configmap --from-file projects/instavote/config/redis.conf redis\n\n\n\n\nUpdate your redis-deploy.yaml file to use this confimap.\nFile: \nredis-deploy.yaml\n\n\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always\n\n\n\n\nSecrets\n\n\nSecrets are for storing sensitive data like \npasswords and keychains\n. We will see how db deployment uses username and password in form of a secret.\n\n\nYou would define two fields for db,\n  * username\n  * password\n\n\nTo create secrets for db you need to generate  \nbase64\n format as follows,\n\n\necho \nadmin\n | base64\necho \npassword\n | base64\n\n\n\n\nwhere \nadmin\n and \npassword\n are the actual values that you would want to inject into the pod environment.\n\n\nIf you do not have a unix host, you can make use of online base64 utility to generate these strings.\n\n\nhttp://www.utilities-online.info/base64\n\n\n\n\nLets now add it to the secrets file,\n\n\nFile: projects/instavote/dev/db-secrets.yaml\n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  # base64 of admin\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n  # base64 of password\n\n\n\n\nTo consume these secrets, update the deployment as\n\n\nfile: db-deploy.yaml.\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n# Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always", 
            "title": "Using Configmaps and Secrets"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#injecting-env-variables-with-configmaps", 
            "text": "Create our configmap for vote app  file:  projects/instavote/dev/vote-cm.yaml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: EMACS\n  OPTION_B: VI  In the above given configmap, we define two environment variables,\n  1. OPTION_A=EMACS\n  2. OPTION_B=VI  In order to use this configmap in the deployment, we need to reference it from the deployment file.\nCheck the deployment file for vote add for the following block.  file:  vote-deploy.yaml  ...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      restartPolicy: Always  So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.", 
            "title": "Injecting env variables with configmaps"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#configmap-as-a-configuration-file", 
            "text": "In the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.  Syntax for consuming file as a configmap is as follows    kubectl create configmap --from-file  CONF-FILE-PATH   NAME-OF-CONFIGMAP   We have redis configuration as a file named  projects/instavote/config/redis.conf . We are going to convert this file into a configmap  kubectl create configmap --from-file projects/instavote/config/redis.conf redis  Update your redis-deploy.yaml file to use this confimap.\nFile:  redis-deploy.yaml      spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always", 
            "title": "Configmap as a configuration file"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#secrets", 
            "text": "Secrets are for storing sensitive data like  passwords and keychains . We will see how db deployment uses username and password in form of a secret.  You would define two fields for db,\n  * username\n  * password  To create secrets for db you need to generate   base64  format as follows,  echo  admin  | base64\necho  password  | base64  where  admin  and  password  are the actual values that you would want to inject into the pod environment.  If you do not have a unix host, you can make use of online base64 utility to generate these strings.  http://www.utilities-online.info/base64  Lets now add it to the secrets file,  File: projects/instavote/dev/db-secrets.yaml  apiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  # base64 of admin\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n  # base64 of password  To consume these secrets, update the deployment as  file: db-deploy.yaml.  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n# Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always", 
            "title": "Secrets"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/", 
            "text": "Kubernetes Horizonntal Pod Autoscaling\n\n\nWith Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).\n\n\nThe Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user\n\n\nPrerequisites\n\n\nHeapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.\n\n\nDeploying Heapster\n\n\nGo to the below directory and create the deployment and services.\n\n\ngit clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml\n\n\n\n\nValidate that heapster, influxdb and grafana are started\n\n\nkubectl get pods -n kube-system\nkubectl get svc -n kube-system\n\n\n\n\n\nNow this will deploy the heapster monitoring.\n\n\nRun \n expose php-apache server\n\n\nTo demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80  \n\n\n\n\nSample Output\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice \nphp-apache\n created\ndeployment \nphp-apache\n created\n\n\n\n\nTo verify the created pod:\n\n\nkubectl get pods\n\n\n\n\nWait untill the pod changes to running state.\n\n\nCreate Horizontal Pod Autoscaler\n\n\nNow that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n\n\n\n\nSample Output\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment \nphp-apache\n autoscaled\n\n\n\n\nWe may check the current status of autoscaler by running:\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s\n\n\n\n\nIncrease load\n\n\nNow we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service\n\n\nkubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done\n\n\n\n\n\nNow open a new window of the same machine.\n\n\nAnd check the status of the hpa\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m\n\n\n\n\nNow if you check the pods it will be automatically scaled to the desired value.\n\n\nkubectl get pods\n\n\n\n\nSample Output\n\n\nkubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h\n\n\n\n\nStop load\n\n\nIn the terminal where we created the container with busybox image, terminate the load generation by typing \n + C\n\n\nThen we will verify the result state (after a minute or so)\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Auto Scaling Capacity with HPA"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#kubernetes-horizonntal-pod-autoscaling", 
            "text": "With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).  The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user", 
            "title": "Kubernetes Horizonntal Pod Autoscaling"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#prerequisites", 
            "text": "Heapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#deploying-heapster", 
            "text": "Go to the below directory and create the deployment and services.  git clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml  Validate that heapster, influxdb and grafana are started  kubectl get pods -n kube-system\nkubectl get svc -n kube-system  Now this will deploy the heapster monitoring.", 
            "title": "Deploying Heapster"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#run-expose-php-apache-server", 
            "text": "To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80    Sample Output  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice  php-apache  created\ndeployment  php-apache  created  To verify the created pod:  kubectl get pods  Wait untill the pod changes to running state.", 
            "title": "Run &amp; expose php-apache server"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#create-horizontal-pod-autoscaler", 
            "text": "Now that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  Sample Output  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment  php-apache  autoscaled  We may check the current status of autoscaler by running:  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s", 
            "title": "Create Horizontal Pod Autoscaler"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#increase-load", 
            "text": "Now we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service  kubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done  Now open a new window of the same machine.  And check the status of the hpa  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m  Now if you check the pods it will be automatically scaled to the desired value.  kubectl get pods  Sample Output  kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h", 
            "title": "Increase load"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#stop-load", 
            "text": "In the terminal where we created the container with busybox image, terminate the load generation by typing   + C  Then we will verify the result state (after a minute or so)  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Stop load"
        }, 
        {
            "location": "/12_troubleshooting/", 
            "text": "Troubleshooting the Kubernetes cluster\n\n\nIn this chapter we will learn about how to trouble shoot our Kubernetes cluster at \ncontrol plane\n level and at \napplication level\n.\n\n\nTroubleshooting the control plane\n\n\nListing the nodes in a cluster\n\n\nFirst thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.\n\n\nkubectl get nodes\n\n\n\n\nMake sure that all nodes are in \nReady\n state.\n\n\nList the control plane pods\n\n\nIf your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,\n\n\nkubectl get pods -n kube-system\n\n\n\n\nIf any of the pod is \nrestarting or crashing\n, look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster \nkube-dns\n is crashing. In order to fix this first check the deployment for errors.\n\n\nkubectl describe deployment -n kube-system kube-dns\n\n\n\n\nLog files\n\n\nMaster\n\n\nIf your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...\n\n\n/var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs\n\n\n\n\nIf your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the \nactual pod's name may differ\n from cluster to cluster...\n\n\nkubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1\n\n\n\n\nWorker Nodes\n\n\nIn your worker, you will need to check for errors in kubelet's log...\n\n\nsudo journalctl -u  kubelet\n\n\n\n\nTroubleshooting the application\n\n\nSometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.\n\n\nGetting detailed status of an object (pods, deployments)\n\n\nobject.status\n shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.\n\n\nExample\n\n\nkubectl get pod vote -o yaml\n\n\n\n\n\nexample output snippet when a wrong image was used to create a pod. \n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nChecking the status of Deployment\n\n\nFor this example I have a sample deployment called nginx.\n\n\nFILE: nginx-deployment.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80\n\n\n\n\nList the deployment to check for the \navailability of pods\n\n\nkubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h\n\n\n\n\nIt is clear that my pod is unavailable. Lets dig further.\n\n\nCheck the \nevents\n of your deployment.\n\n\nkubectl describe deployment nginx\n\n\n\n\nList the pods to check for any \nregistry related error\n\n\nkubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m\n\n\n\n\nAs we can see, we are not able to pull the image(\nImagePullBackOff\n). Let's investigate further.\n\n\nkubectl describe pod nginx-57c88d7bb8-c6kpc\n\n\n\n\nCheck the events of the pod's description.\n\n\nEvents:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \ndefault-token-8cwn4\n\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \nngnix\n\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \nngnix\n: rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \nngnix\n\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod\n\n\n\n\nBingo! The name of the image is \nngnix\n instead of \nnginx\n. So \nfix the typo in your deployment file and redo the deployment\n.\n\n\nSometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.\n\n\nkubectl logs -f nginx-57c88d7bb8-c6kpc\n\n\n\n\nIf you have any errors it will get populated in your logs.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-kubernetes-cluster", 
            "text": "In this chapter we will learn about how to trouble shoot our Kubernetes cluster at  control plane  level and at  application level .", 
            "title": "Troubleshooting the Kubernetes cluster"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-control-plane", 
            "text": "", 
            "title": "Troubleshooting the control plane"
        }, 
        {
            "location": "/12_troubleshooting/#listing-the-nodes-in-a-cluster", 
            "text": "First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.  kubectl get nodes  Make sure that all nodes are in  Ready  state.", 
            "title": "Listing the nodes in a cluster"
        }, 
        {
            "location": "/12_troubleshooting/#list-the-control-plane-pods", 
            "text": "If your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,  kubectl get pods -n kube-system  If any of the pod is  restarting or crashing , look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster  kube-dns  is crashing. In order to fix this first check the deployment for errors.  kubectl describe deployment -n kube-system kube-dns", 
            "title": "List the control plane pods"
        }, 
        {
            "location": "/12_troubleshooting/#log-files", 
            "text": "", 
            "title": "Log files"
        }, 
        {
            "location": "/12_troubleshooting/#master", 
            "text": "If your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...  /var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs  If your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the  actual pod's name may differ  from cluster to cluster...  kubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1", 
            "title": "Master"
        }, 
        {
            "location": "/12_troubleshooting/#worker-nodes", 
            "text": "In your worker, you will need to check for errors in kubelet's log...  sudo journalctl -u  kubelet", 
            "title": "Worker Nodes"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-application", 
            "text": "Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.", 
            "title": "Troubleshooting the application"
        }, 
        {
            "location": "/12_troubleshooting/#getting-detailed-status-of-an-object-pods-deployments", 
            "text": "object.status  shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.  Example  kubectl get pod vote -o yaml  example output snippet when a wrong image was used to create a pod.   status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248", 
            "title": "Getting detailed status of an object (pods, deployments)"
        }, 
        {
            "location": "/12_troubleshooting/#checking-the-status-of-deployment", 
            "text": "For this example I have a sample deployment called nginx.  FILE: nginx-deployment.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80  List the deployment to check for the  availability of pods  kubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h  It is clear that my pod is unavailable. Lets dig further.  Check the  events  of your deployment.  kubectl describe deployment nginx  List the pods to check for any  registry related error  kubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m  As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further.  kubectl describe pod nginx-57c88d7bb8-c6kpc  Check the events of the pod's description.  Events:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume  default-token-8cwn4 \n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image  ngnix \n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image  ngnix : rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image  ngnix \n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod  Bingo! The name of the image is  ngnix  instead of  nginx . So  fix the typo in your deployment file and redo the deployment .  Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.  kubectl logs -f nginx-57c88d7bb8-c6kpc  If you have any errors it will get populated in your logs.", 
            "title": "Checking the status of Deployment"
        }, 
        {
            "location": "/11_deploying_sample_app/", 
            "text": "Mini Project: Deploying Multi Tier Application Stack\n\n\nIn this project , you would write definitions for deploying the vote application stack with all components/tiers which include,\n\n\n\n\nvote ui\n\n\nredis\n\n\nworker\n\n\ndb\n\n\nresults ui\n\n\n\n\nTasks\n\n\n\n\nCreate deployments for all applications\n\n\nDefine services for each tier\n\n\nLaunch/appy the definitions\n\n\n\n\nFollowing table depicts the state of readiness of the above services.\n\n\n\n\n\n\n\n\nApp\n\n\nDeployment\n\n\nService\n\n\n\n\n\n\n\n\n\n\nvote\n\n\nready\n\n\nready\n\n\n\n\n\n\nredis\n\n\nin progress\n\n\nready\n\n\n\n\n\n\nworker\n\n\nin progress\n\n\nin progress\n\n\n\n\n\n\ndb\n\n\nin progress\n\n\ntodo\n\n\n\n\n\n\nresults\n\n\ntodo\n\n\ntodo\n\n\n\n\n\n\n\n\nDeploying the sample application\n\n\nTo create deploy the sample applications,\n\n\nkubectl create -f projects/instavote/dev\n\n\n\n\nSample output is like:\n\n\ndeployment \ndb\n created\nservice \ndb\n created\ndeployment \nredis\n created\nservice \nredis\n created\ndeployment \nvote\n created\nservice \nvote\n created\ndeployment \nworker\n created\ndeployment \nresults\n created\nservice \nresults\n created\n\n\n\n\nTo Validate:\n\n\nkubectl get svc -n instavote\n\n\n\n\nSample Output is:\n\n\nkubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   \npending\n     80:31808/TCP   1h\n\n\n\n\nHere the port assigned is 31808, go to the browser and enter\n\n\nmasterip:31808\n\n\n\n\n\n\nThis will load the page where you can vote.\n\n\nTo check the result:\n\n\nkubectl get service result\n\n\n\n\nSample Output is:\n\n\nkubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   \npending\n     80:32511/TCP   1h\n\n\n\n\nHere the port assigned is 32511, go to the browser and enter\n\n\nmasterip:32511\n\n\n\n\n\n\nThis is the page where we can see the results of the vote.", 
            "title": "Mini Project"
        }, 
        {
            "location": "/11_deploying_sample_app/#mini-project-deploying-multi-tier-application-stack", 
            "text": "In this project , you would write definitions for deploying the vote application stack with all components/tiers which include,   vote ui  redis  worker  db  results ui", 
            "title": "Mini Project: Deploying Multi Tier Application Stack"
        }, 
        {
            "location": "/11_deploying_sample_app/#tasks", 
            "text": "Create deployments for all applications  Define services for each tier  Launch/appy the definitions   Following table depicts the state of readiness of the above services.     App  Deployment  Service      vote  ready  ready    redis  in progress  ready    worker  in progress  in progress    db  in progress  todo    results  todo  todo", 
            "title": "Tasks"
        }, 
        {
            "location": "/11_deploying_sample_app/#deploying-the-sample-application", 
            "text": "To create deploy the sample applications,  kubectl create -f projects/instavote/dev  Sample output is like:  deployment  db  created\nservice  db  created\ndeployment  redis  created\nservice  redis  created\ndeployment  vote  created\nservice  vote  created\ndeployment  worker  created\ndeployment  results  created\nservice  results  created", 
            "title": "Deploying the sample application"
        }, 
        {
            "location": "/11_deploying_sample_app/#to-validate", 
            "text": "kubectl get svc -n instavote  Sample Output is:  kubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243    pending      80:31808/TCP   1h  Here the port assigned is 31808, go to the browser and enter  masterip:31808   This will load the page where you can vote.  To check the result:  kubectl get service result  Sample Output is:  kubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16    pending      80:32511/TCP   1h  Here the port assigned is 32511, go to the browser and enter  masterip:32511   This is the page where we can see the results of the vote.", 
            "title": "To Validate:"
        }, 
        {
            "location": "/12_troubleshooting/", 
            "text": "Troubleshooting the Kubernetes cluster\n\n\nIn this chapter we will learn about how to trouble shoot our Kubernetes cluster at \ncontrol plane\n level and at \napplication level\n.\n\n\nTroubleshooting the control plane\n\n\nListing the nodes in a cluster\n\n\nFirst thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.\n\n\nkubectl get nodes\n\n\n\n\nMake sure that all nodes are in \nReady\n state.\n\n\nList the control plane pods\n\n\nIf your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,\n\n\nkubectl get pods -n kube-system\n\n\n\n\nIf any of the pod is \nrestarting or crashing\n, look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster \nkube-dns\n is crashing. In order to fix this first check the deployment for errors.\n\n\nkubectl describe deployment -n kube-system kube-dns\n\n\n\n\nLog files\n\n\nMaster\n\n\nIf your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...\n\n\n/var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs\n\n\n\n\nIf your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the \nactual pod's name may differ\n from cluster to cluster...\n\n\nkubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1\n\n\n\n\nWorker Nodes\n\n\nIn your worker, you will need to check for errors in kubelet's log...\n\n\nsudo journalctl -u  kubelet\n\n\n\n\nTroubleshooting the application\n\n\nSometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.\n\n\nGetting detailed status of an object (pods, deployments)\n\n\nobject.status\n shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.\n\n\nExample\n\n\nkubectl get pod vote -o yaml\n\n\n\n\n\nexample output snippet when a wrong image was used to create a pod. \n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nChecking the status of Deployment\n\n\nFor this example I have a sample deployment called nginx.\n\n\nFILE: nginx-deployment.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80\n\n\n\n\nList the deployment to check for the \navailability of pods\n\n\nkubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h\n\n\n\n\nIt is clear that my pod is unavailable. Lets dig further.\n\n\nCheck the \nevents\n of your deployment.\n\n\nkubectl describe deployment nginx\n\n\n\n\nList the pods to check for any \nregistry related error\n\n\nkubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m\n\n\n\n\nAs we can see, we are not able to pull the image(\nImagePullBackOff\n). Let's investigate further.\n\n\nkubectl describe pod nginx-57c88d7bb8-c6kpc\n\n\n\n\nCheck the events of the pod's description.\n\n\nEvents:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \ndefault-token-8cwn4\n\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \nngnix\n\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \nngnix\n: rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \nngnix\n\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod\n\n\n\n\nBingo! The name of the image is \nngnix\n instead of \nnginx\n. So \nfix the typo in your deployment file and redo the deployment\n.\n\n\nSometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.\n\n\nkubectl logs -f nginx-57c88d7bb8-c6kpc\n\n\n\n\nIf you have any errors it will get populated in your logs.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-kubernetes-cluster", 
            "text": "In this chapter we will learn about how to trouble shoot our Kubernetes cluster at  control plane  level and at  application level .", 
            "title": "Troubleshooting the Kubernetes cluster"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-control-plane", 
            "text": "", 
            "title": "Troubleshooting the control plane"
        }, 
        {
            "location": "/12_troubleshooting/#listing-the-nodes-in-a-cluster", 
            "text": "First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.  kubectl get nodes  Make sure that all nodes are in  Ready  state.", 
            "title": "Listing the nodes in a cluster"
        }, 
        {
            "location": "/12_troubleshooting/#list-the-control-plane-pods", 
            "text": "If your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,  kubectl get pods -n kube-system  If any of the pod is  restarting or crashing , look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster  kube-dns  is crashing. In order to fix this first check the deployment for errors.  kubectl describe deployment -n kube-system kube-dns", 
            "title": "List the control plane pods"
        }, 
        {
            "location": "/12_troubleshooting/#log-files", 
            "text": "", 
            "title": "Log files"
        }, 
        {
            "location": "/12_troubleshooting/#master", 
            "text": "If your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...  /var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs  If your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the  actual pod's name may differ  from cluster to cluster...  kubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1", 
            "title": "Master"
        }, 
        {
            "location": "/12_troubleshooting/#worker-nodes", 
            "text": "In your worker, you will need to check for errors in kubelet's log...  sudo journalctl -u  kubelet", 
            "title": "Worker Nodes"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-application", 
            "text": "Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.", 
            "title": "Troubleshooting the application"
        }, 
        {
            "location": "/12_troubleshooting/#getting-detailed-status-of-an-object-pods-deployments", 
            "text": "object.status  shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.  Example  kubectl get pod vote -o yaml  example output snippet when a wrong image was used to create a pod.   status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248", 
            "title": "Getting detailed status of an object (pods, deployments)"
        }, 
        {
            "location": "/12_troubleshooting/#checking-the-status-of-deployment", 
            "text": "For this example I have a sample deployment called nginx.  FILE: nginx-deployment.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80  List the deployment to check for the  availability of pods  kubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h  It is clear that my pod is unavailable. Lets dig further.  Check the  events  of your deployment.  kubectl describe deployment nginx  List the pods to check for any  registry related error  kubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m  As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further.  kubectl describe pod nginx-57c88d7bb8-c6kpc  Check the events of the pod's description.  Events:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume  default-token-8cwn4 \n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image  ngnix \n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image  ngnix : rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image  ngnix \n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod  Bingo! The name of the image is  ngnix  instead of  nginx . So  fix the typo in your deployment file and redo the deployment .  Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.  kubectl logs -f nginx-57c88d7bb8-c6kpc  If you have any errors it will get populated in your logs.", 
            "title": "Checking the status of Deployment"
        }
    ]
}