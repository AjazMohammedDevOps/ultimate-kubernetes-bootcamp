{
    "docs": [
        {
            "location": "/", 
            "text": "Ultimate Kubernetes Bootcamp\n\n\nWelcome to Kubernetes Fundamentals by School of Devops\n\n\nThis is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.com\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy\n\n\nVenkat", 
            "title": "Home"
        }, 
        {
            "location": "/#ultimate-kubernetes-bootcamp", 
            "text": "Welcome to Kubernetes Fundamentals by School of Devops  This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.com .", 
            "title": "Ultimate Kubernetes Bootcamp"
        }, 
        {
            "location": "/#team", 
            "text": "Gourav Shah  Vijayboopathy  Venkat", 
            "title": "Team"
        }, 
        {
            "location": "/minikube/", 
            "text": "Single node k8s cluster with Minikube\n\n\nMinikube offers one of the easiest zero to dev experience\nto setup a single node kubernetes cluster. Its also the ideal way\nto create a local dev environment to test kubernetes code on.\n\n\nThis document explains how to setup and work with single node kubernetes cluster with minikube.\n\n\nInstall Minikube\n\n\nInstructions to install minikube may vary based on the operating system and choice of the hypervisor.\n\nThis is the official document\n which explains how to install minikube.\n\n\nStart all in one single node cluster with minikube\n\n\nminikube status\n\n\n\n\n[output]\n\n\nminikube:\ncluster:\nkubectl:\n\n\n\n\nminikube start\n\n\n\n\n[output]\n\n\nStarting local Kubernetes v1.8.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.\n\n\n\n\nminikube status\n\n\n\n\n\n[output]\n\n\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100\n\n\n\n\nLaunch a kubernetes dashboard\n\n\nminikube dashboard\n\n\n\n\nSetting up docker environment\n\n\nminikube docker-env\nexport DOCKER_TLS_VERIFY=\n1\n\nexport DOCKER_HOST=\ntcp://192.168.99.100:2376\n\nexport DOCKER_CERT_PATH=\n/Users/gouravshah/.minikube/certs\n\nexport DOCKER_API_VERSION=\n1.23\n\n# Run this command to configure your shell:\n# eval $(minikube docker-env)\n\n\n\n\nRun the command given above,\ne.g.\n\n\neval $(minikube docker-env)\n\n\n\n\nNow your docker client should be able to connect with the minikube cluster\n\n\ndocker ps\n\n\n\n\nAdditional Commands\n\n\nminikube ip\nminikube get-k8s-versions\nminikube logs", 
            "title": "Option 1 - Minikube"
        }, 
        {
            "location": "/minikube/#single-node-k8s-cluster-with-minikube", 
            "text": "Minikube offers one of the easiest zero to dev experience\nto setup a single node kubernetes cluster. Its also the ideal way\nto create a local dev environment to test kubernetes code on.  This document explains how to setup and work with single node kubernetes cluster with minikube.", 
            "title": "Single node k8s cluster with Minikube"
        }, 
        {
            "location": "/minikube/#install-minikube", 
            "text": "Instructions to install minikube may vary based on the operating system and choice of the hypervisor. This is the official document  which explains how to install minikube.", 
            "title": "Install Minikube"
        }, 
        {
            "location": "/minikube/#start-all-in-one-single-node-cluster-with-minikube", 
            "text": "minikube status  [output]  minikube:\ncluster:\nkubectl:  minikube start  [output]  Starting local Kubernetes v1.8.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.  minikube status  [output]  minikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100", 
            "title": "Start all in one single node cluster with minikube"
        }, 
        {
            "location": "/minikube/#launch-a-kubernetes-dashboard", 
            "text": "minikube dashboard", 
            "title": "Launch a kubernetes dashboard"
        }, 
        {
            "location": "/minikube/#setting-up-docker-environment", 
            "text": "minikube docker-env\nexport DOCKER_TLS_VERIFY= 1 \nexport DOCKER_HOST= tcp://192.168.99.100:2376 \nexport DOCKER_CERT_PATH= /Users/gouravshah/.minikube/certs \nexport DOCKER_API_VERSION= 1.23 \n# Run this command to configure your shell:\n# eval $(minikube docker-env)  Run the command given above,\ne.g.  eval $(minikube docker-env)  Now your docker client should be able to connect with the minikube cluster  docker ps", 
            "title": "Setting up docker environment"
        }, 
        {
            "location": "/minikube/#additional-commands", 
            "text": "minikube ip\nminikube get-k8s-versions\nminikube logs", 
            "title": "Additional Commands"
        }, 
        {
            "location": "/minikube/", 
            "text": "Single node k8s cluster with Minikube\n\n\nMinikube offers one of the easiest zero to dev experience\nto setup a single node kubernetes cluster. Its also the ideal way\nto create a local dev environment to test kubernetes code on.\n\n\nThis document explains how to setup and work with single node kubernetes cluster with minikube.\n\n\nInstall Minikube\n\n\nInstructions to install minikube may vary based on the operating system and choice of the hypervisor.\n\nThis is the official document\n which explains how to install minikube.\n\n\nStart all in one single node cluster with minikube\n\n\nminikube status\n\n\n\n\n[output]\n\n\nminikube:\ncluster:\nkubectl:\n\n\n\n\nminikube start\n\n\n\n\n[output]\n\n\nStarting local Kubernetes v1.8.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.\n\n\n\n\nminikube status\n\n\n\n\n\n[output]\n\n\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100\n\n\n\n\nLaunch a kubernetes dashboard\n\n\nminikube dashboard\n\n\n\n\nSetting up docker environment\n\n\nminikube docker-env\nexport DOCKER_TLS_VERIFY=\n1\n\nexport DOCKER_HOST=\ntcp://192.168.99.100:2376\n\nexport DOCKER_CERT_PATH=\n/Users/gouravshah/.minikube/certs\n\nexport DOCKER_API_VERSION=\n1.23\n\n# Run this command to configure your shell:\n# eval $(minikube docker-env)\n\n\n\n\nRun the command given above,\ne.g.\n\n\neval $(minikube docker-env)\n\n\n\n\nNow your docker client should be able to connect with the minikube cluster\n\n\ndocker ps\n\n\n\n\nAdditional Commands\n\n\nminikube ip\nminikube get-k8s-versions\nminikube logs", 
            "title": "Option 2 - Docker for Mac/Windows"
        }, 
        {
            "location": "/minikube/#single-node-k8s-cluster-with-minikube", 
            "text": "Minikube offers one of the easiest zero to dev experience\nto setup a single node kubernetes cluster. Its also the ideal way\nto create a local dev environment to test kubernetes code on.  This document explains how to setup and work with single node kubernetes cluster with minikube.", 
            "title": "Single node k8s cluster with Minikube"
        }, 
        {
            "location": "/minikube/#install-minikube", 
            "text": "Instructions to install minikube may vary based on the operating system and choice of the hypervisor. This is the official document  which explains how to install minikube.", 
            "title": "Install Minikube"
        }, 
        {
            "location": "/minikube/#start-all-in-one-single-node-cluster-with-minikube", 
            "text": "minikube status  [output]  minikube:\ncluster:\nkubectl:  minikube start  [output]  Starting local Kubernetes v1.8.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.  minikube status  [output]  minikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100", 
            "title": "Start all in one single node cluster with minikube"
        }, 
        {
            "location": "/minikube/#launch-a-kubernetes-dashboard", 
            "text": "minikube dashboard", 
            "title": "Launch a kubernetes dashboard"
        }, 
        {
            "location": "/minikube/#setting-up-docker-environment", 
            "text": "minikube docker-env\nexport DOCKER_TLS_VERIFY= 1 \nexport DOCKER_HOST= tcp://192.168.99.100:2376 \nexport DOCKER_CERT_PATH= /Users/gouravshah/.minikube/certs \nexport DOCKER_API_VERSION= 1.23 \n# Run this command to configure your shell:\n# eval $(minikube docker-env)  Run the command given above,\ne.g.  eval $(minikube docker-env)  Now your docker client should be able to connect with the minikube cluster  docker ps", 
            "title": "Setting up docker environment"
        }, 
        {
            "location": "/minikube/#additional-commands", 
            "text": "minikube ip\nminikube get-k8s-versions\nminikube logs", 
            "title": "Additional Commands"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/", 
            "text": "Install VirtualBox and Vagrant\n\n\n\n\n\n\n\n\nTOOL\n\n\nVERSION\n\n\nLINK\n\n\n\n\n\n\n\n\n\n\nVirtualBox\n\n\n5.1.26\n\n\nhttps://www.virtualbox.org/wiki/Downloads\n\n\n\n\n\n\nVagrant\n\n\n1.9.7\n\n\nhttps://www.vagrantup.com/downloads.html\n\n\n\n\n\n\n\n\nImporting a VM Template\n\n\nIf you have already copied/downloaded the box file \nubuntu-xenial64.box\n, go to the directory which contains that file. If you do not have a box file, skip to next section.\n\n\nvagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list\n\n\n\n\n\nProvisioning Vagrant Nodes\n\n\nClone repo if not already\n\n\ngit clone https://github.com/schoolofdevops/lab-setup.git\n\n\n\n\n\n\nLaunch environments with Vagrant\n\n\ncd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up\n\n\n\n\n\nLogin to nodes\n\n\nOpen three different terminals to login to 3 nodes created with above command\n\n\nTerminal 1\n\n\nvagrant ssh kube-01\nsudo su\n\n\n\n\n\nTerminal 2\n\n\nvagrant ssh kube-02\nsudo su\n\n\n\n\nTerminal 3\n\n\nvagrant ssh kube-03\nsudo su\n\n\n\n\nOnce the environment is setup, follow \nInitialization of Master\n onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning Nodes with Vagrant"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#install-virtualbox-and-vagrant", 
            "text": "TOOL  VERSION  LINK      VirtualBox  5.1.26  https://www.virtualbox.org/wiki/Downloads    Vagrant  1.9.7  https://www.vagrantup.com/downloads.html", 
            "title": "Install VirtualBox and Vagrant"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#importing-a-vm-template", 
            "text": "If you have already copied/downloaded the box file  ubuntu-xenial64.box , go to the directory which contains that file. If you do not have a box file, skip to next section.  vagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list", 
            "title": "Importing a VM Template"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#provisioning-vagrant-nodes", 
            "text": "Clone repo if not already  git clone https://github.com/schoolofdevops/lab-setup.git  Launch environments with Vagrant  cd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up  Login to nodes  Open three different terminals to login to 3 nodes created with above command  Terminal 1  vagrant ssh kube-01\nsudo su  Terminal 2  vagrant ssh kube-02\nsudo su  Terminal 3  vagrant ssh kube-03\nsudo su  Once the environment is setup, follow  Initialization of Master  onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning Vagrant Nodes"
        }, 
        {
            "location": "/3_install_kubernetes/", 
            "text": "Kubeadm : Bring Your Own Nodes (BYON)\n\n\nThis documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses \nkubeadm\n to install and configure kubernetes cluster.\n\n\nCompatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicable for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup\n\n\n Skip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described here,\n\n\nCreate Kubernetes Repository\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat \nEOF \n /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01\n\n\nkubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\nCopy and paste it on all node.\n\n\nTroubleshooting Tips\n\n\nIf you lose  the join token, you could retrieve it using\n\n\nkubeadm token list\n\n\n\n\nOn successfully joining the master, you should see output similar to following,\n\n\nroot@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \n159.203.170.84:6443\n\n[discovery] Created cluster-info discovery client, requesting info from \nhttps://159.203.170.84:6443\n\n[discovery] Requesting info from \nhttps://159.203.170.84:6443\n again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \n159.203.170.84:6443\n\n[discovery] Successfully established connection with API Server \n159.203.170.84:6443\n\n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n\n\n\n\nSetup the admin client - Kubectl\n\n\nOn Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nInstalling CNI with Weave\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nWeave Net\n.\n\n\n\nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f \nhttps://cloud.weave.works/k8s/net?k8s-version=$kubever\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\nkubectl cs\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     \nnone\n    4m        v1.8.2\nkube-03   Ready     \nnone\n    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nPossible Issues\n\n\n\n\nNodes are node in Ready status\n\n\nkube-dns is crashing constantly\n\n\nSome of the systems services are not up\n\n\n\n\nMost of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,\n\n\nTroubleshooting Tips\n\n\nCheck events\n\n\nkubectl get events\n\n\n\n\nCheck Logs\n\n\nkubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs \npod\n -n kube-system\n\n\n\n\n\ne.g.\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....\n\n\n\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nTo access the Dashboard in th browser, run the below command\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\n\n\n\n\nSample output:\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None\n\n\n\n\nNow check for the node port, here it is 32756, and go to the browser,\n\n\nmasterip:32756\n\n\n\n\nThe Dashboard Looks like:\n\n\n\n\nCheck out the supporting code\n\n\nBefore we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.\n\n\ngit clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Option 1 - Kubeadm - Bring Your Own Nodes (BYON)"
        }, 
        {
            "location": "/3_install_kubernetes/#kubeadm-bring-your-own-nodes-byon", 
            "text": "This documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses  kubeadm  to install and configure kubernetes cluster.", 
            "title": "Kubeadm : Bring Your Own Nodes (BYON)"
        }, 
        {
            "location": "/3_install_kubernetes/#compatibility", 
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicable for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04  Xenial", 
            "title": "Compatibility"
        }, 
        {
            "location": "/3_install_kubernetes/#base-setup", 
            "text": "Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described here,", 
            "title": "Base Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#create-kubernetes-repository", 
            "text": "We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat  EOF   /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF", 
            "title": "Create Kubernetes Repository"
        }, 
        {
            "location": "/3_install_kubernetes/#installation-of-the-packages", 
            "text": "We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*", 
            "title": "Installation of the packages"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-sysctl-configs", 
            "text": "In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.", 
            "title": "Setup sysctl configs"
        }, 
        {
            "location": "/3_install_kubernetes/#initializing-master", 
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01  kubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16", 
            "title": "Initializing Master"
        }, 
        {
            "location": "/3_install_kubernetes/#initialization-of-the-nodes-previously-minions", 
            "text": "After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  Copy and paste it on all node.", 
            "title": "Initialization of the Nodes (Previously Minions)"
        }, 
        {
            "location": "/3_install_kubernetes/#troubleshooting-tips", 
            "text": "If you lose  the join token, you could retrieve it using  kubeadm token list  On successfully joining the master, you should see output similar to following,  root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server  159.203.170.84:6443 \n[discovery] Created cluster-info discovery client, requesting info from  https://159.203.170.84:6443 \n[discovery] Requesting info from  https://159.203.170.84:6443  again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server  159.203.170.84:6443 \n[discovery] Successfully established connection with API Server  159.203.170.84:6443 \n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-the-admin-client-kubectl", 
            "text": "On Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Setup the admin client - Kubectl"
        }, 
        {
            "location": "/3_install_kubernetes/#installing-cni-with-weave", 
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Weave Net .  \nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f  https://cloud.weave.works/k8s/net?k8s-version=$kubever", 
            "title": "Installing CNI with Weave"
        }, 
        {
            "location": "/3_install_kubernetes/#validating-the-setup", 
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes\nkubectl cs  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready      none     4m        v1.8.2\nkube-03   Ready      none     4m        v1.8.2  Additional Status Commands  kubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.", 
            "title": "Validating the Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#possible-issues", 
            "text": "Nodes are node in Ready status  kube-dns is crashing constantly  Some of the systems services are not up   Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,  Troubleshooting Tips  Check events  kubectl get events  Check Logs  kubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs  pod  -n kube-system  e.g.  root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....", 
            "title": "Possible Issues"
        }, 
        {
            "location": "/3_install_kubernetes/#enable-kubernetes-dashboard", 
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  To access the Dashboard in th browser, run the below command  kubectl describe svc kubernetes-dashboard -n kube-system  Sample output:  kubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                    unset  80/TCP\nNodePort:                unset  32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None  Now check for the node port, here it is 32756, and go to the browser,  masterip:32756  The Dashboard Looks like:", 
            "title": "Enable Kubernetes Dashboard"
        }, 
        {
            "location": "/3_install_kubernetes/#check-out-the-supporting-code", 
            "text": "Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.  git clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Check out the supporting code"
        }, 
        {
            "location": "/3_install_kubernetes/", 
            "text": "Kubeadm : Bring Your Own Nodes (BYON)\n\n\nThis documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses \nkubeadm\n to install and configure kubernetes cluster.\n\n\nCompatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicable for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup\n\n\n Skip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described here,\n\n\nCreate Kubernetes Repository\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat \nEOF \n /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01\n\n\nkubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\nCopy and paste it on all node.\n\n\nTroubleshooting Tips\n\n\nIf you lose  the join token, you could retrieve it using\n\n\nkubeadm token list\n\n\n\n\nOn successfully joining the master, you should see output similar to following,\n\n\nroot@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \n159.203.170.84:6443\n\n[discovery] Created cluster-info discovery client, requesting info from \nhttps://159.203.170.84:6443\n\n[discovery] Requesting info from \nhttps://159.203.170.84:6443\n again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \n159.203.170.84:6443\n\n[discovery] Successfully established connection with API Server \n159.203.170.84:6443\n\n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n\n\n\n\nSetup the admin client - Kubectl\n\n\nOn Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nInstalling CNI with Weave\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nWeave Net\n.\n\n\n\nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f \nhttps://cloud.weave.works/k8s/net?k8s-version=$kubever\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\nkubectl cs\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     \nnone\n    4m        v1.8.2\nkube-03   Ready     \nnone\n    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nPossible Issues\n\n\n\n\nNodes are node in Ready status\n\n\nkube-dns is crashing constantly\n\n\nSome of the systems services are not up\n\n\n\n\nMost of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,\n\n\nTroubleshooting Tips\n\n\nCheck events\n\n\nkubectl get events\n\n\n\n\nCheck Logs\n\n\nkubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs \npod\n -n kube-system\n\n\n\n\n\ne.g.\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....\n\n\n\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nTo access the Dashboard in th browser, run the below command\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\n\n\n\n\nSample output:\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None\n\n\n\n\nNow check for the node port, here it is 32756, and go to the browser,\n\n\nmasterip:32756\n\n\n\n\nThe Dashboard Looks like:\n\n\n\n\nCheck out the supporting code\n\n\nBefore we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.\n\n\ngit clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Option 2 - Google Cloud Platform"
        }, 
        {
            "location": "/3_install_kubernetes/#kubeadm-bring-your-own-nodes-byon", 
            "text": "This documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses  kubeadm  to install and configure kubernetes cluster.", 
            "title": "Kubeadm : Bring Your Own Nodes (BYON)"
        }, 
        {
            "location": "/3_install_kubernetes/#compatibility", 
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicable for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04  Xenial", 
            "title": "Compatibility"
        }, 
        {
            "location": "/3_install_kubernetes/#base-setup", 
            "text": "Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described here,", 
            "title": "Base Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#create-kubernetes-repository", 
            "text": "We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat  EOF   /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF", 
            "title": "Create Kubernetes Repository"
        }, 
        {
            "location": "/3_install_kubernetes/#installation-of-the-packages", 
            "text": "We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*", 
            "title": "Installation of the packages"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-sysctl-configs", 
            "text": "In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.", 
            "title": "Setup sysctl configs"
        }, 
        {
            "location": "/3_install_kubernetes/#initializing-master", 
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01  kubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16", 
            "title": "Initializing Master"
        }, 
        {
            "location": "/3_install_kubernetes/#initialization-of-the-nodes-previously-minions", 
            "text": "After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  Copy and paste it on all node.", 
            "title": "Initialization of the Nodes (Previously Minions)"
        }, 
        {
            "location": "/3_install_kubernetes/#troubleshooting-tips", 
            "text": "If you lose  the join token, you could retrieve it using  kubeadm token list  On successfully joining the master, you should see output similar to following,  root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server  159.203.170.84:6443 \n[discovery] Created cluster-info discovery client, requesting info from  https://159.203.170.84:6443 \n[discovery] Requesting info from  https://159.203.170.84:6443  again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server  159.203.170.84:6443 \n[discovery] Successfully established connection with API Server  159.203.170.84:6443 \n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-the-admin-client-kubectl", 
            "text": "On Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Setup the admin client - Kubectl"
        }, 
        {
            "location": "/3_install_kubernetes/#installing-cni-with-weave", 
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Weave Net .  \nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f  https://cloud.weave.works/k8s/net?k8s-version=$kubever", 
            "title": "Installing CNI with Weave"
        }, 
        {
            "location": "/3_install_kubernetes/#validating-the-setup", 
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes\nkubectl cs  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready      none     4m        v1.8.2\nkube-03   Ready      none     4m        v1.8.2  Additional Status Commands  kubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.", 
            "title": "Validating the Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#possible-issues", 
            "text": "Nodes are node in Ready status  kube-dns is crashing constantly  Some of the systems services are not up   Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,  Troubleshooting Tips  Check events  kubectl get events  Check Logs  kubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs  pod  -n kube-system  e.g.  root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....", 
            "title": "Possible Issues"
        }, 
        {
            "location": "/3_install_kubernetes/#enable-kubernetes-dashboard", 
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  To access the Dashboard in th browser, run the below command  kubectl describe svc kubernetes-dashboard -n kube-system  Sample output:  kubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                    unset  80/TCP\nNodePort:                unset  32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None  Now check for the node port, here it is 32756, and go to the browser,  masterip:32756  The Dashboard Looks like:", 
            "title": "Enable Kubernetes Dashboard"
        }, 
        {
            "location": "/3_install_kubernetes/#check-out-the-supporting-code", 
            "text": "Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.  git clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Check out the supporting code"
        }, 
        {
            "location": "/4_configs/", 
            "text": "In this lesson we are going to cover the following topics\n\n\n\n\nSetting up monitors\n\n\nConfigs\n\n\nContext\n\n\nNamespaces\n\n\n\n\nSetup monitoring console for Kubernetes\n\n\nUnix \nscreen\n is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a \nscreenrc\n script as follows on kube-01 node (node where \nkubectl\n is configured)\n\n\nfile: k8s-code/monitoring/deploy.screenrc\n\n\nscreen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get deploy\nsplit\nfocus down\nscreen watch -n 1 kubectl get services\nsplit\nfocus down\nscreen watch -n 1 kubectl get replicasets\nfocus bottom\n\n\n\n\nOpen a dedicated terminal to run this utility.  Launch it using\n\n\nscreen -c monitoring/deploy.screenrc\n\n\n\n\n\nWorking with Unix Screen\n\n\nTo detach\n\n\n^a d  \n\n\n\n\nTo list existing screen sessions\n\n\nscreen -ls\n\n\n\n\nTo re attach\n\n\nscreen  -x \nsession_id\n\n\n\n\n\nTo delete a session\n\n\nscreen -X -S \nsession_id\n quit\n\n\n\n\ne.g.\n\n\nscreen -ls\nThere are screens on:\n    26484.pts-2.kube-01 (01/12/2018 06:47:41 AM)    (Detached)\n    18472.pts-2.kube-01 (01/12/2018 06:43:21 AM)    (Detached)\n2 Sockets in /var/run/screen/S-root.\n\nscreen -X -S 26 quit\n\n\n\n\nListing Configurations\n\n\nCheck current config\n\n\nkubectl config view\n\n\n\n\nYou could also examine the current configs in file \ncat ~/.kube/config\n\n\nCreating a namespace for instavote project\n\n\nNamespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.   \n\n\nLets create a namespace called \nmogambo\n  \n\n\nfile: k8s-code/projects/mogambo/mogambo-ns.yml\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: mogambo\n\n\n\n\nTo create namespace\n\n\ncd k8s-code/projects/mogamgo\nkubectl get ns\nkubectl apply -f mogamgo-ns.yml\nkubectl get ns\n\n\n\n\nAnd switch to it\n\n\nkubectl config set-context $(kubectl config current-context) --namespace=mogambo", 
            "title": "Configuring Cluster"
        }, 
        {
            "location": "/4_configs/#setup-monitoring-console-for-kubernetes", 
            "text": "Unix  screen  is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a  screenrc  script as follows on kube-01 node (node where  kubectl  is configured)  file: k8s-code/monitoring/deploy.screenrc  screen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get deploy\nsplit\nfocus down\nscreen watch -n 1 kubectl get services\nsplit\nfocus down\nscreen watch -n 1 kubectl get replicasets\nfocus bottom  Open a dedicated terminal to run this utility.  Launch it using  screen -c monitoring/deploy.screenrc", 
            "title": "Setup monitoring console for Kubernetes"
        }, 
        {
            "location": "/4_configs/#working-with-unix-screen", 
            "text": "To detach  ^a d    To list existing screen sessions  screen -ls  To re attach  screen  -x  session_id   To delete a session  screen -X -S  session_id  quit  e.g.  screen -ls\nThere are screens on:\n    26484.pts-2.kube-01 (01/12/2018 06:47:41 AM)    (Detached)\n    18472.pts-2.kube-01 (01/12/2018 06:43:21 AM)    (Detached)\n2 Sockets in /var/run/screen/S-root.\n\nscreen -X -S 26 quit", 
            "title": "Working with Unix Screen"
        }, 
        {
            "location": "/4_configs/#listing-configurations", 
            "text": "Check current config  kubectl config view  You could also examine the current configs in file  cat ~/.kube/config", 
            "title": "Listing Configurations"
        }, 
        {
            "location": "/4_configs/#creating-a-namespace-for-instavote-project", 
            "text": "Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.     Lets create a namespace called  mogambo     file: k8s-code/projects/mogambo/mogambo-ns.yml  kind: Namespace\napiVersion: v1\nmetadata:\n  name: mogambo  To create namespace  cd k8s-code/projects/mogamgo\nkubectl get ns\nkubectl apply -f mogamgo-ns.yml\nkubectl get ns  And switch to it  kubectl config set-context $(kubectl config current-context) --namespace=mogambo", 
            "title": "Creating a namespace for instavote project"
        }, 
        {
            "location": "/5_deploying_pods/", 
            "text": "Deploying Pods\n\n\nLife of a pod\n\n\n\n\nPending : in progress\n\n\nRunning\n\n\nSucceeded : successfully exited\n\n\nFailed\n\n\nUnknown\n\n\n\n\nProbes\n\n\n\n\nlivenessProbe : Containers are Alive\n\n\nreadinessProbe : Ready to Serve Traffic\n\n\n\n\nResource Configs\n\n\nEach entity created with kubernetes is a resource including pod, service, pods, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.\n\n\nAKMS\n =\n Resource Configs Specs\n\n\napiVersion: v1\nkind:\nmetadata:\nspec:\n\n\n\n\nSpec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/\n\n\nTo list supported version of apis\n\n\nkubectl api-versions\n\n\n\n\nCommon Configurations\n\n\nThroughout this tutorial, we would be deploying different components of  example voting application. Lets assume we are deploying it in a \ndev\n environment. Lets create the common specs for this app with the AKMS schema discussed above.\n\n\nfile: k8s-code/projects/mogambo/common.yml\n\n\napiVersion: v1\nkind:\nmetadata:\n  name: front-end\n  labels:\n    app: front-end\n    role: ui\n    tier: front\nspec:\n\n\n\n\nLets now create the  Pod config by adding the kind and specs to above schema.\n\n\nFilename: k8s-code/pods/frontend-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  labels:\n    app: front-end\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: front-end\n      image: schoolofdevops/frontend:latest\n\n\n\n\nUse this link to refer to pod spec\n\n\nLaunching and operating a Pod\n\n\nSyntax:\n\n\n kubectl apply -f FILE\n\n\n\n\nTo Launch pod using configs above,\n\n\nkubectl apply -f frontend-pod.yml\n\n\n\n\n\nTo view pods\n\n\nkubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods front-end\n\n\n\n\nTo get detailed info\n\n\nkubectl describe pods front-end\n\n\n\n\n[Output:]\n\n\nName:         front-end\nNamespace:    mogambo\nNode:         gke-test-cluster-default-pool-d29bc0e9-jz8w/10.128.0.2\nStart Time:   Thu, 22 Feb 2018 14:13:01 +0530\nLabels:       app=front-end\n              role=ui\n              tier=front\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nPod\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\napp\n:\nfront-end\n,\nrole\n:\nui\n,\ntier\n:\nfront\n},\nname\n:\nfront-end\n,\nnamespace\n:\nmo...\nStatus:       Running\nIP:           10.8.1.12\nContainers:\n  front-end:\n    Container ID:   docker://587d8f8e40a23548eb2e2a70c1a43d32dce9922155781d1f76fbdd64f920c545\n    Image:          schoolofdevops/frontend:latest\n    Image ID:       docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50\n    Port:           \nnone\n\n    State:          Running\n      Started:      Thu, 22 Feb 2018 14:13:12 +0530\n    Ready:          True\n    Restart Count:  0\n    Environment:    \nnone\n\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfsps (ro)\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          True\n  PodScheduled   True\nVolumes:\n  default-token-lfsps:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lfsps\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  \nnone\n\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason                 Age   From                                                  Message\n  ----    ------                 ----  ----                                                  -------\n  Normal  Scheduled              32s   default-scheduler                                     Successfully assigned front-end to gke-test-cluster-default-pool-d29bc0e9-jz8w\n  Normal  SuccessfulMountVolume  31s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  MountVolume.SetUp succeeded for volume \ndefault-token-lfsps\n\n  Normal  Pulling                30s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  pulling image \nschoolofdevops/frontend:latest\n\n  Normal  Pulled                 21s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  Successfully pulled image \nschoolofdevops/frontend:latest\n\n  Normal  Created                21s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  Created container\n  Normal  Started                21s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  Started container\n\n\n\n\nCommands to operate the pod\n\n\nkubectl exec -it front-end ps sh\n\nkubectl exec -it front-end sh\n\nkubectl logs front-end\n\n\n\n\n\nTroubleshooting Tip\n\n\nIf you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.\n\n\nkubectl get pod front-end -o yaml\n\n\n\n\nLets learn by example. Update pod spec and change the image to something that does not exist.\n\n\nkubectl edit pod front-end\n\n\n\n\nThis will open a editor. Go to the line which defines image  and change it to a tag that does not exist\n\n\ne.g.\n\n\nspec:\n  containers:\n  - image: schoolofdevops/frontend:latst\n    imagePullPolicy: Always\n\n\n\n\nwhere tag \nlatst\n does not exist. As soon as you save this file, kubernetes will apply the change.\n\n\nNow check the status,\n\n\nkubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nfront-end      0/1       ImagePullBackOff   0          27m\n\n\n\n\nThe above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.\n\n\nObserve the \nstatus\n field.  \n\n\nkubectl get pod front-end -o yaml\n\n\n\n\nNow the status field shows a detailed information, including what the exact error. Observe the following snippet...\n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/frontend:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nThis will help you to pinpoint to the exact cause and fix it quickly.\n\n\nNow that you  are done experimenting with pod, delete it with the following command,\n\n\nkubectl delete pod front-end\n\nkubectl get pods\n\n\n\n\nAttach a Volume to the Pod\n\n\nLets create a pod for database and attach a volume to it. To achieve this we will need to\n\n\n\n\ncreate a \nvolumes\n definition\n\n\nattach volume to container using \nVolumeMounts\n property\n\n\n\n\nLocal host volumes are of two types:\n\n\n\n\nemptyDir  \n\n\nhostPath  \n\n\n\n\nWe will pick hostPath. \nRefer to this doc to read more about hostPath.\n\n\nFile: /k8s-code/pods/db-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate\n\n\n\n\nTo create this pod,\n\n\nkubectl apply -f db-pod.yml\n\nkubectl describe pod db\n\nkubectl get events\n\n\n\n\nSelecting A Node to run on\n\n\nkubectl get nodes --show-labels\n\nkubectl label nodes \nnode-name\n zone=aaa\n\nkubectl get nodes --show-labels\n\n\n\n\n\nUpdate pod definition with nodeSelector\n\n\nfile: k8s-code/pods/frontend-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  labels:\n    app: front-end\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: front-end\n      image: schoolofdevops/frontend:latest\n      ports:\n        - containerPort: 8079\n  nodeSelector:\n    zone: 'aaa'\n\n\n\n\nFor this change, pod needs to be re created.\n\n\nkubectl apply -f frontend-pod.yml\n\n\n\n\nLiveness Probe\n\n\nLiveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is \nSuccess\n.\n\n\nLet us add liveness probe to our \nfrontend\n pod. The following probe will check whether it is able to \naccess the port or not\n.\n\n\nFile:  k8s-code/pods/frontend-pod.yml\n\n\n[...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n\n\n\n\nExpected output:\n\n\nkubectl apply -f front-end/frontend-pod.yml\nkubectl get pods\nkubectl describe pod front-end-757db58546-fkgdw\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              22s   default-scheduler  Successfully assigned front-end-757db58546-fkgdw to node4\n  Normal  SuccessfulMountVolume  22s   kubelet, node4     MountVolume.SetUp succeeded for volume \ndefault-token-w4279\n\n  Normal  Pulling                20s   kubelet, node4     pulling image \nschoolofdevops/frontend\n\n  Normal  Pulled                 17s   kubelet, node4     Successfully pulled image \nschoolofdevops/frontend\n\n  Normal  Created                17s   kubelet, node4     Created container\n  Normal  Started                17s   kubelet, node4     Started container\n\n\n\n\nLet us change the livenessProbe check port to 8080.\n\n\n          livenessProbe:\n            tcpSocket:\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 5\n\n\n\n\nApply this pod file and check the description of the pod\n\n\nExpected output:\n\n\nkubectl apply -f frontend-pod.yml\nkubectl get pods\nkubectl describe pod front-end-bf86ffd8b-bjb7p\n\n[...]\nEvents:\n  Type     Reason                 Age               From               Message\n  ----     ------                 ----              ----               -------\n  Normal   Scheduled              1m                default-scheduler  Successfully assigned front-end-bf86ffd8b-bjb7p to node3\n  Normal   SuccessfulMountVolume  1m                kubelet, node3     MountVolume.SetUp succeeded for volume \ndefault-token-w4279\n\n  Normal   Pulling                38s (x2 over 1m)  kubelet, node3     pulling image \nschoolofdevops/frontend\n\n  Normal   Killing                38s               kubelet, node3     Killing container with id docker://front-end:Container failed liveness probe.. Container will be killed and recreated.\n  Normal   Pulled                 35s (x2 over 1m)  kubelet, node3     Successfully pulled image \nschoolofdevops/frontend\n\n  Normal   Created                35s (x2 over 1m)  kubelet, node3     Created container\n  Normal   Started                35s (x2 over 1m)  kubelet, node3     Started container\n  Warning  Unhealthy              27s (x5 over 1m)  kubelet, node3     Liveness probe failed: Get http://10.233.71.50:8080/: dial tcp 10.233.71.50:8080: getsockopt: connection refused\n\n\n\n\nReadiness Probe\n\n\nReadiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is \nSuccess\n.\n\n\nReadiness probe is configured just like liveness probe. But this time we will use \nhttpGet request\n.\n\n\nFile:  k8s-code/pods/frontend-pod.yml\n\n\n[...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          [...]\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3\n\n\n\n\nExpected output:\n\n\nkubectl apply -f front-end/frontend-pod.yml\nkubectl get pods\nkubectl describe pod front-end-c5bc89b57-g42nc\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              11s   default-scheduler  Successfully assigned front-end-c5bc89b57-g42nc to node4\n  Normal  SuccessfulMountVolume  10s   kubelet, node4     MountVolume.SetUp succeeded for volume \ndefault-token-w4279\n\n  Normal  Pulling                8s    kubelet, node4     pulling image \nschoolofdevops/frontend\n\n  Normal  Pulled                 6s    kubelet, node4     Successfully pulled image \nschoolofdevops/frontend\n\n  Normal  Created                5s    kubelet, node4     Created container\n  Normal  Started                5s    kubelet, node4     Started container\n\n\n\n\nTask\n: Change the readinessProbe port to 8080 and check what happens to the pod.\n\n\nResource requests and limits\n\n\nWe can control the amount of resource requested and used by all the pods. This can be done by adding following data the pod template.\n\n\nResource Request\n\n\nFile:  k8s-code/pods/frontend-pod.yml\n\n\n[...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          [...]\n          resources:\n            requests:\n              memory: \n128Mi\n\n              cpu: \n250m\n\n\n\n\n\nThis ensures that pod always get the minimum cpu and memory specified. But this does not restrict the pod from accessing additional resources if needed. Thats why we have to use \nresource limit\n to limit the resource usage by a pod.\n\n\nExpected output:\n\n\nkubectl describe pod front-end-5c64b7c5cc-cwgr5\n\n[...]\nContainers:\n  front-end:\n    Container ID:\n    Image:          schoolofdevops/frontend\n    Image ID:\n    Port:           8079/TCP\n    State:          Waiting\n      Reason:       ContainerCreating\n    Ready:          False\n    Restart Count:  0\n    Requests:\n      cpu:        250m\n      memory:     128Mi\n\n\n\n\nResource limit\n\n\nFile: code/frontend-pod.yml\n\n\n[...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n          [...]\n            limits:\n              memory: \n256Mi\n\n              cpu: \n500m\n\n\n\n\n\nExpected output:\n\n\nkubectl describe pod front-end-5b877b4dff-5twdd\n\n[...]\nContainers:\n  front-end:\n    Container ID:   docker://d49a08c18fd9651af2f3dd28772da977b238a4010f14372e72e0ca24dcec8554\n    Image:          schoolofdevops/frontend\n    Image ID:       docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50\n    Port:           8079/TCP\n    State:          Running\n      Started:      Thu, 08 Feb 2018 17:14:54 +0530\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     500m\n      memory:  256Mi\n    Requests:\n      cpu:        250m\n      memory:     128Mi\n\n\n\n\nDefining node/pod affinity and anti-affinity\n\n\nWe have discussed about scheduling a pod on a particular node using \nNodeSelector\n, but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions.\n\n\nNode Affinity\n\n\nSoft Node Affinity\n\n\nLet us label our node1 to \nfruit=apple\n.\n\n\nkubectl label node node1 fruit=apple\n\n\n\n\n\nLet us modify our pod file to take advantage of this newly labeled node.\n\n\nFile: k8s-code/pods/frontend-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: fruit\n                operator: In\n                values:\n                - apple\n      containers:\n        [...]\n\n\n\n\nThis is a soft affinity. If there are no nodes labeled as apple, the pod would have still be scheduled.\n\n\nTask:\n Change the label value to some other fruit name and check the scheduling.\n\n\nHard Node Affinity\n\n\nLet us test the \nhard node affinity\n. This condition must be met for the pod to be scheduled. Change the pod file as given below.\n\n\nFile: k8s-code/pods/frontend-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: fruit\n                operator: In\n                values:\n                - orange\n      containers:\n      [...]\n\n\n\n\nExpected output:\n\n\nkubectl describe pod front-end-d7b787cdf-hhvfr\n\n[...]\nEvents:\n  Type     Reason            Age               From               Message\n  ----     ------            ----              ----               -------\n  Warning  FailedScheduling  32s (x8 over 1m)  default-scheduler  0/4 nodes are available: 4 MatchNodeSelector.\n\n\n\n\nNode Anti-Affinity\n\n\nNode anti-affinity can be achieved by using \nNotIn\n operator. This will help us to ignore nodes while scheduling.\n\n\nFile:  k8s-code/pods/frontend-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: fruit\n                operator: NotIn\n                values:\n                - orange\n      containers:\n      [...]\n\n\n\n\nThis will schedule the pod on nodes other than node1.\n\n\nPod Affinity\n\n\nHard Pod Affinity\n\n\nNode affinity allows you to schedule pods on selective nodes. But what if you want to run pods along with other pods selectively. Pod affinity helps us with that.\n\n\nFile:  k8s-code/pods/frontend-pod.yml\n\n\n[..]\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - catalogue\n              topologyKey: kubernetes.io/hostname\n\n\n\n\nExpected output:\n\n\nkubectl describe pod front-end-85cc68d56b-4djtt\n\n[...]\nEvents:\n  Type     Reason            Age               From               Message\n  ----     ------            ----              ----               -------\n  Warning  FailedScheduling  7s (x5 over 14s)  default-scheduler  0/4 nodes are available: 4 MatchInterPodAffinity, 4 PodAffinityRulesNotMatch\n\n\n\n\nThis is a hard pod affinity. If none of the node has a pod running with label \napp=catalogue\n, the pod will not be scheduled at all. Here \ntopologyKey\n is a label of a node. This can be any node label.\n\n\nPod Anti-Affinity\n\n\nPod anti-affinity works the opposite way of pod affinity.\n\n\nLets create \nfrontend\n pod this time.\n\n\nkubectl apply\nkubectl apply -f catalogue-pod.yml\n\n\n\n\ncatalogue pod has a label called \napp=catalogue\n. Check on which node, catalogue pod is running.\n\n\nkubectl get pods -o wide\n\nNAME                         READY     STATUS    RESTARTS   AGE       IP             NODE\ncatalogue-86647dcb5b-f6t2j   1/1       Running   0          9s        10.233.71.52   node3\n\n\n\n\nChange the front-end pod file as follows.\n\n\nFile: k8s-code/pods/frontend-pod.yml\n\n\n[...]\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - catalogue\n              topologyKey: kubernetes.io/hostname\n\n\n\n\nNow apply the pod.\n\n\nkubectl apply -f frontend-pod.yml\nkubectl get pods -o wide\n\nNAME                         READY     STATUS    RESTARTS   AGE       IP               NODE\ncatalogue-86647dcb5b-f6t2j   1/1       Running   0          2m        10.233.71.52     node3\nfront-end-5cbc986f44-mjr5m   1/1       Running   0          1m        10.233.102.134   node1\n\n\n\n\nTaints and tolerations\n\n\nTaint the node.\n\n\nkubectl taint node node4 dedicate=frontend:NoExecute\n\n\n\n\nApply toleration in the pod manifest.\n\n\nFile:  k8s-code/pods/frontend-pod.yml\n\n\napiVersion: v1\nkind: pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      tolerations:\n        - key: \ndedicate\n\n          operator: \nEqual\n\n          value: \nfrontend\n\n          effect: \nNoExecute\n\n      containers:\n        - name: frontend\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n\n\n\n\nCheck the pod list. This frontend pod will only be scheduled in node4. If there were any pod running in node4, before it got tainted, will be evicted to other hosts.\n\n\nCreating Multi Container Pods\n\n\nfile: k8s-code/pods/multi_container_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n    - name: sync\n      image: schoolofdevops/synch\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n  volumes:\n  - name: data\n    emptyDir: {}\n\n\n\n\nTo create this pod\n\n\nkubectl apply -f multi_container_pod.yml\n\n\n\n\nCheck Status\n\n\nroot@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m\n\n\n\n\nChecking logs, logging in\n\n\nkubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync\n\n\n\n\n\nExercise\n\n\nCreate a pod definition for redis and deploy.\n\n\nReading List\n\n\n\n\nPodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core\n\n\nManaging Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/\n\n\nNode Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Launching Pods"
        }, 
        {
            "location": "/5_deploying_pods/#deploying-pods", 
            "text": "Life of a pod   Pending : in progress  Running  Succeeded : successfully exited  Failed  Unknown", 
            "title": "Deploying Pods"
        }, 
        {
            "location": "/5_deploying_pods/#probes", 
            "text": "livenessProbe : Containers are Alive  readinessProbe : Ready to Serve Traffic", 
            "title": "Probes"
        }, 
        {
            "location": "/5_deploying_pods/#resource-configs", 
            "text": "Each entity created with kubernetes is a resource including pod, service, pods, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.  AKMS  =  Resource Configs Specs  apiVersion: v1\nkind:\nmetadata:\nspec:  Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/  To list supported version of apis  kubectl api-versions", 
            "title": "Resource Configs"
        }, 
        {
            "location": "/5_deploying_pods/#common-configurations", 
            "text": "Throughout this tutorial, we would be deploying different components of  example voting application. Lets assume we are deploying it in a  dev  environment. Lets create the common specs for this app with the AKMS schema discussed above.  file: k8s-code/projects/mogambo/common.yml  apiVersion: v1\nkind:\nmetadata:\n  name: front-end\n  labels:\n    app: front-end\n    role: ui\n    tier: front\nspec:  Lets now create the  Pod config by adding the kind and specs to above schema.  Filename: k8s-code/pods/frontend-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  labels:\n    app: front-end\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: front-end\n      image: schoolofdevops/frontend:latest  Use this link to refer to pod spec", 
            "title": "Common Configurations"
        }, 
        {
            "location": "/5_deploying_pods/#launching-and-operating-a-pod", 
            "text": "Syntax:   kubectl apply -f FILE  To Launch pod using configs above,  kubectl apply -f frontend-pod.yml  To view pods  kubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods front-end  To get detailed info  kubectl describe pods front-end  [Output:]  Name:         front-end\nNamespace:    mogambo\nNode:         gke-test-cluster-default-pool-d29bc0e9-jz8w/10.128.0.2\nStart Time:   Thu, 22 Feb 2018 14:13:01 +0530\nLabels:       app=front-end\n              role=ui\n              tier=front\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Pod , metadata :{ annotations :{}, labels :{ app : front-end , role : ui , tier : front }, name : front-end , namespace : mo...\nStatus:       Running\nIP:           10.8.1.12\nContainers:\n  front-end:\n    Container ID:   docker://587d8f8e40a23548eb2e2a70c1a43d32dce9922155781d1f76fbdd64f920c545\n    Image:          schoolofdevops/frontend:latest\n    Image ID:       docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50\n    Port:            none \n    State:          Running\n      Started:      Thu, 22 Feb 2018 14:13:12 +0530\n    Ready:          True\n    Restart Count:  0\n    Environment:     none \n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfsps (ro)\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          True\n  PodScheduled   True\nVolumes:\n  default-token-lfsps:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lfsps\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:   none \nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason                 Age   From                                                  Message\n  ----    ------                 ----  ----                                                  -------\n  Normal  Scheduled              32s   default-scheduler                                     Successfully assigned front-end to gke-test-cluster-default-pool-d29bc0e9-jz8w\n  Normal  SuccessfulMountVolume  31s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  MountVolume.SetUp succeeded for volume  default-token-lfsps \n  Normal  Pulling                30s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  pulling image  schoolofdevops/frontend:latest \n  Normal  Pulled                 21s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  Successfully pulled image  schoolofdevops/frontend:latest \n  Normal  Created                21s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  Created container\n  Normal  Started                21s   kubelet, gke-test-cluster-default-pool-d29bc0e9-jz8w  Started container  Commands to operate the pod  kubectl exec -it front-end ps sh\n\nkubectl exec -it front-end sh\n\nkubectl logs front-end", 
            "title": "Launching and operating a Pod"
        }, 
        {
            "location": "/5_deploying_pods/#troubleshooting-tip", 
            "text": "If you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.  kubectl get pod front-end -o yaml  Lets learn by example. Update pod spec and change the image to something that does not exist.  kubectl edit pod front-end  This will open a editor. Go to the line which defines image  and change it to a tag that does not exist  e.g.  spec:\n  containers:\n  - image: schoolofdevops/frontend:latst\n    imagePullPolicy: Always  where tag  latst  does not exist. As soon as you save this file, kubernetes will apply the change.  Now check the status,  kubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nfront-end      0/1       ImagePullBackOff   0          27m  The above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.  Observe the  status  field.    kubectl get pod front-end -o yaml  Now the status field shows a detailed information, including what the exact error. Observe the following snippet...  status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/frontend:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248  This will help you to pinpoint to the exact cause and fix it quickly.  Now that you  are done experimenting with pod, delete it with the following command,  kubectl delete pod front-end\n\nkubectl get pods", 
            "title": "Troubleshooting Tip"
        }, 
        {
            "location": "/5_deploying_pods/#attach-a-volume-to-the-pod", 
            "text": "Lets create a pod for database and attach a volume to it. To achieve this we will need to   create a  volumes  definition  attach volume to container using  VolumeMounts  property   Local host volumes are of two types:   emptyDir    hostPath     We will pick hostPath.  Refer to this doc to read more about hostPath.  File: /k8s-code/pods/db-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate  To create this pod,  kubectl apply -f db-pod.yml\n\nkubectl describe pod db\n\nkubectl get events", 
            "title": "Attach a Volume to the Pod"
        }, 
        {
            "location": "/5_deploying_pods/#selecting-a-node-to-run-on", 
            "text": "kubectl get nodes --show-labels\n\nkubectl label nodes  node-name  zone=aaa\n\nkubectl get nodes --show-labels  Update pod definition with nodeSelector  file: k8s-code/pods/frontend-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  labels:\n    app: front-end\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: front-end\n      image: schoolofdevops/frontend:latest\n      ports:\n        - containerPort: 8079\n  nodeSelector:\n    zone: 'aaa'  For this change, pod needs to be re created.  kubectl apply -f frontend-pod.yml", 
            "title": "Selecting A Node to run on"
        }, 
        {
            "location": "/5_deploying_pods/#liveness-probe", 
            "text": "Liveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is  Success .  Let us add liveness probe to our  frontend  pod. The following probe will check whether it is able to  access the port or not .  File:  k8s-code/pods/frontend-pod.yml  [...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5  Expected output:  kubectl apply -f front-end/frontend-pod.yml\nkubectl get pods\nkubectl describe pod front-end-757db58546-fkgdw\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              22s   default-scheduler  Successfully assigned front-end-757db58546-fkgdw to node4\n  Normal  SuccessfulMountVolume  22s   kubelet, node4     MountVolume.SetUp succeeded for volume  default-token-w4279 \n  Normal  Pulling                20s   kubelet, node4     pulling image  schoolofdevops/frontend \n  Normal  Pulled                 17s   kubelet, node4     Successfully pulled image  schoolofdevops/frontend \n  Normal  Created                17s   kubelet, node4     Created container\n  Normal  Started                17s   kubelet, node4     Started container  Let us change the livenessProbe check port to 8080.            livenessProbe:\n            tcpSocket:\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 5  Apply this pod file and check the description of the pod  Expected output:  kubectl apply -f frontend-pod.yml\nkubectl get pods\nkubectl describe pod front-end-bf86ffd8b-bjb7p\n\n[...]\nEvents:\n  Type     Reason                 Age               From               Message\n  ----     ------                 ----              ----               -------\n  Normal   Scheduled              1m                default-scheduler  Successfully assigned front-end-bf86ffd8b-bjb7p to node3\n  Normal   SuccessfulMountVolume  1m                kubelet, node3     MountVolume.SetUp succeeded for volume  default-token-w4279 \n  Normal   Pulling                38s (x2 over 1m)  kubelet, node3     pulling image  schoolofdevops/frontend \n  Normal   Killing                38s               kubelet, node3     Killing container with id docker://front-end:Container failed liveness probe.. Container will be killed and recreated.\n  Normal   Pulled                 35s (x2 over 1m)  kubelet, node3     Successfully pulled image  schoolofdevops/frontend \n  Normal   Created                35s (x2 over 1m)  kubelet, node3     Created container\n  Normal   Started                35s (x2 over 1m)  kubelet, node3     Started container\n  Warning  Unhealthy              27s (x5 over 1m)  kubelet, node3     Liveness probe failed: Get http://10.233.71.50:8080/: dial tcp 10.233.71.50:8080: getsockopt: connection refused", 
            "title": "Liveness Probe"
        }, 
        {
            "location": "/5_deploying_pods/#readiness-probe", 
            "text": "Readiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is  Success .  Readiness probe is configured just like liveness probe. But this time we will use  httpGet request .  File:  k8s-code/pods/frontend-pod.yml  [...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          [...]\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3  Expected output:  kubectl apply -f front-end/frontend-pod.yml\nkubectl get pods\nkubectl describe pod front-end-c5bc89b57-g42nc\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              11s   default-scheduler  Successfully assigned front-end-c5bc89b57-g42nc to node4\n  Normal  SuccessfulMountVolume  10s   kubelet, node4     MountVolume.SetUp succeeded for volume  default-token-w4279 \n  Normal  Pulling                8s    kubelet, node4     pulling image  schoolofdevops/frontend \n  Normal  Pulled                 6s    kubelet, node4     Successfully pulled image  schoolofdevops/frontend \n  Normal  Created                5s    kubelet, node4     Created container\n  Normal  Started                5s    kubelet, node4     Started container  Task : Change the readinessProbe port to 8080 and check what happens to the pod.", 
            "title": "Readiness Probe"
        }, 
        {
            "location": "/5_deploying_pods/#resource-requests-and-limits", 
            "text": "We can control the amount of resource requested and used by all the pods. This can be done by adding following data the pod template.", 
            "title": "Resource requests and limits"
        }, 
        {
            "location": "/5_deploying_pods/#resource-request", 
            "text": "File:  k8s-code/pods/frontend-pod.yml  [...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          [...]\n          resources:\n            requests:\n              memory:  128Mi \n              cpu:  250m   This ensures that pod always get the minimum cpu and memory specified. But this does not restrict the pod from accessing additional resources if needed. Thats why we have to use  resource limit  to limit the resource usage by a pod.  Expected output:  kubectl describe pod front-end-5c64b7c5cc-cwgr5\n\n[...]\nContainers:\n  front-end:\n    Container ID:\n    Image:          schoolofdevops/frontend\n    Image ID:\n    Port:           8079/TCP\n    State:          Waiting\n      Reason:       ContainerCreating\n    Ready:          False\n    Restart Count:  0\n    Requests:\n      cpu:        250m\n      memory:     128Mi", 
            "title": "Resource Request"
        }, 
        {
            "location": "/5_deploying_pods/#resource-limit", 
            "text": "File: code/frontend-pod.yml  [...]\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n          [...]\n            limits:\n              memory:  256Mi \n              cpu:  500m   Expected output:  kubectl describe pod front-end-5b877b4dff-5twdd\n\n[...]\nContainers:\n  front-end:\n    Container ID:   docker://d49a08c18fd9651af2f3dd28772da977b238a4010f14372e72e0ca24dcec8554\n    Image:          schoolofdevops/frontend\n    Image ID:       docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50\n    Port:           8079/TCP\n    State:          Running\n      Started:      Thu, 08 Feb 2018 17:14:54 +0530\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     500m\n      memory:  256Mi\n    Requests:\n      cpu:        250m\n      memory:     128Mi", 
            "title": "Resource limit"
        }, 
        {
            "location": "/5_deploying_pods/#defining-nodepod-affinity-and-anti-affinity", 
            "text": "We have discussed about scheduling a pod on a particular node using  NodeSelector , but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions.", 
            "title": "Defining node/pod affinity and anti-affinity"
        }, 
        {
            "location": "/5_deploying_pods/#node-affinity", 
            "text": "", 
            "title": "Node Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#soft-node-affinity", 
            "text": "Let us label our node1 to  fruit=apple .  kubectl label node node1 fruit=apple  Let us modify our pod file to take advantage of this newly labeled node.  File: k8s-code/pods/frontend-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: fruit\n                operator: In\n                values:\n                - apple\n      containers:\n        [...]  This is a soft affinity. If there are no nodes labeled as apple, the pod would have still be scheduled.  Task:  Change the label value to some other fruit name and check the scheduling.", 
            "title": "Soft Node Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#hard-node-affinity", 
            "text": "Let us test the  hard node affinity . This condition must be met for the pod to be scheduled. Change the pod file as given below.  File: k8s-code/pods/frontend-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: fruit\n                operator: In\n                values:\n                - orange\n      containers:\n      [...]  Expected output:  kubectl describe pod front-end-d7b787cdf-hhvfr\n\n[...]\nEvents:\n  Type     Reason            Age               From               Message\n  ----     ------            ----              ----               -------\n  Warning  FailedScheduling  32s (x8 over 1m)  default-scheduler  0/4 nodes are available: 4 MatchNodeSelector.", 
            "title": "Hard Node Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#node-anti-affinity", 
            "text": "Node anti-affinity can be achieved by using  NotIn  operator. This will help us to ignore nodes while scheduling.  File:  k8s-code/pods/frontend-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: fruit\n                operator: NotIn\n                values:\n                - orange\n      containers:\n      [...]  This will schedule the pod on nodes other than node1.", 
            "title": "Node Anti-Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#pod-affinity", 
            "text": "", 
            "title": "Pod Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#hard-pod-affinity", 
            "text": "Node affinity allows you to schedule pods on selective nodes. But what if you want to run pods along with other pods selectively. Pod affinity helps us with that.  File:  k8s-code/pods/frontend-pod.yml  [..]\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - catalogue\n              topologyKey: kubernetes.io/hostname  Expected output:  kubectl describe pod front-end-85cc68d56b-4djtt\n\n[...]\nEvents:\n  Type     Reason            Age               From               Message\n  ----     ------            ----              ----               -------\n  Warning  FailedScheduling  7s (x5 over 14s)  default-scheduler  0/4 nodes are available: 4 MatchInterPodAffinity, 4 PodAffinityRulesNotMatch  This is a hard pod affinity. If none of the node has a pod running with label  app=catalogue , the pod will not be scheduled at all. Here  topologyKey  is a label of a node. This can be any node label.", 
            "title": "Hard Pod Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#pod-anti-affinity", 
            "text": "Pod anti-affinity works the opposite way of pod affinity.  Lets create  frontend  pod this time.  kubectl apply\nkubectl apply -f catalogue-pod.yml  catalogue pod has a label called  app=catalogue . Check on which node, catalogue pod is running.  kubectl get pods -o wide\n\nNAME                         READY     STATUS    RESTARTS   AGE       IP             NODE\ncatalogue-86647dcb5b-f6t2j   1/1       Running   0          9s        10.233.71.52   node3  Change the front-end pod file as follows.  File: k8s-code/pods/frontend-pod.yml  [...]\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - catalogue\n              topologyKey: kubernetes.io/hostname  Now apply the pod.  kubectl apply -f frontend-pod.yml\nkubectl get pods -o wide\n\nNAME                         READY     STATUS    RESTARTS   AGE       IP               NODE\ncatalogue-86647dcb5b-f6t2j   1/1       Running   0          2m        10.233.71.52     node3\nfront-end-5cbc986f44-mjr5m   1/1       Running   0          1m        10.233.102.134   node1", 
            "title": "Pod Anti-Affinity"
        }, 
        {
            "location": "/5_deploying_pods/#taints-and-tolerations", 
            "text": "Taint the node.  kubectl taint node node4 dedicate=frontend:NoExecute  Apply toleration in the pod manifest.  File:  k8s-code/pods/frontend-pod.yml  apiVersion: v1\nkind: pod\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      tolerations:\n        - key:  dedicate \n          operator:  Equal \n          value:  frontend \n          effect:  NoExecute \n      containers:\n        - name: frontend\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079  Check the pod list. This frontend pod will only be scheduled in node4. If there were any pod running in node4, before it got tainted, will be evicted to other hosts.", 
            "title": "Taints and tolerations"
        }, 
        {
            "location": "/5_deploying_pods/#creating-multi-container-pods", 
            "text": "file: k8s-code/pods/multi_container_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n    - name: sync\n      image: schoolofdevops/synch\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n  volumes:\n  - name: data\n    emptyDir: {}  To create this pod  kubectl apply -f multi_container_pod.yml  Check Status  root@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m  Checking logs, logging in  kubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync", 
            "title": "Creating Multi Container Pods"
        }, 
        {
            "location": "/5_deploying_pods/#exercise", 
            "text": "Create a pod definition for redis and deploy.", 
            "title": "Exercise"
        }, 
        {
            "location": "/5_deploying_pods/#reading-list", 
            "text": "PodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core  Managing Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/  Node Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Reading List"
        }, 
        {
            "location": "/6_kubernetes_deployment/", 
            "text": "Creating a Deployment\n\n\nA Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.\n\n\nDeployment has mainly two responsibilities,\n\n\n\n\nProvide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.\n\n\nUpdate Strategy: Define a release strategy and update the pods accordingly.\n\n\n\n\nFile: k8s-code/projects/mogambo/dev/frontend-deploy.yml\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: mogambo\nspec:\n  replicas: 3\n  revisionHistoryLimit: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  minReadySeconds: 20\n  paused: false\n  template:\n    metadata:\n      labels:\n        app: front-end\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/frontend\n        imagePullPolicy: Always\n        name: front-end\n        ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n\n\nDeployment spec (deployment.spec) contains the following,\n\n\n\n\nreplicaset specs\n\n\nselectors  \n\n\nreplicas  \n\n\n\n\n\n\ndeployment spec\n\n\nstrategy\n\n\nrollingUpdate\n\n\nminReadySeconds\n\n\n\n\n\n\npod template\n\n\nmetadata, labels\n\n\ncontainer specs\n\n\n\n\n\n\n\n\nLets  create the Deployment\n\n\nkubectl apply -f frontend-deploy.yml --record\n\n\n\n\nNow that the deployment is created. To validate,\n\n\nkubectl get deployment\nkubectl get rs\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/front-end\nkubectl get pods --show-labels\n\n\n\n\nSample Output\n\n\nkubectl get deployments\n\n[output]\nNAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nfront-end   3         3         3            3           59s\n\n\n\n\nScaling a deployment\n\n\nTo scale a deployment in Kubernetes:\n\n\nkubectl scale deployment/front-end --replicas=5\n\n[output]\ndeployment \nfront-end\n scaled", 
            "title": "Creating Deployments"
        }, 
        {
            "location": "/6_kubernetes_deployment/#creating-a-deployment", 
            "text": "A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.  Deployment has mainly two responsibilities,   Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.  Update Strategy: Define a release strategy and update the pods accordingly.   File: k8s-code/projects/mogambo/dev/frontend-deploy.yml  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: mogambo\nspec:\n  replicas: 3\n  revisionHistoryLimit: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  minReadySeconds: 20\n  paused: false\n  template:\n    metadata:\n      labels:\n        app: front-end\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/frontend\n        imagePullPolicy: Always\n        name: front-end\n        ports:\n        - containerPort: 80\n          protocol: TCP  Deployment spec (deployment.spec) contains the following,   replicaset specs  selectors    replicas      deployment spec  strategy  rollingUpdate  minReadySeconds    pod template  metadata, labels  container specs     Lets  create the Deployment  kubectl apply -f frontend-deploy.yml --record  Now that the deployment is created. To validate,  kubectl get deployment\nkubectl get rs\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/front-end\nkubectl get pods --show-labels  Sample Output  kubectl get deployments\n\n[output]\nNAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nfront-end   3         3         3            3           59s", 
            "title": "Creating a Deployment"
        }, 
        {
            "location": "/6_kubernetes_deployment/#scaling-a-deployment", 
            "text": "To scale a deployment in Kubernetes:  kubectl scale deployment/front-end --replicas=5\n\n[output]\ndeployment  front-end  scaled", 
            "title": "Scaling a deployment"
        }, 
        {
            "location": "/7_exposing_app_with_service/", 
            "text": "Exposing Application with  a Service\n\n\nTypes of Services:   \n\n\n\n\nClusterIP\n\n\nNodePort\n\n\nLoadBalancer\n\n\nExternalName\n\n\n\n\n\n\nkubectl get pods\nkubectl get svc\n\n\n\n\nSample Output:\n\n\nNAME                         READY     STATUS    RESTARTS   AGE\nfront-end-64597c55f8-dk6g7   1/1       Running   0          4m\nfront-end-64597c55f8-mkp2h   1/1       Running   0          4m\nfront-end-64597c55f8-r2c54   1/1       Running   0          4m\n\n\n\n\nPublishing a service with NodePort\n\n\nFilename: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: front-end\n  namespace: mogambo\nspec:\n  selector:\n    app: front-end\n  ports:\n  - port: 8079\n    protocol: TCP\n    targetPort: 8079\n  type: NodePort\n\n\n\n\nSave the file.\n\n\nNow to create a service:\n\n\nkubectl apply -f frontend-svc.yaml\nkubectl get svc\n\n\n\n\nNow to check which port the pod is connected\n\n\nkubectl describe service front-end\n\n\n\n\nCheck for the Nodeport here\n\n\nSample Output\n\n\nName:                     front-end\nNamespace:                mogambo\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nService\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\nrole\n:\nsvc\n,\ntier\n:\nfront\n},\nname\n:\nfront-end\n,\nnamespace\n:\nmogambo\n},\nspec...\nSelector:                 app=front-end\nType:                     NodePort\nIP:                       10.11.248.77\nPort:                     \nunset\n  8079/TCP\nTargetPort:               8079/TCP\nNodePort:                 \nunset\n  30197/TCP\nEndpoints:                10.8.1.13:8079,10.8.1.14:8079,10.8.1.15:8079 + 3 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \nnone\n\n\n\n\n\nGo to browser and check hostip:NodePort\n\n\nHere the node port is 30197.\n\n\nSample output will be:\n\n\n\n\nExposing the app with ExternalIP\n\n\nfile: /k8s-code/projects/mogambo/dev/frontend-svc.yml\n\n\nspec:\n  selector:\n    app: front-end\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n  externalIPs:\n    - 192.168.12.11\n    - 192.168.12.12\n\n\n\n\napply\n\n\nkubectl apply -f frontend-svc.yml\nkubectl  get svc\nkubectl describe svc front-end\n\n\n\n\nExposing the app with LoadBalancer\n\n\nService type LoadBalancer is only supported on AWS,GCE,Azure and Openstack. If you have you cluster running on any of these clouds, give it a try. It will create a load balancer for us with a ephemeral IP. We can also specify a loadBalancerIP. Mostly not recommended to use. Using service type LoadBalancer will rise your cloud provider spendings. Think about launching 10 load balancers for 10 different services. Thats where \ningress\n come into picture (explained in the later part).\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  selector:\n    app: front-end\n  ports:\n    - protocol: TCP\n      port: 8079\n  type: LoadBalancer\n\n\n\n\nHeadless services with Endpoints/External Names\n\n\nSometimes you may to decouple the services from backend pods. Sometimes you may want to use some external services outside the cluster and want to integrate those external services with the cluster. For these kind of use cases we can use \nHeadless services\n. Let us see an example.\n\n\nLet us consider a scenario where you have your redis cluster hosted outside the cluster. Your backend in cluster need to use this redis cluster. In this case, you will create a service with ExternalName, which is nothing but a CNAME record of your redis cluster. This type of services will neither have any ports defined nor have any selectors. But, how do you make sure the backend pods uses this service? For that, you will create a custom endpoint, in which map your service to specific endpoints.\n\n\nEx: External Name service\n\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: redis-aws\n  namespace: mogambo\nspec:\n  type: ExternalName\n  externalName: redis.aws.schoolofdevops.com\n\n\n\n\nEx: Endpoints\n\n\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: redis-aws\nsubsets:\n  - addresses:\n      - ip: 10.40.30.123\n      - ip: 10.40.30.324\n      - ip: 10.40.30.478\n    ports:\n      - port: 9376\n\n\n\n\nThese IPs have to be manually managed by the cluster administrator.", 
            "title": "Service Endpoints"
        }, 
        {
            "location": "/7_exposing_app_with_service/#exposing-application-with-a-service", 
            "text": "Types of Services:      ClusterIP  NodePort  LoadBalancer  ExternalName    kubectl get pods\nkubectl get svc  Sample Output:  NAME                         READY     STATUS    RESTARTS   AGE\nfront-end-64597c55f8-dk6g7   1/1       Running   0          4m\nfront-end-64597c55f8-mkp2h   1/1       Running   0          4m\nfront-end-64597c55f8-r2c54   1/1       Running   0          4m", 
            "title": "Exposing Application with  a Service"
        }, 
        {
            "location": "/7_exposing_app_with_service/#publishing-a-service-with-nodeport", 
            "text": "Filename: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: front-end\n  namespace: mogambo\nspec:\n  selector:\n    app: front-end\n  ports:\n  - port: 8079\n    protocol: TCP\n    targetPort: 8079\n  type: NodePort  Save the file.  Now to create a service:  kubectl apply -f frontend-svc.yaml\nkubectl get svc  Now to check which port the pod is connected  kubectl describe service front-end  Check for the Nodeport here  Sample Output  Name:                     front-end\nNamespace:                mogambo\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Service , metadata :{ annotations :{}, labels :{ role : svc , tier : front }, name : front-end , namespace : mogambo }, spec...\nSelector:                 app=front-end\nType:                     NodePort\nIP:                       10.11.248.77\nPort:                      unset   8079/TCP\nTargetPort:               8079/TCP\nNodePort:                  unset   30197/TCP\nEndpoints:                10.8.1.13:8079,10.8.1.14:8079,10.8.1.15:8079 + 3 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                    none   Go to browser and check hostip:NodePort  Here the node port is 30197.  Sample output will be:", 
            "title": "Publishing a service with NodePort"
        }, 
        {
            "location": "/7_exposing_app_with_service/#exposing-the-app-with-externalip", 
            "text": "file: /k8s-code/projects/mogambo/dev/frontend-svc.yml  spec:\n  selector:\n    app: front-end\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n  externalIPs:\n    - 192.168.12.11\n    - 192.168.12.12  apply  kubectl apply -f frontend-svc.yml\nkubectl  get svc\nkubectl describe svc front-end", 
            "title": "Exposing the app with ExternalIP"
        }, 
        {
            "location": "/7_exposing_app_with_service/#exposing-the-app-with-loadbalancer", 
            "text": "Service type LoadBalancer is only supported on AWS,GCE,Azure and Openstack. If you have you cluster running on any of these clouds, give it a try. It will create a load balancer for us with a ephemeral IP. We can also specify a loadBalancerIP. Mostly not recommended to use. Using service type LoadBalancer will rise your cloud provider spendings. Think about launching 10 load balancers for 10 different services. Thats where  ingress  come into picture (explained in the later part).  apiVersion: v1\nkind: Service\nmetadata:\n  name: front-end\n  namespace: mogambo\n  labels:\n    app: front-end\n    env: dev\nspec:\n  selector:\n    app: front-end\n  ports:\n    - protocol: TCP\n      port: 8079\n  type: LoadBalancer", 
            "title": "Exposing the app with LoadBalancer"
        }, 
        {
            "location": "/7_exposing_app_with_service/#headless-services-with-endpointsexternal-names", 
            "text": "Sometimes you may to decouple the services from backend pods. Sometimes you may want to use some external services outside the cluster and want to integrate those external services with the cluster. For these kind of use cases we can use  Headless services . Let us see an example.  Let us consider a scenario where you have your redis cluster hosted outside the cluster. Your backend in cluster need to use this redis cluster. In this case, you will create a service with ExternalName, which is nothing but a CNAME record of your redis cluster. This type of services will neither have any ports defined nor have any selectors. But, how do you make sure the backend pods uses this service? For that, you will create a custom endpoint, in which map your service to specific endpoints.  Ex: External Name service  kind: Service\napiVersion: v1\nmetadata:\n  name: redis-aws\n  namespace: mogambo\nspec:\n  type: ExternalName\n  externalName: redis.aws.schoolofdevops.com  Ex: Endpoints  kind: Endpoints\napiVersion: v1\nmetadata:\n  name: redis-aws\nsubsets:\n  - addresses:\n      - ip: 10.40.30.123\n      - ip: 10.40.30.324\n      - ip: 10.40.30.478\n    ports:\n      - port: 9376  These IPs have to be manually managed by the cluster administrator.", 
            "title": "Headless services with Endpoints/External Names"
        }, 
        {
            "location": "/8_rollouts_and_rollbacks/", 
            "text": "Rolling updates with deployments\n\n\nUpdate the version of the image in frontend-deploy.yml\n\n\nFile: k8s-code/projects/mogambo/dev/frontend-deploy.yml\n\n\n...\n    app: front-end\n    spec:\n      containers:\n      - image: schoolofdevops/frontend:dogs\n\n\n\n\n\nApply Changes and monitor the rollout\n\n\nkubectl apply -f frontend-deploy.yaml\nkubectl rollout status deployment/front-end\n\n\n\n\nRolling Back a Failed Update\n\n\nLets update the image to a tag which is non existent. We intentionally introduce this intentional error to fail fail the deployment.\n\n\nFile: k8s-code/projects/mogambo/dev/frontend-deploy.yml\n\n\n...\n    app: front-end\n    spec:\n      containers:\n      - image: schoolofdevops/frontend:movi\n\n\n\n\n\nDo a new rollout and monitor\n\n\nkubectl apply -f frontend-deploy.yml\nkubectl rollout status deployment/front-end\n\n\n\n\nAlso watch the pod status which might look like\n\n\nfront-end-645bb6fcd5-xsndp   1/1       Running            0          3m\nfront-end-b9bc5495f-8nbz5    0/1       ImagePullBackOff   0          54s\nfront-end-b9bc5495f-9x5gr    0/1       ImagePullBackOff   0          54s\nfront-end-b9bc5495f-pdx2l    0/1       ImagePullBackOff   0          55s\n\n\n\n\nTo get the revision history and details  \n\n\nkubectl rollout history deployment/front-end\nkubectl rollout history deployment/front-end --revision=x\n[replace x with the latest revision]\n\n\n\n\n[Sample Output]\n\n\nroot@kube-01:~# kubectl rollout history deployment/front-end\ndeployments \nfront-end\n\nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/front-end --replicas=5\n3       \nnone\n\n6       \nnone\n\n7       \nnone\n\n\nroot@kube-01:~# kubectl rollout history deployment/front-end --revision=7\ndeployments \nfront-end\n with revision #7\nPod Template:\n  Labels:   app=front-end\n    pod-template-hash=656710519\n    role=ui\n    tier=front\n  Containers:\n   front-end:\n    Image:  schoolofdevops/frontend:movi\n    Port:   8079/TCP\n    Environment:    \nnone\n\n    Mounts: \nnone\n\n  Volumes:  \nnone\n\n\n\n\n\nTo undo rollout,\n\n\nkubectl rollout undo deployment/front-end\n\n\n\n\nor\n\n\nkubectl rollout undo deployment/front-end --to-revision=1\nkubectl get rs\nkubectl describe deployment front-end", 
            "title": "Rollouts and Rollbacks"
        }, 
        {
            "location": "/8_rollouts_and_rollbacks/#rolling-updates-with-deployments", 
            "text": "Update the version of the image in frontend-deploy.yml  File: k8s-code/projects/mogambo/dev/frontend-deploy.yml  ...\n    app: front-end\n    spec:\n      containers:\n      - image: schoolofdevops/frontend:dogs  Apply Changes and monitor the rollout  kubectl apply -f frontend-deploy.yaml\nkubectl rollout status deployment/front-end", 
            "title": "Rolling updates with deployments"
        }, 
        {
            "location": "/8_rollouts_and_rollbacks/#rolling-back-a-failed-update", 
            "text": "Lets update the image to a tag which is non existent. We intentionally introduce this intentional error to fail fail the deployment.  File: k8s-code/projects/mogambo/dev/frontend-deploy.yml  ...\n    app: front-end\n    spec:\n      containers:\n      - image: schoolofdevops/frontend:movi  Do a new rollout and monitor  kubectl apply -f frontend-deploy.yml\nkubectl rollout status deployment/front-end  Also watch the pod status which might look like  front-end-645bb6fcd5-xsndp   1/1       Running            0          3m\nfront-end-b9bc5495f-8nbz5    0/1       ImagePullBackOff   0          54s\nfront-end-b9bc5495f-9x5gr    0/1       ImagePullBackOff   0          54s\nfront-end-b9bc5495f-pdx2l    0/1       ImagePullBackOff   0          55s  To get the revision history and details    kubectl rollout history deployment/front-end\nkubectl rollout history deployment/front-end --revision=x\n[replace x with the latest revision]  [Sample Output]  root@kube-01:~# kubectl rollout history deployment/front-end\ndeployments  front-end \nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/front-end --replicas=5\n3        none \n6        none \n7        none \n\nroot@kube-01:~# kubectl rollout history deployment/front-end --revision=7\ndeployments  front-end  with revision #7\nPod Template:\n  Labels:   app=front-end\n    pod-template-hash=656710519\n    role=ui\n    tier=front\n  Containers:\n   front-end:\n    Image:  schoolofdevops/frontend:movi\n    Port:   8079/TCP\n    Environment:     none \n    Mounts:  none \n  Volumes:   none   To undo rollout,  kubectl rollout undo deployment/front-end  or  kubectl rollout undo deployment/front-end --to-revision=1\nkubectl get rs\nkubectl describe deployment front-end", 
            "title": "Rolling Back a Failed Update"
        }, 
        {
            "location": "/9_deployement_strategies/", 
            "text": "Recreate\n\n\nWhen the \nRecreate\n deployment strategy is used,\n  * The old pods will be deleted\n  * Then the new pods will be created.\n\n\nThis will create some downtime in our stack. Hence, this strategy is only recommended for development use cases.\n\n\nLet us change the deployment strategy to \nrecreate\n and image tag to \nlatest\n.\n\n\nFile: k8s-code/projects/mogambo/dev/catalogue-deploy.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: catalogue\n  namespace: mogambo\n  labels:\n    app: catalogue\n    env: dev\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: catalogue\n        env: dev\n    spec:\n      tolerations:\n        - key: \ndedicate\n\n          operator: \nEqual\n\n          value: \ncatalogue\n\n          effect: \nNoExecute\n\n      containers:\n        - name: catalogue\n          image: schoolofdevops/catalogue:latest\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 80\n\n\n\n\n\nPause/Unpause\n\n\nWhen you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations,\n  1. we can pause the update,\n  2. fix the issue,\n  3. resume the update.\n\n\nLet us change the image tag to V2 in pod spec.\n\n\nFile: catalogue-deploy.yml\n\n\n      containers:\n        - name: catalogue\n          image: schoolofdevops/catalogue:V2\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 80\n\n\n\n\nApply the changes.\n\n\nkubectl apply -f catalogue-deploy.yml\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS         RESTARTS   AGE\ncatalogue-6c4f7b49d8-g5dgc   1/1       Running        0          16m\ncatalogue-765554cc7-xsbhs    0/1       ErrImagePull   0          9s\n\n\n\n\nOur deployment is failing. From some debugging, we can conclude that we are using a wrong image tag.\n\n\nNow pause the update\n\n\nkubectl rollout pause\n\n\n\n\nSet the deployment to use \nv2\n version of the image.\n\n\nkubectl set image deployment catalogue catalogue=schoolofdevops/catalogue:v2\n\n[output]\ndeployment \ncatalogue\n image updated\n\n\n\n\nNow resume the update\n\n\nkubectl rollout resume deployment catalogue\nkubectl rollout status deployment catalogue\n\n[Ouput]\ndeployment \ncatalogue\n successfully rolled out\n\n\n\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS    RESTARTS   AGE\ncatalogue-6875c8df8f-k4hls   1/1       Running   0          1m\n\n\n\n\nWhen we do this, we skip the need of creating a new rolling update altogether.\n\n\nAdditional Release Strategies\n\n\nAlong with rolling update and recreate strategies, we can also do,\n\n\n\n\nCanary releases\n\n\nBlue/Green deployments.\n\n\n\n\nBlue/Green Deployments\n\n\n\n\n\n\nRecreate type gives an error. Need to be fixed.\n\n\nAdd Use-cases for recreate strategy type.\n\n\nAdd additional details of why we skip creating a replica set / rolling update in pause/unpause section.", 
            "title": "Deployment Strategies"
        }, 
        {
            "location": "/9_deployement_strategies/#recreate", 
            "text": "When the  Recreate  deployment strategy is used,\n  * The old pods will be deleted\n  * Then the new pods will be created.  This will create some downtime in our stack. Hence, this strategy is only recommended for development use cases.  Let us change the deployment strategy to  recreate  and image tag to  latest .  File: k8s-code/projects/mogambo/dev/catalogue-deploy.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: catalogue\n  namespace: mogambo\n  labels:\n    app: catalogue\n    env: dev\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: catalogue\n        env: dev\n    spec:\n      tolerations:\n        - key:  dedicate \n          operator:  Equal \n          value:  catalogue \n          effect:  NoExecute \n      containers:\n        - name: catalogue\n          image: schoolofdevops/catalogue:latest\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 80", 
            "title": "Recreate"
        }, 
        {
            "location": "/9_deployement_strategies/#pauseunpause", 
            "text": "When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations,\n  1. we can pause the update,\n  2. fix the issue,\n  3. resume the update.  Let us change the image tag to V2 in pod spec.  File: catalogue-deploy.yml        containers:\n        - name: catalogue\n          image: schoolofdevops/catalogue:V2\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 80  Apply the changes.  kubectl apply -f catalogue-deploy.yml\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS         RESTARTS   AGE\ncatalogue-6c4f7b49d8-g5dgc   1/1       Running        0          16m\ncatalogue-765554cc7-xsbhs    0/1       ErrImagePull   0          9s  Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag.  Now pause the update  kubectl rollout pause  Set the deployment to use  v2  version of the image.  kubectl set image deployment catalogue catalogue=schoolofdevops/catalogue:v2\n\n[output]\ndeployment  catalogue  image updated  Now resume the update  kubectl rollout resume deployment catalogue\nkubectl rollout status deployment catalogue\n\n[Ouput]\ndeployment  catalogue  successfully rolled out  kubectl get pods\n\n[Output]\nNAME                         READY     STATUS    RESTARTS   AGE\ncatalogue-6875c8df8f-k4hls   1/1       Running   0          1m  When we do this, we skip the need of creating a new rolling update altogether.", 
            "title": "Pause/Unpause"
        }, 
        {
            "location": "/9_deployement_strategies/#additional-release-strategies", 
            "text": "Along with rolling update and recreate strategies, we can also do,   Canary releases  Blue/Green deployments.", 
            "title": "Additional Release Strategies"
        }, 
        {
            "location": "/9_deployement_strategies/#bluegreen-deployments", 
            "text": "Recreate type gives an error. Need to be fixed.  Add Use-cases for recreate strategy type.  Add additional details of why we skip creating a replica set / rolling update in pause/unpause section.", 
            "title": "Blue/Green Deployments"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/", 
            "text": "Configmap\n\n\nConfigmap is one of the ways to provide configurations to your application.\n\n\nInjecting env variables with configmaps\n\n\nCreate our configmap for vote app\n\n\nfile:  projects/instavote/dev/vote-cm.yaml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: EMACS\n  OPTION_B: VI\n\n\n\n\nIn the above given configmap, we define two environment variables,\n\n\n\n\nOPTION_A=EMACS\n\n\nOPTION_B=VI\n\n\n\n\nIn order to use this configmap in the deployment, we need to reference it from the deployment file.\n\n\nCheck the deployment file for vote add for the following block.\n\n\nfile: \nvote-deploy.yaml\n\n\n...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always\n\n\n\n\nSo when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.\n\n\n\n\nConfigmap as a configuration file\n\n\nIn the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.\n\n\nSyntax for consuming file as a configmap is as follows\n\n\n  kubectl create configmap --from-file \nCONF-FILE-PATH\n \nNAME-OF-CONFIGMAP\n\n\n\n\n\nWe have redis configuration as a file named \nprojects/instavote/config/redis.conf\n. We are going to convert this file into a configmap\n\n\nkubectl create configmap --from-file projects/instavote/config/redis.conf redis\n\n\n\n\nUpdate your redis-deploy.yaml file to use this confimap.\nFile: \nredis-deploy.yaml\n\n\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always\n\n\n\n\nSecrets\n\n\nSecrets are for storing sensitive data like \npasswords and keychains\n. We will see how db deployment uses username and password in form of a secret.\n\n\nYou would define two fields for db,\n\n\n\n\nusername \n\n\npassword\n\n\n\n\nTo create secrets for db you need to generate  \nbase64\n format as follows,\n\n\necho \nadmin\n | base64\necho \npassword\n | base64\n\n\n\n\nwhere \nadmin\n and \npassword\n are the actual values that you would want to inject into the pod environment.\n\n\nIf you do not have a unix host, you can make use of online base64 utility to generate these strings.\n\n\nhttp://www.utilities-online.info/base64\n\n\n\n\nLets now add it to the secrets file,\n\n\nFile: projects/instavote/dev/db-secrets.yaml\n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  # base64 of admin\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n  # base64 of password\n\n\n\n\nTo consume these secrets, update the deployment as\n\n\nfile: db-deploy.yaml.\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n# Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always\n\n\n\n\nNote: Automatic Updation of deployments on ConfigMap  Updates\n\n\nCurrently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.  \n\n\nThere is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368\n\n\nCurrently, this can be done by using immutable configMaps.  \n\n\n\n\nCreate a configMaps and apply it with deployment. \n\n\nTo update, create a new configMaps and do not update the previous one. Treat it as immutable. \n\n\nUpdate deployment spec to use the new version of the configMaps. This will ensure immediate update.", 
            "title": "Using Configmaps and Secrets"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#configmap", 
            "text": "Configmap is one of the ways to provide configurations to your application.", 
            "title": "Configmap"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#injecting-env-variables-with-configmaps", 
            "text": "Create our configmap for vote app  file:  projects/instavote/dev/vote-cm.yaml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: EMACS\n  OPTION_B: VI  In the above given configmap, we define two environment variables,   OPTION_A=EMACS  OPTION_B=VI   In order to use this configmap in the deployment, we need to reference it from the deployment file.  Check the deployment file for vote add for the following block.  file:  vote-deploy.yaml  ...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always  So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.", 
            "title": "Injecting env variables with configmaps"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#configmap-as-a-configuration-file", 
            "text": "In the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.  Syntax for consuming file as a configmap is as follows    kubectl create configmap --from-file  CONF-FILE-PATH   NAME-OF-CONFIGMAP   We have redis configuration as a file named  projects/instavote/config/redis.conf . We are going to convert this file into a configmap  kubectl create configmap --from-file projects/instavote/config/redis.conf redis  Update your redis-deploy.yaml file to use this confimap.\nFile:  redis-deploy.yaml      spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always", 
            "title": "Configmap as a configuration file"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#secrets", 
            "text": "Secrets are for storing sensitive data like  passwords and keychains . We will see how db deployment uses username and password in form of a secret.  You would define two fields for db,   username   password   To create secrets for db you need to generate   base64  format as follows,  echo  admin  | base64\necho  password  | base64  where  admin  and  password  are the actual values that you would want to inject into the pod environment.  If you do not have a unix host, you can make use of online base64 utility to generate these strings.  http://www.utilities-online.info/base64  Lets now add it to the secrets file,  File: projects/instavote/dev/db-secrets.yaml  apiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  # base64 of admin\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n  # base64 of password  To consume these secrets, update the deployment as  file: db-deploy.yaml.  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n# Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always", 
            "title": "Secrets"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#note-automatic-updation-of-deployments-on-configmap-updates", 
            "text": "Currently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.    There is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368  Currently, this can be done by using immutable configMaps.     Create a configMaps and apply it with deployment.   To update, create a new configMaps and do not update the previous one. Treat it as immutable.   Update deployment spec to use the new version of the configMaps. This will ensure immediate update.", 
            "title": "Note: Automatic Updation of deployments on ConfigMap  Updates"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/", 
            "text": "Kubernetes Horizonntal Pod Autoscaling\n\n\nWith Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).\n\n\nThe Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user\n\n\nPrerequisites\n\n\nHeapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.\n\n\nDeploying Heapster\n\n\nGo to the below directory and create the deployment and services.\n\n\ngit clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml\n\n\n\n\nValidate that heapster, influxdb and grafana are started\n\n\nkubectl get pods -n kube-system\nkubectl get svc -n kube-system\n\n\n\n\n\nNow this will deploy the heapster monitoring.\n\n\nRun \n expose php-apache server\n\n\nTo demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80  \n\n\n\n\nSample Output\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice \nphp-apache\n created\ndeployment \nphp-apache\n created\n\n\n\n\nTo verify the created pod:\n\n\nkubectl get pods\n\n\n\n\nWait untill the pod changes to running state.\n\n\nCreate Horizontal Pod Autoscaler\n\n\nNow that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n\n\n\n\nSample Output\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment \nphp-apache\n autoscaled\n\n\n\n\nWe may check the current status of autoscaler by running:\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s\n\n\n\n\nIncrease load\n\n\nNow we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service\n\n\nkubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done\n\n\n\n\n\nNow open a new window of the same machine.\n\n\nAnd check the status of the hpa\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m\n\n\n\n\nNow if you check the pods it will be automatically scaled to the desired value.\n\n\nkubectl get pods\n\n\n\n\nSample Output\n\n\nkubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h\n\n\n\n\nStop load\n\n\nIn the terminal where we created the container with busybox image, terminate the load generation by typing \n + C\n\n\nThen we will verify the result state (after a minute or so)\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Auto Scaling Capacity with HPA"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#kubernetes-horizonntal-pod-autoscaling", 
            "text": "With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).  The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user", 
            "title": "Kubernetes Horizonntal Pod Autoscaling"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#prerequisites", 
            "text": "Heapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#deploying-heapster", 
            "text": "Go to the below directory and create the deployment and services.  git clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml  Validate that heapster, influxdb and grafana are started  kubectl get pods -n kube-system\nkubectl get svc -n kube-system  Now this will deploy the heapster monitoring.", 
            "title": "Deploying Heapster"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#run-expose-php-apache-server", 
            "text": "To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80    Sample Output  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice  php-apache  created\ndeployment  php-apache  created  To verify the created pod:  kubectl get pods  Wait untill the pod changes to running state.", 
            "title": "Run &amp; expose php-apache server"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#create-horizontal-pod-autoscaler", 
            "text": "Now that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  Sample Output  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment  php-apache  autoscaled  We may check the current status of autoscaler by running:  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s", 
            "title": "Create Horizontal Pod Autoscaler"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#increase-load", 
            "text": "Now we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service  kubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done  Now open a new window of the same machine.  And check the status of the hpa  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m  Now if you check the pods it will be automatically scaled to the desired value.  kubectl get pods  Sample Output  kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h", 
            "title": "Increase load"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#stop-load", 
            "text": "In the terminal where we created the container with busybox image, terminate the load generation by typing   + C  Then we will verify the result state (after a minute or so)  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Stop load"
        }, 
        {
            "location": "/kube_visualizer/", 
            "text": "Kubernetes Visualizer\n\n\nIn this chapter we will see how to set up kubernetes visualizer that will show us the changes in our cluster in real time.\n\n\nSet up\n\n\nFork the repository\n\n\nyum install -y git #only if you use play-with-k8s\n\ngit clone  https://github.com/schoolofdevops/kube-ops-view\n\ncd  kube-ops-view\n\n\n\n\nDeploy the visualizer on kubernetes\n\n\nkubectl apply -f deploy/\n\n[ouput]\nserviceaccount \nkube-ops-view\n created\nclusterrole \nkube-ops-view\n created\nclusterrolebinding \nkube-ops-view\n created\ndeployment \nkube-ops-view\n created\ningress \nkube-ops-view\n created\ndeployment \nkube-ops-view-redis\n created\nservice \nkube-ops-view-redis\n created\nservice \nkube-ops-view\n created\n\n\n\n\nGet the nodeport for the service.\n\n\nkubectl get svc\n\n[output]\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkube-ops-view         NodePort    10.107.204.74   \nnone\n        80:**30073**/TCP   1m\nkube-ops-view-redis   ClusterIP   10.104.50.176   \nnone\n        6379/TCP       1m\nkubernetes            ClusterIP   10.96.0.1       \nnone\n        443/TCP        8m\n\n\n\n\nIn my case, port \n30073\n is the nodeport.\n\n\nVisit the port from the browser.\n\n\nhttp://\nNODE_IP:NODE_PORT\n\n\n\n\n\n\n\nTest\n\n\nLet us test how kubernetes cluster works.\n\n\nClone kubernetes code repository from School of Devops.\n\n\ngit clone https://github.com/schoolofdevops/k8s-code\n\ncd k8s-code\n\n\n\n\nCreate the namespace.\n\n\ncd projects/instavote/\n\nkubectl apply -f instavote-ns.yaml\n\n[output]\nnamespace \ninstavote\n created\n\n\n\n\nDeploy vote application on kubernetes\n\n\ncd dev\n\nkubectl apply  -f vote-deploy.yaml\n\n[output]\ndeployment \nvote\n created", 
            "title": "Kuberentes Visualizer"
        }, 
        {
            "location": "/kube_visualizer/#kubernetes-visualizer", 
            "text": "In this chapter we will see how to set up kubernetes visualizer that will show us the changes in our cluster in real time.", 
            "title": "Kubernetes Visualizer"
        }, 
        {
            "location": "/kube_visualizer/#set-up", 
            "text": "Fork the repository  yum install -y git #only if you use play-with-k8s\n\ngit clone  https://github.com/schoolofdevops/kube-ops-view\n\ncd  kube-ops-view  Deploy the visualizer on kubernetes  kubectl apply -f deploy/\n\n[ouput]\nserviceaccount  kube-ops-view  created\nclusterrole  kube-ops-view  created\nclusterrolebinding  kube-ops-view  created\ndeployment  kube-ops-view  created\ningress  kube-ops-view  created\ndeployment  kube-ops-view-redis  created\nservice  kube-ops-view-redis  created\nservice  kube-ops-view  created  Get the nodeport for the service.  kubectl get svc\n\n[output]\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkube-ops-view         NodePort    10.107.204.74    none         80:**30073**/TCP   1m\nkube-ops-view-redis   ClusterIP   10.104.50.176    none         6379/TCP       1m\nkubernetes            ClusterIP   10.96.0.1        none         443/TCP        8m  In my case, port  30073  is the nodeport.  Visit the port from the browser.  http:// NODE_IP:NODE_PORT", 
            "title": "Set up"
        }, 
        {
            "location": "/kube_visualizer/#test", 
            "text": "Let us test how kubernetes cluster works.  Clone kubernetes code repository from School of Devops.  git clone https://github.com/schoolofdevops/k8s-code\n\ncd k8s-code  Create the namespace.  cd projects/instavote/\n\nkubectl apply -f instavote-ns.yaml\n\n[output]\nnamespace  instavote  created  Deploy vote application on kubernetes  cd dev\n\nkubectl apply  -f vote-deploy.yaml\n\n[output]\ndeployment  vote  created", 
            "title": "Test"
        }, 
        {
            "location": "/11_deploying_sample_app/", 
            "text": "Mini Project: Deploying Multi Tier Application Stack\n\n\nIn this project , you would write definitions for deploying the vote application stack with all components/tiers which include,\n\n\n\n\nvote ui\n\n\nredis\n\n\nworker\n\n\ndb\n\n\nresults ui\n\n\n\n\nTasks\n\n\n\n\nCreate deployments for all applications\n\n\nDefine services for each tier\n\n\nLaunch/appy the definitions\n\n\n\n\nFollowing table depicts the state of readiness of the above services.\n\n\n\n\n\n\n\n\nApp\n\n\nDeployment\n\n\nService\n\n\n\n\n\n\n\n\n\n\nvote\n\n\nready\n\n\nready\n\n\n\n\n\n\nredis\n\n\nin progress\n\n\nready\n\n\n\n\n\n\nworker\n\n\nin progress\n\n\nin progress\n\n\n\n\n\n\ndb\n\n\nin progress\n\n\ntodo\n\n\n\n\n\n\nresults\n\n\ntodo\n\n\ntodo\n\n\n\n\n\n\n\n\nDeploying the sample application\n\n\nTo create deploy the sample applications,\n\n\nkubectl create -f projects/instavote/dev\n\n\n\n\nSample output is like:\n\n\ndeployment \ndb\n created\nservice \ndb\n created\ndeployment \nredis\n created\nservice \nredis\n created\ndeployment \nvote\n created\nservice \nvote\n created\ndeployment \nworker\n created\ndeployment \nresults\n created\nservice \nresults\n created\n\n\n\n\nTo Validate:\n\n\nkubectl get svc -n instavote\n\n\n\n\nSample Output is:\n\n\nkubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   \npending\n     80:31808/TCP   1h\n\n\n\n\nHere the port assigned is 31808, go to the browser and enter\n\n\nmasterip:31808\n\n\n\n\n\n\nThis will load the page where you can vote.\n\n\nTo check the result:\n\n\nkubectl get service result\n\n\n\n\nSample Output is:\n\n\nkubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   \npending\n     80:32511/TCP   1h\n\n\n\n\nHere the port assigned is 32511, go to the browser and enter\n\n\nmasterip:32511\n\n\n\n\n\n\nThis is the page where we can see the results of the vote.", 
            "title": "Mini Project"
        }, 
        {
            "location": "/11_deploying_sample_app/#mini-project-deploying-multi-tier-application-stack", 
            "text": "In this project , you would write definitions for deploying the vote application stack with all components/tiers which include,   vote ui  redis  worker  db  results ui", 
            "title": "Mini Project: Deploying Multi Tier Application Stack"
        }, 
        {
            "location": "/11_deploying_sample_app/#tasks", 
            "text": "Create deployments for all applications  Define services for each tier  Launch/appy the definitions   Following table depicts the state of readiness of the above services.     App  Deployment  Service      vote  ready  ready    redis  in progress  ready    worker  in progress  in progress    db  in progress  todo    results  todo  todo", 
            "title": "Tasks"
        }, 
        {
            "location": "/11_deploying_sample_app/#deploying-the-sample-application", 
            "text": "To create deploy the sample applications,  kubectl create -f projects/instavote/dev  Sample output is like:  deployment  db  created\nservice  db  created\ndeployment  redis  created\nservice  redis  created\ndeployment  vote  created\nservice  vote  created\ndeployment  worker  created\ndeployment  results  created\nservice  results  created", 
            "title": "Deploying the sample application"
        }, 
        {
            "location": "/11_deploying_sample_app/#to-validate", 
            "text": "kubectl get svc -n instavote  Sample Output is:  kubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243    pending      80:31808/TCP   1h  Here the port assigned is 31808, go to the browser and enter  masterip:31808   This will load the page where you can vote.  To check the result:  kubectl get service result  Sample Output is:  kubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16    pending      80:32511/TCP   1h  Here the port assigned is 32511, go to the browser and enter  masterip:32511   This is the page where we can see the results of the vote.", 
            "title": "To Validate:"
        }, 
        {
            "location": "/cluster_setup_kubespray/", 
            "text": "Kubernetes - Cluster Setup using Kubespray\n\n\nKubespray is an \nAnsible\n based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.\n\n\nPrerequisites\n\n\nHardware Pre requisites\n  * 4 Nodes: Virtual/Physical Machines\n  * Memory: 2GB each\n  * CPU: 2 cores recommended\n  * Hard disk: 20GB available\n\n\nSoftware Pre Requisites\n  * Ubuntu 16.04 Operating System\n  * Python\n\n\nArchitecture of a high available kubernetes cluster\n\n\nPreparing the kubernetes nodes\n\n\nOn control node\n\n\nAnsible is the base provisioner in our cluster. But installing Ansible is out of the scope of this training. You can learn about installing and configuring Ansible from \nhere.\n\n\nSetup passwordless SSH between control and kubernetes nodes\n\n\nOn control node\n\n\nAnsible uses passwordless ssh\n1\n to create the cluster. Let us see how to set it up from your \ncontrol node\n.\n\n\nssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   E    ..       |\n|  . o +..        |\n| . +o*+o         |\n|. .o+Bo+         |\n|. .++.X S        |\n|+ +ooX .         |\n|.=.OB.+ .        |\n| .=o*= . .       |\n|  .o.   .        |\n+----[SHA256]-----+\n\n\n\n\nJust leave the fields to defaults. This command will generate a public key and private key for you.\n\n\ncat ~/.ssh/id_rsa.pub | ssh ubuntu@10.40.1.26 'cat \n ~/.ssh/authorized_keys'\n\n\n\n\nThis will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace \n10.40.1.26\n with your respective machine's IP.\n\n\nEnable IPv4 Forwarding\n\n\nOn all nodes\n\n\nssh ubuntu@10.40.1.26\nsudo su\nvim /etc/sysctl.conf\n\n\n\n\nEnalbe IPv4 forwarding by uncommenting the following line\n\n\nnet.ipv4.ip_forward=1\n\n\n\n\nStop and Disable UFW Service\n\n\nOn all nodes\n\n\nExecute the following commands to stop and disable ufw service.\n\n\nsystemctl stop ufw.service\nsystemctl disable ufw.service\n\n\n\n\nInstall Python\n\n\nOn all nodes\n\n\nAnsible needs python to be installed on all the machines.\n\n\nsudo apt update\nsudo apt install python\n\n\n\n\nConfiguring Ansible Control node and Kubespray\n\n\nOn control node\n\n\nKubespray is hosted on GitHub. Let us the clone the \nofficial repository\n.\n\n\ngit clone https://github.com/kubernetes-incubator/kubespray.git\ncd kubespray\n\n\n\n\nSet Remote User for Ansible\n\n\nOn control node\n\n\nAdd the following section in ansible.cfg file\n\n\nremote_user=ubuntu\n\n\n\n\nYour \nansible.cfg\n file should look like this.\n\n\n[ssh_connection]\npipelining=True\nssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\n#control_path = ~/.ssh/ansible-%%r@%%h:%%p\n[defaults]\nhost_key_checking=False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp\nstdout_callback = skippy\nlibrary = ./library\ncallback_whitelist = profile_tasks\nroles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles\ndeprecation_warnings=False\nremote_user=ubuntu\n\n\n\n\nDownload Inventory Builder\n\n\nOn control node\n\n\nInventory builder (a python script) helps us to create inventory file. \nInventory\n file is something with which we specify the groups of masters and nodes of our cluster.\n\n\ncd inventory\nwget https://raw.githubusercontent.com/kubernetes-incubator/kubespray/master/contrib/inventory_builder/inventory.py\n\n\n\n\nTo build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments\n\n\npython inventory.py 10.40.1.26 10.40.1.25 10.40.1.20 10.40.1.11\n\n\n\n\nThese IPs are different for you. Please replace them with your corresponding IPs.\nThis step will result in the creation of a new file \ninventory.cfg\n. This is our inventory file.\n\n\ninventory.cfg\n\n\n[all]\nnode1    ansible_host=10.40.1.26 ip=10.40.1.26\nnode2    ansible_host=10.40.1.25 ip=10.40.1.25\nnode3    ansible_host=10.40.1.20 ip=10.40.1.20\nnode4    ansible_host=10.40.1.11 ip=10.40.1.11\n\n[kube-master]\nnode1\nnode2\n\n[kube-node]\nnode1\nnode2\nnode3\nnode4\n\n[etcd]\nnode1\nnode2\nnode3\n\n[k8s-cluster:children]\nkube-node\nkube-master\n\n[calico-rr]\n\n[vault]\nnode1\nnode2\nnode3\n\n\n\n\nProvisioning  kubernetes cluster with kubespray\n\n\nOn control node\n\n\nWe are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster.\n\n\nansible-playbook -i inventory/inventory.cfg cluster.yml -b -v\n\n\n\n\nOption -i = Inventory file path\nOption -b = Become as root user\nOption -v = Give verbose output\n\n\nThis Ansible run will take around 30 mins to complete.\n\n\nInstall Kubectl\n\n\nOn control node\n\n\nBefore we proceed further, we will need to install \nkubectl\n binary in our control node. Read installation procedure from this \nlink\n.\n\n\nGetting the Kubernetes Configuration File\n\n\nOn control node\n\n\nOnce the cluster setup is done, we have to copy over the cluster config file from the master machine. We will discuss about this file extensively in the next chapter.\n\n\nssh ubuntu@10.40.1.26\nsudo su\ncp /etc/kubernetes/admin.conf /home/ubuntu\nchown ubuntu:ubuntu /home/ubuntu/admin.conf\nexit\nexit\nscp ubuntu@10.40.1.26:~/admin.conf .\ncd\nmkdir .kube\nmv admin.conf .kube/config\n\n\n\n\nCheck the State of the Cluster\n\n\nOn control node\n\n\nLet us check the state of the cluster by running,\n\n\nkubectl cluster-info\n\nKubernetes master is running at https://10.40.1.26:6443\nKubeDNS is running at https://10.40.1.26:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n\n\n\nkubectl get nodes\n\nNAME      STATUS    ROLES         AGE       VERSION\nnode1     Ready     master,node   21h       v1.9.0+coreos.0\nnode2     Ready     master,node   21h       v1.9.0+coreos.0\nnode3     Ready     node          21h       v1.9.0+coreos.0\nnode4     Ready     node          21h       v1.9.0+coreos.0\n\n\n\n\nIf you are able to see this, your cluster has been set up successfully.\n\n\n\n\n1\n You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.", 
            "title": "Production grade setup with Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#kubernetes-cluster-setup-using-kubespray", 
            "text": "Kubespray is an  Ansible  based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.", 
            "title": "Kubernetes - Cluster Setup using Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#prerequisites", 
            "text": "Hardware Pre requisites\n  * 4 Nodes: Virtual/Physical Machines\n  * Memory: 2GB each\n  * CPU: 2 cores recommended\n  * Hard disk: 20GB available  Software Pre Requisites\n  * Ubuntu 16.04 Operating System\n  * Python", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#architecture-of-a-high-available-kubernetes-cluster", 
            "text": "", 
            "title": "Architecture of a high available kubernetes cluster"
        }, 
        {
            "location": "/cluster_setup_kubespray/#preparing-the-kubernetes-nodes", 
            "text": "On control node  Ansible is the base provisioner in our cluster. But installing Ansible is out of the scope of this training. You can learn about installing and configuring Ansible from  here.", 
            "title": "Preparing the kubernetes nodes"
        }, 
        {
            "location": "/cluster_setup_kubespray/#setup-passwordless-ssh-between-control-and-kubernetes-nodes", 
            "text": "On control node  Ansible uses passwordless ssh 1  to create the cluster. Let us see how to set it up from your  control node .  ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   E    ..       |\n|  . o +..        |\n| . +o*+o         |\n|. .o+Bo+         |\n|. .++.X S        |\n|+ +ooX .         |\n|.=.OB.+ .        |\n| .=o*= . .       |\n|  .o.   .        |\n+----[SHA256]-----+  Just leave the fields to defaults. This command will generate a public key and private key for you.  cat ~/.ssh/id_rsa.pub | ssh ubuntu@10.40.1.26 'cat   ~/.ssh/authorized_keys'  This will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace  10.40.1.26  with your respective machine's IP.", 
            "title": "Setup passwordless SSH between control and kubernetes nodes"
        }, 
        {
            "location": "/cluster_setup_kubespray/#enable-ipv4-forwarding", 
            "text": "On all nodes  ssh ubuntu@10.40.1.26\nsudo su\nvim /etc/sysctl.conf  Enalbe IPv4 forwarding by uncommenting the following line  net.ipv4.ip_forward=1", 
            "title": "Enable IPv4 Forwarding"
        }, 
        {
            "location": "/cluster_setup_kubespray/#stop-and-disable-ufw-service", 
            "text": "On all nodes  Execute the following commands to stop and disable ufw service.  systemctl stop ufw.service\nsystemctl disable ufw.service", 
            "title": "Stop and Disable UFW Service"
        }, 
        {
            "location": "/cluster_setup_kubespray/#install-python", 
            "text": "On all nodes  Ansible needs python to be installed on all the machines.  sudo apt update\nsudo apt install python", 
            "title": "Install Python"
        }, 
        {
            "location": "/cluster_setup_kubespray/#configuring-ansible-control-node-and-kubespray", 
            "text": "On control node  Kubespray is hosted on GitHub. Let us the clone the  official repository .  git clone https://github.com/kubernetes-incubator/kubespray.git\ncd kubespray", 
            "title": "Configuring Ansible Control node and Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#set-remote-user-for-ansible", 
            "text": "On control node  Add the following section in ansible.cfg file  remote_user=ubuntu  Your  ansible.cfg  file should look like this.  [ssh_connection]\npipelining=True\nssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\n#control_path = ~/.ssh/ansible-%%r@%%h:%%p\n[defaults]\nhost_key_checking=False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp\nstdout_callback = skippy\nlibrary = ./library\ncallback_whitelist = profile_tasks\nroles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles\ndeprecation_warnings=False\nremote_user=ubuntu", 
            "title": "Set Remote User for Ansible"
        }, 
        {
            "location": "/cluster_setup_kubespray/#download-inventory-builder", 
            "text": "On control node  Inventory builder (a python script) helps us to create inventory file.  Inventory  file is something with which we specify the groups of masters and nodes of our cluster.  cd inventory\nwget https://raw.githubusercontent.com/kubernetes-incubator/kubespray/master/contrib/inventory_builder/inventory.py  To build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments  python inventory.py 10.40.1.26 10.40.1.25 10.40.1.20 10.40.1.11  These IPs are different for you. Please replace them with your corresponding IPs.\nThis step will result in the creation of a new file  inventory.cfg . This is our inventory file.  inventory.cfg  [all]\nnode1    ansible_host=10.40.1.26 ip=10.40.1.26\nnode2    ansible_host=10.40.1.25 ip=10.40.1.25\nnode3    ansible_host=10.40.1.20 ip=10.40.1.20\nnode4    ansible_host=10.40.1.11 ip=10.40.1.11\n\n[kube-master]\nnode1\nnode2\n\n[kube-node]\nnode1\nnode2\nnode3\nnode4\n\n[etcd]\nnode1\nnode2\nnode3\n\n[k8s-cluster:children]\nkube-node\nkube-master\n\n[calico-rr]\n\n[vault]\nnode1\nnode2\nnode3", 
            "title": "Download Inventory Builder"
        }, 
        {
            "location": "/cluster_setup_kubespray/#provisioning-kubernetes-cluster-with-kubespray", 
            "text": "On control node  We are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster.  ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v  Option -i = Inventory file path\nOption -b = Become as root user\nOption -v = Give verbose output  This Ansible run will take around 30 mins to complete.", 
            "title": "Provisioning  kubernetes cluster with kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#install-kubectl", 
            "text": "On control node  Before we proceed further, we will need to install  kubectl  binary in our control node. Read installation procedure from this  link .", 
            "title": "Install Kubectl"
        }, 
        {
            "location": "/cluster_setup_kubespray/#getting-the-kubernetes-configuration-file", 
            "text": "On control node  Once the cluster setup is done, we have to copy over the cluster config file from the master machine. We will discuss about this file extensively in the next chapter.  ssh ubuntu@10.40.1.26\nsudo su\ncp /etc/kubernetes/admin.conf /home/ubuntu\nchown ubuntu:ubuntu /home/ubuntu/admin.conf\nexit\nexit\nscp ubuntu@10.40.1.26:~/admin.conf .\ncd\nmkdir .kube\nmv admin.conf .kube/config", 
            "title": "Getting the Kubernetes Configuration File"
        }, 
        {
            "location": "/cluster_setup_kubespray/#check-the-state-of-the-cluster", 
            "text": "On control node  Let us check the state of the cluster by running,  kubectl cluster-info\n\nKubernetes master is running at https://10.40.1.26:6443\nKubeDNS is running at https://10.40.1.26:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  kubectl get nodes\n\nNAME      STATUS    ROLES         AGE       VERSION\nnode1     Ready     master,node   21h       v1.9.0+coreos.0\nnode2     Ready     master,node   21h       v1.9.0+coreos.0\nnode3     Ready     node          21h       v1.9.0+coreos.0\nnode4     Ready     node          21h       v1.9.0+coreos.0  If you are able to see this, your cluster has been set up successfully.   1  You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.", 
            "title": "Check the State of the Cluster"
        }, 
        {
            "location": "/12_troubleshooting/", 
            "text": "Troubleshooting the Kubernetes cluster\n\n\nIn this chapter we will learn about how to trouble shoot our Kubernetes cluster at \ncontrol plane\n level and at \napplication level\n.\n\n\nTroubleshooting the control plane\n\n\nListing the nodes in a cluster\n\n\nFirst thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.\n\n\nkubectl get nodes\n\n\n\n\nMake sure that all nodes are in \nReady\n state.\n\n\nList the control plane pods\n\n\nIf your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,\n\n\nkubectl get pods -n kube-system\n\n\n\n\nIf any of the pod is \nrestarting or crashing\n, look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster \nkube-dns\n is crashing. In order to fix this first check the deployment for errors.\n\n\nkubectl describe deployment -n kube-system kube-dns\n\n\n\n\nLog files\n\n\nMaster\n\n\nIf your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...\n\n\n/var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs\n\n\n\n\nIf your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the \nactual pod's name may differ\n from cluster to cluster...\n\n\nkubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1\n\n\n\n\nWorker Nodes\n\n\nIn your worker, you will need to check for errors in kubelet's log...\n\n\nsudo journalctl -u  kubelet\n\n\n\n\nTroubleshooting the application\n\n\nSometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.\n\n\nGetting detailed status of an object (pods, deployments)\n\n\nobject.status\n shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.\n\n\nExample\n\n\nkubectl get pod vote -o yaml\n\n\n\n\n\nexample output snippet when a wrong image was used to create a pod. \n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nChecking the status of Deployment\n\n\nFor this example I have a sample deployment called nginx.\n\n\nFILE: nginx-deployment.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80\n\n\n\n\nList the deployment to check for the \navailability of pods\n\n\nkubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h\n\n\n\n\nIt is clear that my pod is unavailable. Lets dig further.\n\n\nCheck the \nevents\n of your deployment.\n\n\nkubectl describe deployment nginx\n\n\n\n\nList the pods to check for any \nregistry related error\n\n\nkubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m\n\n\n\n\nAs we can see, we are not able to pull the image(\nImagePullBackOff\n). Let's investigate further.\n\n\nkubectl describe pod nginx-57c88d7bb8-c6kpc\n\n\n\n\nCheck the events of the pod's description.\n\n\nEvents:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \ndefault-token-8cwn4\n\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \nngnix\n\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \nngnix\n: rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \nngnix\n\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod\n\n\n\n\nBingo! The name of the image is \nngnix\n instead of \nnginx\n. So \nfix the typo in your deployment file and redo the deployment\n.\n\n\nSometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.\n\n\nkubectl logs -f nginx-57c88d7bb8-c6kpc\n\n\n\n\nIf you have any errors it will get populated in your logs.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-kubernetes-cluster", 
            "text": "In this chapter we will learn about how to trouble shoot our Kubernetes cluster at  control plane  level and at  application level .", 
            "title": "Troubleshooting the Kubernetes cluster"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-control-plane", 
            "text": "", 
            "title": "Troubleshooting the control plane"
        }, 
        {
            "location": "/12_troubleshooting/#listing-the-nodes-in-a-cluster", 
            "text": "First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.  kubectl get nodes  Make sure that all nodes are in  Ready  state.", 
            "title": "Listing the nodes in a cluster"
        }, 
        {
            "location": "/12_troubleshooting/#list-the-control-plane-pods", 
            "text": "If your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,  kubectl get pods -n kube-system  If any of the pod is  restarting or crashing , look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster  kube-dns  is crashing. In order to fix this first check the deployment for errors.  kubectl describe deployment -n kube-system kube-dns", 
            "title": "List the control plane pods"
        }, 
        {
            "location": "/12_troubleshooting/#log-files", 
            "text": "", 
            "title": "Log files"
        }, 
        {
            "location": "/12_troubleshooting/#master", 
            "text": "If your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...  /var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs  If your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the  actual pod's name may differ  from cluster to cluster...  kubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1", 
            "title": "Master"
        }, 
        {
            "location": "/12_troubleshooting/#worker-nodes", 
            "text": "In your worker, you will need to check for errors in kubelet's log...  sudo journalctl -u  kubelet", 
            "title": "Worker Nodes"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-application", 
            "text": "Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.", 
            "title": "Troubleshooting the application"
        }, 
        {
            "location": "/12_troubleshooting/#getting-detailed-status-of-an-object-pods-deployments", 
            "text": "object.status  shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.  Example  kubectl get pod vote -o yaml  example output snippet when a wrong image was used to create a pod.   status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248", 
            "title": "Getting detailed status of an object (pods, deployments)"
        }, 
        {
            "location": "/12_troubleshooting/#checking-the-status-of-deployment", 
            "text": "For this example I have a sample deployment called nginx.  FILE: nginx-deployment.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80  List the deployment to check for the  availability of pods  kubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h  It is clear that my pod is unavailable. Lets dig further.  Check the  events  of your deployment.  kubectl describe deployment nginx  List the pods to check for any  registry related error  kubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m  As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further.  kubectl describe pod nginx-57c88d7bb8-c6kpc  Check the events of the pod's description.  Events:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume  default-token-8cwn4 \n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image  ngnix \n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image  ngnix : rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image  ngnix \n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod  Bingo! The name of the image is  ngnix  instead of  nginx . So  fix the typo in your deployment file and redo the deployment .  Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.  kubectl logs -f nginx-57c88d7bb8-c6kpc  If you have any errors it will get populated in your logs.", 
            "title": "Checking the status of Deployment"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/", 
            "text": "Configuring Authentication and Authorization\n\n\nCreate namespace for a user(Optional)\n\n\nCreate the user credentials\n\n\nopenssl genrsa -out vibe.key 2048\nopenssl req -new -key vibe.key -out vibe.csr -subj \n/CN=vibe/O=initcron\n\n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in vibe.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out vibe.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/vibe/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials vibe --client-certificate=/home/vibe/.kube-certs/vibe.crt --client-key=/home/vibe/.kube-certs/vibe.key\nkubectl config set-context vibe-context --cluster=\nYOUR-CLUSTER-NAME\n --namespace=\nNAMESPACE-FOR-USER\n  --user=vibe\n# This step will throw an error\nkubectl --context=vibe-context get pods\n\n\n\n\nCreate role for the developers\n\n\nThese steps (3,4) has to be executed from kube-master or the controller machine\n\ndev-role.yml\n\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: developer\nrules:\n- apiGroups: [\n, \nextensions\n, \napps\n]\n  resources: [\ndeployments\n, \nreplicasets\n, \npods\n]\n  verbs: [\nget\n, \nlist\n, \nwatch\n, \ncreate\n, \nupdate\n, \npatch\n, \ndelete\n]\n\n\n\n\nkubectl create -f dev-role.yml\n\n\n\n\nCreate RoleBindings for the dev-role\n\n\ndev-rolebinding.yml\n\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: developer\n  namespace: developer\nsubjects:\n- kind: User\n  name: vibe\n  apiGroup: \n\nroleRef:\n  kind: Role\n  name: developer\n  apiGroup: \n\n\n\n\n\nkubectl create -f dev-rolebinding.yaml\n\n\n\n\nKubernetes Access Control\n\n\nIn this chapter, we will see about how to use authentication and authorisation of Kubernetes.\n\n\nHow one can access the Kubernetes API?\n\n\nThe Kubernetes API can be accessed by three ways. \n  * Kubeclt - A command line utility of Kubernetes \n  * Client libraries - Go, Python, etc., \n  * REST requests \n\n\nWho can access the Kubernetes API?\n\n\nKubernetes API can be accessed by,\n  * Human users \n  * Service Accounts \nEach of these topics will be discussed in detail in the later part of this chapter.\n\n\nStages of a Request\n\n\nWhen a request tries to contact the API , it goes through various stages as illustrated in the image given below.\n\n\n\n\nsource: official kubernetes site\n\n\nTLS in Kubernetes\n\n\nKubernetes API typically runs on two ports.\n  * 8080 \n    * This port is available only for processes running on master machine (kube-scheduler, kube-controller-manager). \n    * This port is insecure. \n  * 6443 \n    * TLS Enabled. \n    * Available for kubectl and others. \n\n\nStage 1: Authentication\n\n\n\n\nAuthentication operation checks whether the \nuser/service account\n has the permission to talk to the api server or not.\n\n\nAuthentication is done by the authentication modules which are configured with the api server.\n\n\nCluster uses with one or more authentication modules enabled.\n\n\nIf the request fails to authenticate itself, it will be served with \n401 error\n.\n\n\n\n\nAuthentication for Normal User\n\n\n\n\nKubernetes uses \nusernames\n for access control.\n\n\nBut it neither has an api object nor stores information about users in its data store.\n\n\nUsers need to be managed externally by the cluster administrator.\n\n\n\n\nAuthentication for Service Accounts\n\n\n\n\nUnlike user accounts, service accounts are managed by Kubernetes.\n\n\nservice accounts\n are bound to specific namespaces.\n\n\nCredentials for \nservice Accounts\n are stored as \nsecrets\n.\n\n\nThese secrets are mounted to pods when a deployment starts using the Service Account.\n\n\n\n\nStage 2: Authorization\n\n\n\n\nAfter a request successfully authenticated, it goes through the authorization process.\n\n\nIn order for a request to be authorized, it must consist following attributes.\n\n\nUsername of the requester(User)\n\n\nRequested action(Verb)\n\n\nThe object affected(Resource)\n\n\n\n\n\n\nAuthorization is done by the following modules. Each of these modules has a special purpose.\n\n\nAttribute Based Access Control(ABAC)\n\n\nRole Based Access Control(RBAC)\n\n\nNode Authorizer\n\n\nWebhook module\n\n\n\n\n\n\nIf a request is failed to get authorized, it will be served with \n403 error\n.\n\n\nAmong these modules, RBAC is the most used authorizer while,\n\n\nABAC is used for,\n\n\nPolicy based, fine grained access control\n\n\nThe caveat is api server has to be restarted whenever we define a ABAC policy\n\n\nNode Authorizer is,\n\n\nEnabled in all the worker nodes\n\n\nGrants access to kubelet for some of the resources.\n\n\n\n\n\n\nWe have already talked about the user in detail. Now lets focus on \nverbs\n and \nresources\n\n\nWe will talk about RBAC in detail in the later part\n\n\n\n\nVerbs\n\n\n\n\nVerbs are the \naction\n to be taken on resources.\n\n\nSome of the verbs in Kubernetes are,\n\n\nget\n\n\nlist\n\n\ncreate\n\n\nupdate\n\n\npatch\n\n\nwatch\n\n\ndelete\n\n\n\n\n\n\n\n\nResources\n\n\n\n\nResources are the object being manipulated by the verb.\n\n\nEx: pods, deployments, service, namespaces, nodes, etc.,\n\n\n\n\nStage 3: Admission Control\n\n\n\n\nAdmission control part is taken care of by the software modules that can modify/reject requests.\n\n\nAdmission control is mainly used for fine-tuning access control.\n\n\nAdmission control can directly act on the object being modified.\n\n\n\n\nRole Based Access Control (RBAC)\n\n\n\n\nRBAC is the most used form of access control to \ngrant or revoke\n permissions to users.\n\n\nIt is used for dynamically configuring policies through API.\n\n\nRBAC has two objects that are used for creating polices and another two objects that are used for implementing those policies.\n\n\nObjects that are used to create policies,\n\n\nRoles,\n\n\nClusterRoles.\n\n\n\n\n\n\nObjects that are used to implement policies,\n\n\nRoleBindings,\n\n\nClusterRoleBindings.\n\n\n\n\n\n\n\n\nRoles\n\n\n\n\nRoles grant access to resources \nwithin a single namespace\n.\n\n\nRoles cannot grant permission for global(cluster-wide) resources.\n\n\n\n\nClusterRoles\n\n\n\n\nClusterRoles works similar to Roles, but for \ncluster-wide\n resources.\n\n\n\n\nRoleBindings\n\n\n\n\nRoleBidings are used to grant permission defined in a Role to \na user or a set of users\n.\n\n\nRoleBindings can also refer to \nClusterRoles\n.\n\n\n\n\nClusrerRoleBindings\n\n\n\n\nClusterRoleBindings works same as RoleBindings, but cluster-wide.\n\n\n\n\nExample\n\n\nLet us assume a scenario, where a cluster admin was asked to add a newly joined developer, John, to the cluster. He needs to create a configuration file for John and restrict him from accessing resources from other environments.\n\n\nStep 1: Create the Namespace\n\n\nCreate the \ndev\n namespace if it has not been created already.\n\n\ndev-namespace.yml\n\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    name: dev\n\n\n\n\nkubectl apply -f dev-namespace.yml\n\n\n\n\nStep 2: Create the user credentials\n\n\nNext step is to create the credentials for John. Before proceeding further, please note that, you will need the server's ca certificate and ca key in your local machine.\n\n\nopenssl genrsa -out john.key 2048\nopenssl req -new -key vibe.key -out john.csr -subj \n/CN=john/O=my-org\n\n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in john.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out john.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/john/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials john --client-certificate=/home/john/.kube-certs/john.crt --client-key=/home/john/.kube-certs/john.key\nkubectl config set-context developer --cluster=\nYOUR-CLUSTER-NAME\n --namespace=dev  --user=dev\n# This step will throw an error\nkubectl --context=developer get pods", 
            "title": "Authentication and Authorization"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#configuring-authentication-and-authorization", 
            "text": "", 
            "title": "Configuring Authentication and Authorization"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-namespace-for-a-useroptional", 
            "text": "", 
            "title": "Create namespace for a user(Optional)"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-the-user-credentials", 
            "text": "openssl genrsa -out vibe.key 2048\nopenssl req -new -key vibe.key -out vibe.csr -subj  /CN=vibe/O=initcron \n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in vibe.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out vibe.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/vibe/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials vibe --client-certificate=/home/vibe/.kube-certs/vibe.crt --client-key=/home/vibe/.kube-certs/vibe.key\nkubectl config set-context vibe-context --cluster= YOUR-CLUSTER-NAME  --namespace= NAMESPACE-FOR-USER   --user=vibe\n# This step will throw an error\nkubectl --context=vibe-context get pods", 
            "title": "Create the user credentials"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-role-for-the-developers", 
            "text": "These steps (3,4) has to be executed from kube-master or the controller machine dev-role.yml  kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: developer\nrules:\n- apiGroups: [ ,  extensions ,  apps ]\n  resources: [ deployments ,  replicasets ,  pods ]\n  verbs: [ get ,  list ,  watch ,  create ,  update ,  patch ,  delete ]  kubectl create -f dev-role.yml", 
            "title": "Create role for the developers"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-rolebindings-for-the-dev-role", 
            "text": "dev-rolebinding.yml  kind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: developer\n  namespace: developer\nsubjects:\n- kind: User\n  name: vibe\n  apiGroup:  \nroleRef:\n  kind: Role\n  name: developer\n  apiGroup:    kubectl create -f dev-rolebinding.yaml", 
            "title": "Create RoleBindings for the dev-role"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#kubernetes-access-control", 
            "text": "In this chapter, we will see about how to use authentication and authorisation of Kubernetes.", 
            "title": "Kubernetes Access Control"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#how-one-can-access-the-kubernetes-api", 
            "text": "The Kubernetes API can be accessed by three ways. \n  * Kubeclt - A command line utility of Kubernetes \n  * Client libraries - Go, Python, etc., \n  * REST requests", 
            "title": "How one can access the Kubernetes API?"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#who-can-access-the-kubernetes-api", 
            "text": "Kubernetes API can be accessed by,\n  * Human users \n  * Service Accounts \nEach of these topics will be discussed in detail in the later part of this chapter.", 
            "title": "Who can access the Kubernetes API?"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stages-of-a-request", 
            "text": "When a request tries to contact the API , it goes through various stages as illustrated in the image given below.   source: official kubernetes site", 
            "title": "Stages of a Request"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#tls-in-kubernetes", 
            "text": "Kubernetes API typically runs on two ports.\n  * 8080 \n    * This port is available only for processes running on master machine (kube-scheduler, kube-controller-manager). \n    * This port is insecure. \n  * 6443 \n    * TLS Enabled. \n    * Available for kubectl and others.", 
            "title": "TLS in Kubernetes"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stage-1-authentication", 
            "text": "Authentication operation checks whether the  user/service account  has the permission to talk to the api server or not.  Authentication is done by the authentication modules which are configured with the api server.  Cluster uses with one or more authentication modules enabled.  If the request fails to authenticate itself, it will be served with  401 error .", 
            "title": "Stage 1: Authentication"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#authentication-for-normal-user", 
            "text": "Kubernetes uses  usernames  for access control.  But it neither has an api object nor stores information about users in its data store.  Users need to be managed externally by the cluster administrator.", 
            "title": "Authentication for Normal User"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#authentication-for-service-accounts", 
            "text": "Unlike user accounts, service accounts are managed by Kubernetes.  service accounts  are bound to specific namespaces.  Credentials for  service Accounts  are stored as  secrets .  These secrets are mounted to pods when a deployment starts using the Service Account.", 
            "title": "Authentication for Service Accounts"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stage-2-authorization", 
            "text": "After a request successfully authenticated, it goes through the authorization process.  In order for a request to be authorized, it must consist following attributes.  Username of the requester(User)  Requested action(Verb)  The object affected(Resource)    Authorization is done by the following modules. Each of these modules has a special purpose.  Attribute Based Access Control(ABAC)  Role Based Access Control(RBAC)  Node Authorizer  Webhook module    If a request is failed to get authorized, it will be served with  403 error .  Among these modules, RBAC is the most used authorizer while,  ABAC is used for,  Policy based, fine grained access control  The caveat is api server has to be restarted whenever we define a ABAC policy  Node Authorizer is,  Enabled in all the worker nodes  Grants access to kubelet for some of the resources.    We have already talked about the user in detail. Now lets focus on  verbs  and  resources  We will talk about RBAC in detail in the later part", 
            "title": "Stage 2: Authorization"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#verbs", 
            "text": "Verbs are the  action  to be taken on resources.  Some of the verbs in Kubernetes are,  get  list  create  update  patch  watch  delete", 
            "title": "Verbs"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#resources", 
            "text": "Resources are the object being manipulated by the verb.  Ex: pods, deployments, service, namespaces, nodes, etc.,", 
            "title": "Resources"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stage-3-admission-control", 
            "text": "Admission control part is taken care of by the software modules that can modify/reject requests.  Admission control is mainly used for fine-tuning access control.  Admission control can directly act on the object being modified.", 
            "title": "Stage 3: Admission Control"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#role-based-access-control-rbac", 
            "text": "RBAC is the most used form of access control to  grant or revoke  permissions to users.  It is used for dynamically configuring policies through API.  RBAC has two objects that are used for creating polices and another two objects that are used for implementing those policies.  Objects that are used to create policies,  Roles,  ClusterRoles.    Objects that are used to implement policies,  RoleBindings,  ClusterRoleBindings.", 
            "title": "Role Based Access Control (RBAC)"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#roles", 
            "text": "Roles grant access to resources  within a single namespace .  Roles cannot grant permission for global(cluster-wide) resources.", 
            "title": "Roles"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#clusterroles", 
            "text": "ClusterRoles works similar to Roles, but for  cluster-wide  resources.", 
            "title": "ClusterRoles"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#rolebindings", 
            "text": "RoleBidings are used to grant permission defined in a Role to  a user or a set of users .  RoleBindings can also refer to  ClusterRoles .", 
            "title": "RoleBindings"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#clusrerrolebindings", 
            "text": "ClusterRoleBindings works same as RoleBindings, but cluster-wide.", 
            "title": "ClusrerRoleBindings"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#example", 
            "text": "Let us assume a scenario, where a cluster admin was asked to add a newly joined developer, John, to the cluster. He needs to create a configuration file for John and restrict him from accessing resources from other environments.", 
            "title": "Example"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#step-1-create-the-namespace", 
            "text": "Create the  dev  namespace if it has not been created already.  dev-namespace.yml  apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    name: dev  kubectl apply -f dev-namespace.yml", 
            "title": "Step 1: Create the Namespace"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#step-2-create-the-user-credentials", 
            "text": "Next step is to create the credentials for John. Before proceeding further, please note that, you will need the server's ca certificate and ca key in your local machine.  openssl genrsa -out john.key 2048\nopenssl req -new -key vibe.key -out john.csr -subj  /CN=john/O=my-org \n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in john.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out john.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/john/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials john --client-certificate=/home/john/.kube-certs/john.crt --client-key=/home/john/.kube-certs/john.key\nkubectl config set-context developer --cluster= YOUR-CLUSTER-NAME  --namespace=dev  --user=dev\n# This step will throw an error\nkubectl --context=developer get pods", 
            "title": "Step 2: Create the user credentials"
        }
    ]
}