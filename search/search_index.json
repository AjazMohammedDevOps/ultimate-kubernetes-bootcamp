{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Kubernetes Fundamentals by School of Devops\n\n\nThis is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.com\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy\n\n\nVenkat", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-kubernetes-fundamentals-by-school-of-devops", 
            "text": "This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.com .", 
            "title": "Welcome to Kubernetes Fundamentals by School of Devops"
        }, 
        {
            "location": "/#team", 
            "text": "Gourav Shah  Vijayboopathy  Venkat", 
            "title": "Team"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/", 
            "text": "Install VirtualBox and Vagrant\n\n\n\n\n\n\n\n\nTOOL\n\n\nVERSION\n\n\nLINK\n\n\n\n\n\n\n\n\n\n\nVirtualBox\n\n\n5.1.26\n\n\nhttps://www.virtualbox.org/wiki/Downloads\n\n\n\n\n\n\nVagrant\n\n\n1.9.7\n\n\nhttps://www.vagrantup.com/downloads.html\n\n\n\n\n\n\n\n\nImporting a VM Template\n\n\nIf you have already copied/downloaded the box file \nubuntu-xenial64.box\n, go to the directory which contains that file. If you do not have a box file, skip to next section.\n\n\nvagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list\n\n\n\n\n\nProvisioning Vagrant Nodes\n\n\nClone repo if not already\n\n\ngit clone https://github.com/schoolofdevops/lab-setup.git\n\n\n\n\n\n\nLaunch environments with Vagrant\n\n\ncd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up\n\n\n\n\n\nLogin to nodes\n\n\nOpen three different terminals to login to 3 nodes created with above command\n\n\nTerminal 1\n\n\nvagrant ssh kube-01\nsudo su\n\n\n\n\n\nTerminal 2\n\n\nvagrant ssh kube-02\nsudo su\n\n\n\n\nTerminal 3\n\n\nvagrant ssh kube-03\nsudo su\n\n\n\n\nOnce the environment is setup, follow \nInitialization of Master\n onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning VMs with Vagrant"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#install-virtualbox-and-vagrant", 
            "text": "TOOL  VERSION  LINK      VirtualBox  5.1.26  https://www.virtualbox.org/wiki/Downloads    Vagrant  1.9.7  https://www.vagrantup.com/downloads.html", 
            "title": "Install VirtualBox and Vagrant"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#importing-a-vm-template", 
            "text": "If you have already copied/downloaded the box file  ubuntu-xenial64.box , go to the directory which contains that file. If you do not have a box file, skip to next section.  vagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list", 
            "title": "Importing a VM Template"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#provisioning-vagrant-nodes", 
            "text": "Clone repo if not already  git clone https://github.com/schoolofdevops/lab-setup.git  Launch environments with Vagrant  cd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up  Login to nodes  Open three different terminals to login to 3 nodes created with above command  Terminal 1  vagrant ssh kube-01\nsudo su  Terminal 2  vagrant ssh kube-02\nsudo su  Terminal 3  vagrant ssh kube-03\nsudo su  Once the environment is setup, follow  Initialization of Master  onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning Vagrant Nodes"
        }, 
        {
            "location": "/3_install_kubernetes/", 
            "text": "Compatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicable for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup\n\n\n Skip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described here,\n\n\nCreate Kubernetes Repository\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat \nEOF \n /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01\n\n\nkubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\nCopy and paste it on all node.\n\n\nTroubleshooting Tips\n\n\nIf you lose  the join token, you could retrieve it using\n\n\nkubeadm token list\n\n\n\n\nOn successfully joining the master, you should see output similar to following,\n\n\nroot@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \n159.203.170.84:6443\n\n[discovery] Created cluster-info discovery client, requesting info from \nhttps://159.203.170.84:6443\n\n[discovery] Requesting info from \nhttps://159.203.170.84:6443\n again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \n159.203.170.84:6443\n\n[discovery] Successfully established connection with API Server \n159.203.170.84:6443\n\n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n\n\n\n\nSetup the admin client - Kubectl\n\n\nOn Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nInstalling CNI with Weave\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nWeave Net\n.\n\n\n\nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f \nhttps://cloud.weave.works/k8s/net?k8s-version=$kubever\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     \nnone\n    4m        v1.8.2\nkube-03   Ready     \nnone\n    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nPossible Issues\n\n\n\n\nNodes are node in Ready status\n\n\nkube-dns is crashing constantly\n\n\nSome of the systems services are not up\n\n\n\n\nMost of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,\n\n\nTroubleshooting Tips\n\n\nCheck events\n\n\nkubectl get events\n\n\n\n\nCheck Logs\n\n\nkubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs \npod\n -n kube-system\n\n\n\n\n\ne.g.\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....\n\n\n\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nTo access the Dashboard in th browser, run the below command\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\n\n\n\n\nSample output:\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None\n\n\n\n\nNow check for the node port, here it is 32756, and go to the browser,\n\n\nmasterip:32756\n\n\n\n\nThe Dashboard Looks like:\n\n\n\n\nCheck out the supporting code\n\n\nBefore we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.\n\n\ngit clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Setup Kubernetes Cluster"
        }, 
        {
            "location": "/3_install_kubernetes/#compatibility", 
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicable for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04  Xenial", 
            "title": "Compatibility"
        }, 
        {
            "location": "/3_install_kubernetes/#base-setup", 
            "text": "Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described here,", 
            "title": "Base Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#create-kubernetes-repository", 
            "text": "We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat  EOF   /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF", 
            "title": "Create Kubernetes Repository"
        }, 
        {
            "location": "/3_install_kubernetes/#installation-of-the-packages", 
            "text": "We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*", 
            "title": "Installation of the packages"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-sysctl-configs", 
            "text": "In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.", 
            "title": "Setup sysctl configs"
        }, 
        {
            "location": "/3_install_kubernetes/#initializing-master", 
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01  kubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16", 
            "title": "Initializing Master"
        }, 
        {
            "location": "/3_install_kubernetes/#initialization-of-the-nodes-previously-minions", 
            "text": "After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  Copy and paste it on all node.", 
            "title": "Initialization of the Nodes (Previously Minions)"
        }, 
        {
            "location": "/3_install_kubernetes/#troubleshooting-tips", 
            "text": "If you lose  the join token, you could retrieve it using  kubeadm token list  On successfully joining the master, you should see output similar to following,  root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server  159.203.170.84:6443 \n[discovery] Created cluster-info discovery client, requesting info from  https://159.203.170.84:6443 \n[discovery] Requesting info from  https://159.203.170.84:6443  again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server  159.203.170.84:6443 \n[discovery] Successfully established connection with API Server  159.203.170.84:6443 \n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-the-admin-client-kubectl", 
            "text": "On Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Setup the admin client - Kubectl"
        }, 
        {
            "location": "/3_install_kubernetes/#installing-cni-with-weave", 
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Weave Net .  \nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f  https://cloud.weave.works/k8s/net?k8s-version=$kubever", 
            "title": "Installing CNI with Weave"
        }, 
        {
            "location": "/3_install_kubernetes/#validating-the-setup", 
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready      none     4m        v1.8.2\nkube-03   Ready      none     4m        v1.8.2  Additional Status Commands  kubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.", 
            "title": "Validating the Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#possible-issues", 
            "text": "Nodes are node in Ready status  kube-dns is crashing constantly  Some of the systems services are not up   Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,  Troubleshooting Tips  Check events  kubectl get events  Check Logs  kubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs  pod  -n kube-system  e.g.  root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....", 
            "title": "Possible Issues"
        }, 
        {
            "location": "/3_install_kubernetes/#enable-kubernetes-dashboard", 
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  To access the Dashboard in th browser, run the below command  kubectl describe svc kubernetes-dashboard -n kube-system  Sample output:  kubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                    unset  80/TCP\nNodePort:                unset  32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None  Now check for the node port, here it is 32756, and go to the browser,  masterip:32756  The Dashboard Looks like:", 
            "title": "Enable Kubernetes Dashboard"
        }, 
        {
            "location": "/3_install_kubernetes/#check-out-the-supporting-code", 
            "text": "Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.  git clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Check out the supporting code"
        }, 
        {
            "location": "/4_configs/", 
            "text": "In this lesson we are going to cover the following topics\n\n\n\n\nSetting up monitors\n\n\nConfigs\n\n\nContext\n\n\nNamespaces\n\n\n\n\nSetup monitoring console for Kubernetes\n\n\nUnix \nscreen\n is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a \nscreenrc\n script as follows on kube-01 node (node where \nkubectl\n is configured)\n\n\nfile: k8s-code/monitoring/deploy.screenrc\n\n\nscreen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get deploy\nsplit\nfocus down\nscreen watch -n 1 kubectl get services\nsplit\nfocus down\nscreen watch -n 1 kubectl get replicasets\nfocus bottom\n\n\n\n\nOpen a dedicated terminal to run this utility.  Launch it using\n\n\nscreen -c monitoring/deploy.screenrc\n\n\n\n\n\nWorking with Unix Screen\n\n\nTo detach\n\n\n^a d  \n\n\n\n\nTo list existing screen sessions\n\n\nscreen -ls\n\n\n\n\nTo re attach\n\n\nscreen  -x \nsession_id\n\n\n\n\n\nTo delete a session\n\n\nscreen -X -S \nsession_id\n quit\n\n\n\n\ne.g.\n\n\nscreen -ls\nThere are screens on:\n    26484.pts-2.kube-01 (01/12/2018 06:47:41 AM)    (Detached)\n    18472.pts-2.kube-01 (01/12/2018 06:43:21 AM)    (Detached)\n2 Sockets in /var/run/screen/S-root.\n\nscreen -X -S 26 quit\n\n\n\n\nListing Configurations\n\n\nCheck current config\n\n\nkubectl config view\n\n\n\n\nYou could also examine the current configs in file \ncat ~/.kube/config\n\n\nCreating a namespace for instavote project\n\n\nNamespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.   \n\n\nLets create a namespace called \ninstavote\n  \n\n\nfile: k8s-code/projects/instavote/instavote-ns.yaml\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n\n\n\n\nTo create namespace\n\n\ncd k8s-code/projects/instavote\nkubectl get ns\nkubectl apply -f dev_instavote.yaml\nkubectl get ns\n\n\n\n\nAnd switch to it\n\n\nkubectl config set-context $(kubectl config current-context) --namespace=instavote", 
            "title": "Configuring Cluster"
        }, 
        {
            "location": "/4_configs/#setup-monitoring-console-for-kubernetes", 
            "text": "Unix  screen  is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a  screenrc  script as follows on kube-01 node (node where  kubectl  is configured)  file: k8s-code/monitoring/deploy.screenrc  screen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get deploy\nsplit\nfocus down\nscreen watch -n 1 kubectl get services\nsplit\nfocus down\nscreen watch -n 1 kubectl get replicasets\nfocus bottom  Open a dedicated terminal to run this utility.  Launch it using  screen -c monitoring/deploy.screenrc", 
            "title": "Setup monitoring console for Kubernetes"
        }, 
        {
            "location": "/4_configs/#working-with-unix-screen", 
            "text": "To detach  ^a d    To list existing screen sessions  screen -ls  To re attach  screen  -x  session_id   To delete a session  screen -X -S  session_id  quit  e.g.  screen -ls\nThere are screens on:\n    26484.pts-2.kube-01 (01/12/2018 06:47:41 AM)    (Detached)\n    18472.pts-2.kube-01 (01/12/2018 06:43:21 AM)    (Detached)\n2 Sockets in /var/run/screen/S-root.\n\nscreen -X -S 26 quit", 
            "title": "Working with Unix Screen"
        }, 
        {
            "location": "/4_configs/#listing-configurations", 
            "text": "Check current config  kubectl config view  You could also examine the current configs in file  cat ~/.kube/config", 
            "title": "Listing Configurations"
        }, 
        {
            "location": "/4_configs/#creating-a-namespace-for-instavote-project", 
            "text": "Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.     Lets create a namespace called  instavote     file: k8s-code/projects/instavote/instavote-ns.yaml  kind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote  To create namespace  cd k8s-code/projects/instavote\nkubectl get ns\nkubectl apply -f dev_instavote.yaml\nkubectl get ns  And switch to it  kubectl config set-context $(kubectl config current-context) --namespace=instavote", 
            "title": "Creating a namespace for instavote project"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/", 
            "text": "Configuring Authentication and Authorization\n\n\nCreate namespace for a user(Optional)\n\n\nCreate the user credentials\n\n\nopenssl genrsa -out vibe.key 2048\nopenssl req -new -key vibe.key -out vibe.csr -subj \n/CN=vibe/O=initcron\n\n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in vibe.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out vibe.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/vibe/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials vibe --client-certificate=/home/vibe/.kube-certs/vibe.crt --client-key=/home/vibe/.kube-certs/vibe.key\nkubectl config set-context vibe-context --cluster=\nYOUR-CLUSTER-NAME\n --namespace=\nNAMESPACE-FOR-USER\n  --user=vibe\n# This step will throw an error\nkubectl --context=vibe-context get pods\n\n\n\n\nCreate role for the developers\n\n\nThese steps (3,4) has to be executed from kube-master or the controller machine\n\ndev-role.yml\n\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: developer\nrules:\n- apiGroups: [\n, \nextensions\n, \napps\n]\n  resources: [\ndeployments\n, \nreplicasets\n, \npods\n]\n  verbs: [\nget\n, \nlist\n, \nwatch\n, \ncreate\n, \nupdate\n, \npatch\n, \ndelete\n]\n\n\n\n\nkubectl create -f dev-role.yml\n\n\n\n\nCreate RoleBindings for the dev-role\n\n\ndev-rolebinding.yml\n\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: developer\n  namespace: developer\nsubjects:\n- kind: User\n  name: vibe\n  apiGroup: \n\nroleRef:\n  kind: Role\n  name: developer\n  apiGroup: \n\n\n\n\n\nkubectl create -f dev-rolebinding.yaml\n\n\n\n\nKubernetes Access Control\n\n\nIn this chapter, we will see about how to use authentication and authorisation of Kubernetes.\n\n\nHow one can access the Kubernetes API?\n\n\nThe Kubernetes API can be accessed by three ways.\n  * Kubeclt - A command line utility of Kubernetes\n  * Client libraries - Go, Python, etc.,\n  * REST requests\n\n\nWho can access the Kubernetes API?\n\n\nKubernetes API can be accessed by,\n  * Human users\n  * Service Accounts\nEach of these topics will be discussed in detail in the later part of this chapter.\n\n\nStages of a Request\n\n\nWhen a request tries to contact the API , it goes through various stages as illustrated in the image given below.\n\n\n\n\nsource: official kubernetes site\n\n\nTLS in Kubernetes\n\n\nKubernetes API typically runs on two ports.\n  * 8080\n    * This port is available only for processes running on master machine (kube-scheduler, kube-controller-manager).\n    * This port is insecure.\n  * 6443\n    * TLS Enabled.\n    * Available for kubectl and others.\n\n\nStage 1: Authentication\n\n\n\n\nAuthentication operation checks whether the \nuser/service account\n has the permission to talk to the api server or not.\n\n\nAuthentication is done by the authentication modules which are configured with the api server.\n\n\nCluster uses with one or more authentication modules enabled.\n\n\nIf the request fails to authenticate itself, it will be served with \n401 error\n.\n\n\n\n\nAuthentication for Normal User\n\n\n\n\nKubernetes uses \nusernames\n for access control.\n\n\nBut it neither has an api object nor stores information about users in its data store.\n\n\nUsers need to be managed externally by the cluster administrator.\n\n\n\n\nAuthentication for Service Accounts\n\n\n\n\nUnlike user accounts, service accounts are managed by Kubernetes.\n\n\nservice accounts\n are bound to specific namespaces.\n\n\nCredentials for \nservice Accounts\n are stored as \nsecrets\n.\n\n\nThese secrets are mounted to pods when a deployment starts using the Service Account.\n\n\n\n\nStage 2: Authorization\n\n\n\n\nAfter a request successfully authenticated, it goes through the authorization process.\n\n\nIn order for a request to be authorized, it must consist following attributes.\n\n\nUsername of the requester(User)\n\n\nRequested action(Verb)\n\n\nThe object affected(Resource)\n\n\n\n\n\n\nAuthorization is done by the following modules. Each of these modules has a special purpose.\n\n\nAttribute Based Access Control(ABAC)\n\n\nRole Based Access Control(RBAC)\n\n\nNode Authorizer\n\n\nWebhook module\n\n\n\n\n\n\nIf a request is failed to get authorized, it will be served with \n403 error\n.\n\n\nAmong these modules, RBAC is the most used authorizer while,\n\n\nABAC is used for,\n\n\nPolicy based, fine grained access control\n\n\nThe caveat is api server has to be restarted whenever we define a ABAC policy\n\n\nNode Authorizer is,\n\n\nEnabled in all the worker nodes\n\n\nGrants access to kubelet for some of the resources.\n\n\n\n\n\n\nWe have already talked about the user in detail. Now lets focus on \nverbs\n and \nresources\n\n\nWe will talk about RBAC in detail in the later part\n\n\n\n\nVerbs\n\n\n\n\nVerbs are the \naction\n to be taken on resources.\n\n\nSome of the verbs in Kubernetes are,\n\n\nget\n\n\nlist\n\n\ncreate\n\n\nupdate\n\n\npatch\n\n\nwatch\n\n\ndelete\n\n\n\n\n\n\n\n\nResources\n\n\n\n\nResources are the object being manipulated by the verb.\n\n\nEx: pods, deployments, service, namespaces, nodes, etc.,\n\n\n\n\nStage 3: Admission Control\n\n\n\n\nAdmission control part is taken care of by the software modules that can modify/reject requests.\n\n\nAdmission control is mainly used for fine-tuning access control.\n\n\nAdmission control can directly act on the object being modified.\n\n\n\n\nRole Based Access Control (RBAC)\n\n\n\n\nRBAC is the most used form of access control to \ngrant or revoke\n permissions to users.\n\n\nIt is used for dynamically configuring policies through API.\n\n\nRBAC has two objects that are used for creating polices and another two objects that are used for implementing those policies.\n\n\nObjects that are used to create policies,\n\n\nRoles,\n\n\nClusterRoles.\n\n\n\n\n\n\nObjects that are used to implement policies,\n\n\nRoleBindings,\n\n\nClusterRoleBindings.\n\n\n\n\n\n\n\n\nRoles\n\n\n\n\nRoles grant access to resources \nwithin a single namespace\n.\n\n\nRoles cannot grant permission for global(cluster-wide) resources.\n\n\n\n\nClusterRoles\n\n\n\n\nClusterRoles works similar to Roles, but for \ncluster-wide\n resources.\n\n\n\n\nRoleBindings\n\n\n\n\nRoleBidings are used to grant permission defined in a Role to \na user or a set of users\n.\n\n\nRoleBindings can also refer to \nClusterRoles\n.\n\n\n\n\nClusrerRoleBindings\n\n\n\n\nClusterRoleBindings works same as RoleBindings, but cluster-wide.\n\n\n\n\nExample\n\n\nLet us assume a scenario, where a cluster admin was asked to add a newly joined developer, John, to the cluster. He needs to create a configuration file for John and restrict him from accessing resources from other environments.\n\n\nStep 1: Create the Namespace\n\n\nCreate the \ndev\n namespace if it has not been created already.\n\n\ndev-namespace.yml\n\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    name: dev\n\n\n\n\nkubectl apply -f dev-namespace.yml\n\n\n\n\nStep 2: Create the user credentials\n\n\nNext step is to create the credentials for John. Before proceeding further, please note that, you will need the server's ca certificate and ca key in your local machine.\n\n\nopenssl genrsa -out john.key 2048\nopenssl req -new -key vibe.key -out john.csr -subj \n/CN=john/O=my-org\n\n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in john.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out john.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/john/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials john --client-certificate=/home/john/.kube-certs/john.crt --client-key=/home/john/.kube-certs/john.key\nkubectl config set-context developer --cluster=\nYOUR-CLUSTER-NAME\n --namespace=dev  --user=dev\n# This step will throw an error\nkubectl --context=developer get pods", 
            "title": "Kubernetes RBAC"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#configuring-authentication-and-authorization", 
            "text": "", 
            "title": "Configuring Authentication and Authorization"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-namespace-for-a-useroptional", 
            "text": "", 
            "title": "Create namespace for a user(Optional)"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-the-user-credentials", 
            "text": "openssl genrsa -out vibe.key 2048\nopenssl req -new -key vibe.key -out vibe.csr -subj  /CN=vibe/O=initcron \n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in vibe.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out vibe.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/vibe/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials vibe --client-certificate=/home/vibe/.kube-certs/vibe.crt --client-key=/home/vibe/.kube-certs/vibe.key\nkubectl config set-context vibe-context --cluster= YOUR-CLUSTER-NAME  --namespace= NAMESPACE-FOR-USER   --user=vibe\n# This step will throw an error\nkubectl --context=vibe-context get pods", 
            "title": "Create the user credentials"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-role-for-the-developers", 
            "text": "These steps (3,4) has to be executed from kube-master or the controller machine dev-role.yml  kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: developer\nrules:\n- apiGroups: [ ,  extensions ,  apps ]\n  resources: [ deployments ,  replicasets ,  pods ]\n  verbs: [ get ,  list ,  watch ,  create ,  update ,  patch ,  delete ]  kubectl create -f dev-role.yml", 
            "title": "Create role for the developers"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#create-rolebindings-for-the-dev-role", 
            "text": "dev-rolebinding.yml  kind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: developer\n  namespace: developer\nsubjects:\n- kind: User\n  name: vibe\n  apiGroup:  \nroleRef:\n  kind: Role\n  name: developer\n  apiGroup:    kubectl create -f dev-rolebinding.yaml", 
            "title": "Create RoleBindings for the dev-role"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#kubernetes-access-control", 
            "text": "In this chapter, we will see about how to use authentication and authorisation of Kubernetes.", 
            "title": "Kubernetes Access Control"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#how-one-can-access-the-kubernetes-api", 
            "text": "The Kubernetes API can be accessed by three ways.\n  * Kubeclt - A command line utility of Kubernetes\n  * Client libraries - Go, Python, etc.,\n  * REST requests", 
            "title": "How one can access the Kubernetes API?"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#who-can-access-the-kubernetes-api", 
            "text": "Kubernetes API can be accessed by,\n  * Human users\n  * Service Accounts\nEach of these topics will be discussed in detail in the later part of this chapter.", 
            "title": "Who can access the Kubernetes API?"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stages-of-a-request", 
            "text": "When a request tries to contact the API , it goes through various stages as illustrated in the image given below.   source: official kubernetes site", 
            "title": "Stages of a Request"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#tls-in-kubernetes", 
            "text": "Kubernetes API typically runs on two ports.\n  * 8080\n    * This port is available only for processes running on master machine (kube-scheduler, kube-controller-manager).\n    * This port is insecure.\n  * 6443\n    * TLS Enabled.\n    * Available for kubectl and others.", 
            "title": "TLS in Kubernetes"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stage-1-authentication", 
            "text": "Authentication operation checks whether the  user/service account  has the permission to talk to the api server or not.  Authentication is done by the authentication modules which are configured with the api server.  Cluster uses with one or more authentication modules enabled.  If the request fails to authenticate itself, it will be served with  401 error .", 
            "title": "Stage 1: Authentication"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#authentication-for-normal-user", 
            "text": "Kubernetes uses  usernames  for access control.  But it neither has an api object nor stores information about users in its data store.  Users need to be managed externally by the cluster administrator.", 
            "title": "Authentication for Normal User"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#authentication-for-service-accounts", 
            "text": "Unlike user accounts, service accounts are managed by Kubernetes.  service accounts  are bound to specific namespaces.  Credentials for  service Accounts  are stored as  secrets .  These secrets are mounted to pods when a deployment starts using the Service Account.", 
            "title": "Authentication for Service Accounts"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stage-2-authorization", 
            "text": "After a request successfully authenticated, it goes through the authorization process.  In order for a request to be authorized, it must consist following attributes.  Username of the requester(User)  Requested action(Verb)  The object affected(Resource)    Authorization is done by the following modules. Each of these modules has a special purpose.  Attribute Based Access Control(ABAC)  Role Based Access Control(RBAC)  Node Authorizer  Webhook module    If a request is failed to get authorized, it will be served with  403 error .  Among these modules, RBAC is the most used authorizer while,  ABAC is used for,  Policy based, fine grained access control  The caveat is api server has to be restarted whenever we define a ABAC policy  Node Authorizer is,  Enabled in all the worker nodes  Grants access to kubelet for some of the resources.    We have already talked about the user in detail. Now lets focus on  verbs  and  resources  We will talk about RBAC in detail in the later part", 
            "title": "Stage 2: Authorization"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#verbs", 
            "text": "Verbs are the  action  to be taken on resources.  Some of the verbs in Kubernetes are,  get  list  create  update  patch  watch  delete", 
            "title": "Verbs"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#resources", 
            "text": "Resources are the object being manipulated by the verb.  Ex: pods, deployments, service, namespaces, nodes, etc.,", 
            "title": "Resources"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stage-3-admission-control", 
            "text": "Admission control part is taken care of by the software modules that can modify/reject requests.  Admission control is mainly used for fine-tuning access control.  Admission control can directly act on the object being modified.", 
            "title": "Stage 3: Admission Control"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#role-based-access-control-rbac", 
            "text": "RBAC is the most used form of access control to  grant or revoke  permissions to users.  It is used for dynamically configuring policies through API.  RBAC has two objects that are used for creating polices and another two objects that are used for implementing those policies.  Objects that are used to create policies,  Roles,  ClusterRoles.    Objects that are used to implement policies,  RoleBindings,  ClusterRoleBindings.", 
            "title": "Role Based Access Control (RBAC)"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#roles", 
            "text": "Roles grant access to resources  within a single namespace .  Roles cannot grant permission for global(cluster-wide) resources.", 
            "title": "Roles"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#clusterroles", 
            "text": "ClusterRoles works similar to Roles, but for  cluster-wide  resources.", 
            "title": "ClusterRoles"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#rolebindings", 
            "text": "RoleBidings are used to grant permission defined in a Role to  a user or a set of users .  RoleBindings can also refer to  ClusterRoles .", 
            "title": "RoleBindings"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#clusrerrolebindings", 
            "text": "ClusterRoleBindings works same as RoleBindings, but cluster-wide.", 
            "title": "ClusrerRoleBindings"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#example", 
            "text": "Let us assume a scenario, where a cluster admin was asked to add a newly joined developer, John, to the cluster. He needs to create a configuration file for John and restrict him from accessing resources from other environments.", 
            "title": "Example"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#step-1-create-the-namespace", 
            "text": "Create the  dev  namespace if it has not been created already.  dev-namespace.yml  apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    name: dev  kubectl apply -f dev-namespace.yml", 
            "title": "Step 1: Create the Namespace"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#step-2-create-the-user-credentials", 
            "text": "Next step is to create the credentials for John. Before proceeding further, please note that, you will need the server's ca certificate and ca key in your local machine.  openssl genrsa -out john.key 2048\nopenssl req -new -key vibe.key -out john.csr -subj  /CN=john/O=my-org \n#copy over all key files from /etc/kubernetes directory of master when you run this command\nopenssl x509 -req -in john.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out john.crt -days 500\n# save these newly generated keys in a secure directory of the user's system (/home/john/.kube-certs)\n# Execute the following commands on the users system\nkubectl config set-credentials john --client-certificate=/home/john/.kube-certs/john.crt --client-key=/home/john/.kube-certs/john.key\nkubectl config set-context developer --cluster= YOUR-CLUSTER-NAME  --namespace=dev  --user=dev\n# This step will throw an error\nkubectl --context=developer get pods", 
            "title": "Step 2: Create the user credentials"
        }, 
        {
            "location": "/5_deploying_pods/", 
            "text": "Deploying Pods\n\n\nLife of a pod\n\n\n\n\nPending : in progress\n\n\nRunning\n\n\nSucceeded : successfully exited\n\n\nFailed\n\n\nUnknown\n\n\n\n\nProbes\n\n\n\n\nlivenessProbe : Containers are Alive\n\n\nreadinessProbe : Ready to Serve Traffic\n\n\n\n\nResource Configs\n\n\nEach entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.\n\n\nAKMS\n =\n Resource Configs Specs\n\n\napiVersion: v1\nkind:\nmetadata:\nspec:\n\n\n\n\nSpec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/\n\n\nTo list supported version of apis\n\n\nkubectl api-versions\n\n\n\n\nCommon Configurations\n\n\nThroughout this tutorial, we would be deploying different components of  example voting application. Lets assume we are deploying it in a \ndev\n environment. Lets create the common specs for this app with the AKMS schema discussed above.\n\n\nfile: common.yml\n\n\napiVersion: v1\nkind:\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n\n\n\n\nLets now create the  Pod config by adding the kind and specs to above schema.\n\n\nFilename: k8s-code/pods/vote-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n\n\n\n\nUse this link to refer to pod spec\n\n\nLaunching and operating a Pod\n\n\nSyntax:\n\n\n kubectl apply -f FILE\n\n\n\n\nTo Launch pod using configs above,\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\n\nTo view pods\n\n\nkubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote\n\n\n\n\nTo get detailed info\n\n\nkubectl describe pods vote\n\n\n\n\n[Output:]\n\n\nName:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:    \nnone\n\nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:      \nnone\n\nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:    \nnone\n\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image \nschoolofdevops/vote:latest\n\n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image \nschoolofdevops/vote:latest\n\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945\n\n\n\n\nCommands to operate the pod\n\n\nkubectl exec -it vote ps sh\n\nkubectl exec -it vote  sh\n\nkubectl logs vote\n\n\n\n\n\nTroubleshooting Tip\n\n\nIf you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.\n\n\nkubectl get pod vote -o yaml\n\n\n\n\nLets learn by example. Update pod spec and change the image to something that does not exist.\n\n\nkubectl edit pod vote\n\n\n\n\nThis will open a editor. Go to the line which defines image  and change it to a tag that does not exist\n\n\ne.g.\n\n\nspec:\n  containers:\n  - image: schoolofdevops/vote:latst\n    imagePullPolicy: Always\n\n\n\n\nwhere tag \nlatst\n does not exist. As soon as you save this file, kubernetes will apply the change.\n\n\nNow check the status,\n\n\nkubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nvote      0/1       ImagePullBackOff   0          27m\n\n\n\n\nThe above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.\n\n\nObserve the \nstatus\n field.  \n\n\nkubectl get pod vote -o yaml\n\n\n\n\nNow the status field shows a detailed information, including what the exact error. Observe the following snippet...\n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nThis will help you to pinpoint to the exact cause and fix it quickly.\n\n\nNow that you  are done experimenting with pod, delete it with the following command,\n\n\nkubectl delete pod vote\n\nkubectl get pods\n\n\n\n\nAttach a Volume to the Pod\n\n\nLets create a pod for database and attach a volume to it. To achieve this we will need to\n\n\n\n\ncreate a \nvolumes\n definition\n\n\nattach volume to container using \nVolumeMounts\n property\n\n\n\n\nLocal host volumes are of two types:\n\n  * emptyDir\n\n  * hostPath  \n\n\nWe will pick hostPath. \nRefer to this doc to read more about hostPath.\n\n\nFile: db-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate\n\n\n\n\nTo create this pod,\n\n\nkubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events\n\n\n\n\nSelecting Node to run on\n\n\nkubectl get nodes --show-labels\n\nkubectl label nodes \nnode-name\n zone=aaa\n\nkubectl get nodes --show-labels\n\n\n\n\n\nUpdate pod definition with nodeSelector\n\n\nfile: vote-pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    zone: 'aaa'\n\n\n\n\nFor this change, pod needs to be re created.\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\nCreating Multi Container Pods\n\n\nfile: multi_container_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n    - name: sync\n      image: schoolofdevops/synch\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n  volumes:\n  - name: data\n    emptyDir: {}\n\n\n\n\nTo create this pod\n\n\nkubectl apply -f multi_container_pod.yml\n\n\n\n\nCheck Status\n\n\nroot@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m\n\n\n\n\nChecking logs, logging in\n\n\nkubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync\n\n\n\n\n\nExercise\n\n\nCreate a pod definition for redis and deploy.\n\n\nReading List\n\n\n\n\nPodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core\n\n\nManaging Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/\n\n\nNode Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Launching Pods"
        }, 
        {
            "location": "/5_deploying_pods/#deploying-pods", 
            "text": "Life of a pod   Pending : in progress  Running  Succeeded : successfully exited  Failed  Unknown", 
            "title": "Deploying Pods"
        }, 
        {
            "location": "/5_deploying_pods/#probes", 
            "text": "livenessProbe : Containers are Alive  readinessProbe : Ready to Serve Traffic", 
            "title": "Probes"
        }, 
        {
            "location": "/5_deploying_pods/#resource-configs", 
            "text": "Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.  AKMS  =  Resource Configs Specs  apiVersion: v1\nkind:\nmetadata:\nspec:  Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/  To list supported version of apis  kubectl api-versions", 
            "title": "Resource Configs"
        }, 
        {
            "location": "/5_deploying_pods/#common-configurations", 
            "text": "Throughout this tutorial, we would be deploying different components of  example voting application. Lets assume we are deploying it in a  dev  environment. Lets create the common specs for this app with the AKMS schema discussed above.  file: common.yml  apiVersion: v1\nkind:\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:  Lets now create the  Pod config by adding the kind and specs to above schema.  Filename: k8s-code/pods/vote-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest  Use this link to refer to pod spec", 
            "title": "Common Configurations"
        }, 
        {
            "location": "/5_deploying_pods/#launching-and-operating-a-pod", 
            "text": "Syntax:   kubectl apply -f FILE  To Launch pod using configs above,  kubectl apply -f vote-pod.yaml  To view pods  kubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote  To get detailed info  kubectl describe pods vote  [Output:]  Name:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:     none \nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:       none \nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:     none \nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image  schoolofdevops/vote:latest \n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image  schoolofdevops/vote:latest \n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945  Commands to operate the pod  kubectl exec -it vote ps sh\n\nkubectl exec -it vote  sh\n\nkubectl logs vote", 
            "title": "Launching and operating a Pod"
        }, 
        {
            "location": "/5_deploying_pods/#troubleshooting-tip", 
            "text": "If you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.  kubectl get pod vote -o yaml  Lets learn by example. Update pod spec and change the image to something that does not exist.  kubectl edit pod vote  This will open a editor. Go to the line which defines image  and change it to a tag that does not exist  e.g.  spec:\n  containers:\n  - image: schoolofdevops/vote:latst\n    imagePullPolicy: Always  where tag  latst  does not exist. As soon as you save this file, kubernetes will apply the change.  Now check the status,  kubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nvote      0/1       ImagePullBackOff   0          27m  The above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.  Observe the  status  field.    kubectl get pod vote -o yaml  Now the status field shows a detailed information, including what the exact error. Observe the following snippet...  status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248  This will help you to pinpoint to the exact cause and fix it quickly.  Now that you  are done experimenting with pod, delete it with the following command,  kubectl delete pod vote\n\nkubectl get pods", 
            "title": "Troubleshooting Tip"
        }, 
        {
            "location": "/5_deploying_pods/#attach-a-volume-to-the-pod", 
            "text": "Lets create a pod for database and attach a volume to it. To achieve this we will need to   create a  volumes  definition  attach volume to container using  VolumeMounts  property   Local host volumes are of two types: \n  * emptyDir \n  * hostPath    We will pick hostPath.  Refer to this doc to read more about hostPath.  File: db-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate  To create this pod,  kubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events", 
            "title": "Attach a Volume to the Pod"
        }, 
        {
            "location": "/5_deploying_pods/#selecting-node-to-run-on", 
            "text": "kubectl get nodes --show-labels\n\nkubectl label nodes  node-name  zone=aaa\n\nkubectl get nodes --show-labels  Update pod definition with nodeSelector  file: vote-pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    zone: 'aaa'  For this change, pod needs to be re created.  kubectl apply -f vote-pod.yaml", 
            "title": "Selecting Node to run on"
        }, 
        {
            "location": "/5_deploying_pods/#creating-multi-container-pods", 
            "text": "file: multi_container_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n    - name: sync\n      image: schoolofdevops/synch\n      volumeMounts:\n      - name: data\n        mountPath: /var/www/html-sample-app\n  volumes:\n  - name: data\n    emptyDir: {}  To create this pod  kubectl apply -f multi_container_pod.yml  Check Status  root@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m  Checking logs, logging in  kubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync", 
            "title": "Creating Multi Container Pods"
        }, 
        {
            "location": "/5_deploying_pods/#exercise", 
            "text": "Create a pod definition for redis and deploy.", 
            "title": "Exercise"
        }, 
        {
            "location": "/5_deploying_pods/#reading-list", 
            "text": "PodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core  Managing Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/  Node Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Reading List"
        }, 
        {
            "location": "/6_kubernetes_deployment/", 
            "text": "Creating a Deployment\n\n\nA Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.\n\n\nDeployment has mainly two responsibilities,\n\n\n\n\nProvide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.\n\n\nUpdate Strategy: Define a release strategy and update the pods accordingly.\n\n\n\n\nFile: vote-deploy.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: instavote\nspec:\n  replicas: 8\n  revisionHistoryLimit: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  minReadySeconds: 20\n  paused: false\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n\n\nDeployment spec (deployment.spec) contains the following,\n\n\n\n\nreplicaset specs\n\n\nselectors  \n\n\nreplicas  \n\n\n\n\n\n\ndeployment spec\n\n\nstrategy\n\n\nrollingUpdate\n\n\nminReadySeconds\n\n\n\n\n\n\npod template\n\n\nmetadata, labels\n\n\ncontainer specs\n\n\n\n\n\n\n\n\nLets  create the Deployment\n\n\nkubectl apply -f vote_deploy.yaml --record\n\n\n\n\nNow that the deployment is created. To validate,\n\n\nkubectl get deployment\nkubectl get rs\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels\n\n\n\n\nSample Output\n\n\nkubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m\n\n\n\n\nScaling a deployment\n\n\nTo scale a deployment in Kubernetes:\n\n\nkubectl scale deployment/vote --replicas=5\n\n\n\n\nSample output:\n\n\nkubectl scale deployment/vote --replicas=5\ndeployment \nvote\n scaled", 
            "title": "Creating Deployments"
        }, 
        {
            "location": "/6_kubernetes_deployment/#creating-a-deployment", 
            "text": "A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.  Deployment has mainly two responsibilities,   Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.  Update Strategy: Define a release strategy and update the pods accordingly.   File: vote-deploy.yaml  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: instavote\nspec:\n  replicas: 8\n  revisionHistoryLimit: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  minReadySeconds: 20\n  paused: false\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP  Deployment spec (deployment.spec) contains the following,   replicaset specs  selectors    replicas      deployment spec  strategy  rollingUpdate  minReadySeconds    pod template  metadata, labels  container specs     Lets  create the Deployment  kubectl apply -f vote_deploy.yaml --record  Now that the deployment is created. To validate,  kubectl get deployment\nkubectl get rs\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels  Sample Output  kubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m", 
            "title": "Creating a Deployment"
        }, 
        {
            "location": "/6_kubernetes_deployment/#scaling-a-deployment", 
            "text": "To scale a deployment in Kubernetes:  kubectl scale deployment/vote --replicas=5  Sample output:  kubectl scale deployment/vote --replicas=5\ndeployment  vote  scaled", 
            "title": "Scaling a deployment"
        }, 
        {
            "location": "/7_exposing_app_with_service/", 
            "text": "Exposing Application with  a Service\n\n\nTypes of Services:   \n\n\n\n\nClusterIP\n\n\nNodePort\n\n\nLoadBalancer\n\n\nExternalName\n\n\n\n\n\n\nkubectl get pods\nkubectl get svc\n\n\n\n\nSample Output:\n\n\nNAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m\n\n\n\n\nPublishing a service with NodePort\n\n\nFilename: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: vote-svc\n  namespace: instavote\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n\n\n\n\nSave the file.\n\n\nNow to create a service:\n\n\nkubectl apply -f vote-svc.yaml\nkubectl get svc\n\n\n\n\nNow to check which port the pod is connected\n\n\nkubectl describe service vote\n\n\n\n\nCheck for the Nodeport here\n\n\nSample Output\n\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nService\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\nrole\n:\nsvc\n,\ntier\n:\nfront\n},\nname\n:\nvote\n,\nnamespace\n:\ninstavote\n},\nspec\n:{...\nSelector:                 app=vote\nType:                     NodePort\nIP:                       10.108.108.157\nPort:                     \nunset\n  80/TCP\nTargetPort:               80/TCP\nNodePort:                 \nunset\n  31429/TCP\nEndpoints:                10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \nnone\n\n\n\n\n\nGo to browser and check hostip:NodePort\n\n\nHere the node port is 31429.\n\n\nSample output will be:\n\n\n\n\nExposing the app with ExternalIP\n\n\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n  externalIPs:\n    - 192.168.12.11\n    - 192.168.12.12\n\n\n\n\napply\n\n\nkubectl apply -f vote-svc.yaml\nkubectl  get svc\nkubectl describe svc vote", 
            "title": "Service Endpoints"
        }, 
        {
            "location": "/7_exposing_app_with_service/#exposing-application-with-a-service", 
            "text": "Types of Services:      ClusterIP  NodePort  LoadBalancer  ExternalName    kubectl get pods\nkubectl get svc  Sample Output:  NAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m", 
            "title": "Exposing Application with  a Service"
        }, 
        {
            "location": "/7_exposing_app_with_service/#publishing-a-service-with-nodeport", 
            "text": "Filename: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: vote-svc\n  namespace: instavote\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort  Save the file.  Now to create a service:  kubectl apply -f vote-svc.yaml\nkubectl get svc  Now to check which port the pod is connected  kubectl describe service vote  Check for the Nodeport here  Sample Output  Name:                     vote\nNamespace:                instavote\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Service , metadata :{ annotations :{}, labels :{ role : svc , tier : front }, name : vote , namespace : instavote }, spec :{...\nSelector:                 app=vote\nType:                     NodePort\nIP:                       10.108.108.157\nPort:                      unset   80/TCP\nTargetPort:               80/TCP\nNodePort:                  unset   31429/TCP\nEndpoints:                10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                    none   Go to browser and check hostip:NodePort  Here the node port is 31429.  Sample output will be:", 
            "title": "Publishing a service with NodePort"
        }, 
        {
            "location": "/7_exposing_app_with_service/#exposing-the-app-with-externalip", 
            "text": "spec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n  externalIPs:\n    - 192.168.12.11\n    - 192.168.12.12  apply  kubectl apply -f vote-svc.yaml\nkubectl  get svc\nkubectl describe svc vote", 
            "title": "Exposing the app with ExternalIP"
        }, 
        {
            "location": "/8_rollouts_and_rollbacks/", 
            "text": "Rolling updates with deployments\n\n\nUpdate the version of the image in vote_deploy.yaml\n\n\nFile: vote_deploy.yaml\n\n\n...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movies\n\n\n\n\n\nApply Changes and monitor the rollout\n\n\nkubectl apply -f vote-deploy.yaml\nkubectl rollout status deployment/vote\n\n\n\n\nRolling Back a Failed Update\n\n\nLets update the image to a tag which is non existent. We intentionally introduce this intentional error to fail fail the deployment.\n\n\nFile: vote_deploy.yaml\n\n\n...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movi\n\n\n\n\n\nDo a new rollout and monitor\n\n\nkubectl apply -f vote_deploy.yaml\nkubectl rollout status deployment/vote\n\n\n\n\nAlso watch the pod status which might look like\n\n\nvote-3040199436-sdq17   1/1       Running            0          9m\nvote-4086029260-0vjjb   0/1       ErrImagePull       0          16s\nvote-4086029260-zvgmd   0/1       ImagePullBackOff   0          15s\nvote-rc-fsdsd               1/1       Running            0          27m\nvote-rc-mcxs5               1/1       Running            0\n\n\n\n\nTo get the revision history and details  \n\n\nkubectl rollout history deployment/vote\nkubectl rollout history deployment/vote --revision=x\n[replace x with the latest revision]\n\n\n\n\n[Sample Output]\n\n\nroot@kube-01:~# kubectl rollout history deployment/vote\ndeployments \nvote\n\nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/vote --replicas=5\n3       \nnone\n\n6       \nnone\n\n7       \nnone\n\n\nroot@kube-01:~# kubectl rollout history deployment/vote --revision=7\ndeployments \nvote\n with revision #7\nPod Template:\n  Labels:   app=vote\n    env=dev\n    pod-template-hash=4086029260\n    role=ui\n    stack=voting\n    tier=front\n  Containers:\n   vote:\n    Image:  schoolofdevops/vote:movi\n    Port:   80/TCP\n    Environment:    \nnone\n\n    Mounts: \nnone\n\n  Volumes:  \nnone\n\n\n\n\n\nTo undo rollout,\n\n\nkubectl rollout undo deployment/vote\n\n\n\n\nor\n\n\nkubectl rollout undo deployment/vote --to-revision=1\nkubectl get rs\nkubectl describe deployment vote", 
            "title": "Rollouts and Rollbacks"
        }, 
        {
            "location": "/8_rollouts_and_rollbacks/#rolling-updates-with-deployments", 
            "text": "Update the version of the image in vote_deploy.yaml  File: vote_deploy.yaml  ...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movies  Apply Changes and monitor the rollout  kubectl apply -f vote-deploy.yaml\nkubectl rollout status deployment/vote", 
            "title": "Rolling updates with deployments"
        }, 
        {
            "location": "/8_rollouts_and_rollbacks/#rolling-back-a-failed-update", 
            "text": "Lets update the image to a tag which is non existent. We intentionally introduce this intentional error to fail fail the deployment.  File: vote_deploy.yaml  ...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movi  Do a new rollout and monitor  kubectl apply -f vote_deploy.yaml\nkubectl rollout status deployment/vote  Also watch the pod status which might look like  vote-3040199436-sdq17   1/1       Running            0          9m\nvote-4086029260-0vjjb   0/1       ErrImagePull       0          16s\nvote-4086029260-zvgmd   0/1       ImagePullBackOff   0          15s\nvote-rc-fsdsd               1/1       Running            0          27m\nvote-rc-mcxs5               1/1       Running            0  To get the revision history and details    kubectl rollout history deployment/vote\nkubectl rollout history deployment/vote --revision=x\n[replace x with the latest revision]  [Sample Output]  root@kube-01:~# kubectl rollout history deployment/vote\ndeployments  vote \nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/vote --replicas=5\n3        none \n6        none \n7        none \n\nroot@kube-01:~# kubectl rollout history deployment/vote --revision=7\ndeployments  vote  with revision #7\nPod Template:\n  Labels:   app=vote\n    env=dev\n    pod-template-hash=4086029260\n    role=ui\n    stack=voting\n    tier=front\n  Containers:\n   vote:\n    Image:  schoolofdevops/vote:movi\n    Port:   80/TCP\n    Environment:     none \n    Mounts:  none \n  Volumes:   none   To undo rollout,  kubectl rollout undo deployment/vote  or  kubectl rollout undo deployment/vote --to-revision=1\nkubectl get rs\nkubectl describe deployment vote", 
            "title": "Rolling Back a Failed Update"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/", 
            "text": "Configmap is one of the ways to provide configurations to your application.\n\n\nInjecting env variables with configmaps\n\n\nCreate our configmap for vote app\n\n\nfile:  projects/instavote/dev/vote-cm.yaml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: EMACS\n  OPTION_B: VI\n\n\n\n\nIn the above given configmap, we define two environment variables,\n\n\n\n\nOPTION_A=EMACS\n\n\nOPTION_B=VI\n\n\n\n\nIn order to use this configmap in the deployment, we need to reference it from the deployment file.\n\n\nCheck the deployment file for vote add for the following block.\n\n\nfile: \nvote-deploy.yaml\n\n\n...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always\n\n\n\n\nSo when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.\n\n\n\n\nConfigmap as a configuration file\n\n\nIn the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.\n\n\nSyntax for consuming file as a configmap is as follows\n\n\n  kubectl create configmap --from-file \nCONF-FILE-PATH\n \nNAME-OF-CONFIGMAP\n\n\n\n\n\nWe have redis configuration as a file named \nprojects/instavote/config/redis.conf\n. We are going to convert this file into a configmap\n\n\nkubectl create configmap --from-file projects/instavote/config/redis.conf redis\n\n\n\n\nUpdate your redis-deploy.yaml file to use this confimap.\nFile: \nredis-deploy.yaml\n\n\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always\n\n\n\n\nSecrets\n\n\nSecrets are for storing sensitive data like \npasswords and keychains\n. We will see how db deployment uses username and password in form of a secret.\n\n\nYou would define two fields for db,\n  * username\n  * password\n\n\nTo create secrets for db you need to generate  \nbase64\n format as follows,\n\n\necho \nadmin\n | base64\necho \npassword\n | base64\n\n\n\n\nwhere \nadmin\n and \npassword\n are the actual values that you would want to inject into the pod environment.\n\n\nIf you do not have a unix host, you can make use of online base64 utility to generate these strings.\n\n\nhttp://www.utilities-online.info/base64\n\n\n\n\nLets now add it to the secrets file,\n\n\nFile: projects/instavote/dev/db-secrets.yaml\n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  # base64 of admin\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n  # base64 of password\n\n\n\n\nTo consume these secrets, update the deployment as\n\n\nfile: db-deploy.yaml.\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n# Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always\n\n\n\n\nNote: Automatic Updation of deployments on ConfigMap  Updates\n\n\nCurrently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.  \n\n\nThere is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368\n\n\nCurrently, this can be done by using immutable configMaps. \n\n  * Create a configMaps and apply it with deployment\n  * To update, create a new configMaps and do not update the previous one. Treat it as immutable.\n  * Update deployment spec to use the new version of the configMaps. This will ensure immediate update.", 
            "title": "Using Configmaps and Secrets"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#injecting-env-variables-with-configmaps", 
            "text": "Create our configmap for vote app  file:  projects/instavote/dev/vote-cm.yaml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: EMACS\n  OPTION_B: VI  In the above given configmap, we define two environment variables,   OPTION_A=EMACS  OPTION_B=VI   In order to use this configmap in the deployment, we need to reference it from the deployment file.  Check the deployment file for vote add for the following block.  file:  vote-deploy.yaml  ...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always  So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.", 
            "title": "Injecting env variables with configmaps"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#configmap-as-a-configuration-file", 
            "text": "In the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.  Syntax for consuming file as a configmap is as follows    kubectl create configmap --from-file  CONF-FILE-PATH   NAME-OF-CONFIGMAP   We have redis configuration as a file named  projects/instavote/config/redis.conf . We are going to convert this file into a configmap  kubectl create configmap --from-file projects/instavote/config/redis.conf redis  Update your redis-deploy.yaml file to use this confimap.\nFile:  redis-deploy.yaml      spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always", 
            "title": "Configmap as a configuration file"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#secrets", 
            "text": "Secrets are for storing sensitive data like  passwords and keychains . We will see how db deployment uses username and password in form of a secret.  You would define two fields for db,\n  * username\n  * password  To create secrets for db you need to generate   base64  format as follows,  echo  admin  | base64\necho  password  | base64  where  admin  and  password  are the actual values that you would want to inject into the pod environment.  If you do not have a unix host, you can make use of online base64 utility to generate these strings.  http://www.utilities-online.info/base64  Lets now add it to the secrets file,  File: projects/instavote/dev/db-secrets.yaml  apiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  # base64 of admin\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n  # base64 of password  To consume these secrets, update the deployment as  file: db-deploy.yaml.  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n# Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always", 
            "title": "Secrets"
        }, 
        {
            "location": "/9_using_configmaps_and_secrets/#note-automatic-updation-of-deployments-on-configmap-updates", 
            "text": "Currently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.    There is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368  Currently, this can be done by using immutable configMaps.  \n  * Create a configMaps and apply it with deployment\n  * To update, create a new configMaps and do not update the previous one. Treat it as immutable.\n  * Update deployment spec to use the new version of the configMaps. This will ensure immediate update.", 
            "title": "Note: Automatic Updation of deployments on ConfigMap  Updates"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/", 
            "text": "Kubernetes Horizonntal Pod Autoscaling\n\n\nWith Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).\n\n\nThe Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user\n\n\nPrerequisites\n\n\nHeapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.\n\n\nDeploying Heapster\n\n\nGo to the below directory and create the deployment and services.\n\n\ngit clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml\n\n\n\n\nValidate that heapster, influxdb and grafana are started\n\n\nkubectl get pods -n kube-system\nkubectl get svc -n kube-system\n\n\n\n\n\nNow this will deploy the heapster monitoring.\n\n\nRun \n expose php-apache server\n\n\nTo demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80  \n\n\n\n\nSample Output\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice \nphp-apache\n created\ndeployment \nphp-apache\n created\n\n\n\n\nTo verify the created pod:\n\n\nkubectl get pods\n\n\n\n\nWait untill the pod changes to running state.\n\n\nCreate Horizontal Pod Autoscaler\n\n\nNow that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n\n\n\n\nSample Output\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment \nphp-apache\n autoscaled\n\n\n\n\nWe may check the current status of autoscaler by running:\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s\n\n\n\n\nIncrease load\n\n\nNow we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service\n\n\nkubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done\n\n\n\n\n\nNow open a new window of the same machine.\n\n\nAnd check the status of the hpa\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m\n\n\n\n\nNow if you check the pods it will be automatically scaled to the desired value.\n\n\nkubectl get pods\n\n\n\n\nSample Output\n\n\nkubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h\n\n\n\n\nStop load\n\n\nIn the terminal where we created the container with busybox image, terminate the load generation by typing \n + C\n\n\nThen we will verify the result state (after a minute or so)\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Auto Scaling Capacity with HPA"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#kubernetes-horizonntal-pod-autoscaling", 
            "text": "With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).  The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user", 
            "title": "Kubernetes Horizonntal Pod Autoscaling"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#prerequisites", 
            "text": "Heapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#deploying-heapster", 
            "text": "Go to the below directory and create the deployment and services.  git clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml  Validate that heapster, influxdb and grafana are started  kubectl get pods -n kube-system\nkubectl get svc -n kube-system  Now this will deploy the heapster monitoring.", 
            "title": "Deploying Heapster"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#run-expose-php-apache-server", 
            "text": "To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80    Sample Output  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice  php-apache  created\ndeployment  php-apache  created  To verify the created pod:  kubectl get pods  Wait untill the pod changes to running state.", 
            "title": "Run &amp; expose php-apache server"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#create-horizontal-pod-autoscaler", 
            "text": "Now that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  Sample Output  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment  php-apache  autoscaled  We may check the current status of autoscaler by running:  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s", 
            "title": "Create Horizontal Pod Autoscaler"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#increase-load", 
            "text": "Now we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service  kubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done  Now open a new window of the same machine.  And check the status of the hpa  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m  Now if you check the pods it will be automatically scaled to the desired value.  kubectl get pods  Sample Output  kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h", 
            "title": "Increase load"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#stop-load", 
            "text": "In the terminal where we created the container with busybox image, terminate the load generation by typing   + C  Then we will verify the result state (after a minute or so)  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Stop load"
        }, 
        {
            "location": "/11_deploying_sample_app/", 
            "text": "Mini Project: Deploying Multi Tier Application Stack\n\n\nIn this project , you would write definitions for deploying the vote application stack with all components/tiers which include,\n\n\n\n\nvote ui\n\n\nredis\n\n\nworker\n\n\ndb\n\n\nresults ui\n\n\n\n\nTasks\n\n\n\n\nCreate deployments for all applications\n\n\nDefine services for each tier\n\n\nLaunch/appy the definitions\n\n\n\n\nFollowing table depicts the state of readiness of the above services.\n\n\n\n\n\n\n\n\nApp\n\n\nDeployment\n\n\nService\n\n\n\n\n\n\n\n\n\n\nvote\n\n\nready\n\n\nready\n\n\n\n\n\n\nredis\n\n\nin progress\n\n\nready\n\n\n\n\n\n\nworker\n\n\nin progress\n\n\nin progress\n\n\n\n\n\n\ndb\n\n\nin progress\n\n\ntodo\n\n\n\n\n\n\nresults\n\n\ntodo\n\n\ntodo\n\n\n\n\n\n\n\n\nDeploying the sample application\n\n\nTo create deploy the sample applications,\n\n\nkubectl create -f projects/instavote/dev\n\n\n\n\nSample output is like:\n\n\ndeployment \ndb\n created\nservice \ndb\n created\ndeployment \nredis\n created\nservice \nredis\n created\ndeployment \nvote\n created\nservice \nvote\n created\ndeployment \nworker\n created\ndeployment \nresults\n created\nservice \nresults\n created\n\n\n\n\nTo Validate:\n\n\nkubectl get svc -n instavote\n\n\n\n\nSample Output is:\n\n\nkubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   \npending\n     80:31808/TCP   1h\n\n\n\n\nHere the port assigned is 31808, go to the browser and enter\n\n\nmasterip:31808\n\n\n\n\n\n\nThis will load the page where you can vote.\n\n\nTo check the result:\n\n\nkubectl get service result\n\n\n\n\nSample Output is:\n\n\nkubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   \npending\n     80:32511/TCP   1h\n\n\n\n\nHere the port assigned is 32511, go to the browser and enter\n\n\nmasterip:32511\n\n\n\n\n\n\nThis is the page where we can see the results of the vote.", 
            "title": "Mini Project"
        }, 
        {
            "location": "/11_deploying_sample_app/#mini-project-deploying-multi-tier-application-stack", 
            "text": "In this project , you would write definitions for deploying the vote application stack with all components/tiers which include,   vote ui  redis  worker  db  results ui", 
            "title": "Mini Project: Deploying Multi Tier Application Stack"
        }, 
        {
            "location": "/11_deploying_sample_app/#tasks", 
            "text": "Create deployments for all applications  Define services for each tier  Launch/appy the definitions   Following table depicts the state of readiness of the above services.     App  Deployment  Service      vote  ready  ready    redis  in progress  ready    worker  in progress  in progress    db  in progress  todo    results  todo  todo", 
            "title": "Tasks"
        }, 
        {
            "location": "/11_deploying_sample_app/#deploying-the-sample-application", 
            "text": "To create deploy the sample applications,  kubectl create -f projects/instavote/dev  Sample output is like:  deployment  db  created\nservice  db  created\ndeployment  redis  created\nservice  redis  created\ndeployment  vote  created\nservice  vote  created\ndeployment  worker  created\ndeployment  results  created\nservice  results  created", 
            "title": "Deploying the sample application"
        }, 
        {
            "location": "/11_deploying_sample_app/#to-validate", 
            "text": "kubectl get svc -n instavote  Sample Output is:  kubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243    pending      80:31808/TCP   1h  Here the port assigned is 31808, go to the browser and enter  masterip:31808   This will load the page where you can vote.  To check the result:  kubectl get service result  Sample Output is:  kubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16    pending      80:32511/TCP   1h  Here the port assigned is 32511, go to the browser and enter  masterip:32511   This is the page where we can see the results of the vote.", 
            "title": "To Validate:"
        }, 
        {
            "location": "/cluster_setup_kubespray/", 
            "text": "Kubernetes - Cluster Setup using Kubespray\n\n\nKubespray is an \nAnsible\n based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.\n\n\nPrerequisites\n\n\nHardware Pre requisites\n  * 4 Nodes: Virtual/Physical Machines\n  * Memory: 2GB each\n  * CPU: 2 cores recommended\n  * Hard disk: 20GB available\n\n\nSoftware Pre Requisites\n  * Ubuntu 16.04 Operating System\n  * Python\n\n\nArchitecture of a high available kubernetes cluster\n\n\nPreparing the kubernetes nodes\n\n\nOn control node\n\n\nAnsible is the base provisioner in our cluster. But installing Ansible is out of the scope of this training. You can learn about installing and configuring Ansible from \nhere.\n\n\nSetup passwordless SSH between control and kubernetes nodes\n\n\nOn control node\n\n\nAnsible uses passwordless ssh\n1\n to create the cluster. Let us see how to set it up from your \ncontrol node\n.\n\n\nssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   E    ..       |\n|  . o +..        |\n| . +o*+o         |\n|. .o+Bo+         |\n|. .++.X S        |\n|+ +ooX .         |\n|.=.OB.+ .        |\n| .=o*= . .       |\n|  .o.   .        |\n+----[SHA256]-----+\n\n\n\n\nJust leave the fields to defaults. This command will generate a public key and private key for you.\n\n\ncat ~/.ssh/id_rsa.pub | ssh ubuntu@10.40.1.26 'cat \n ~/.ssh/authorized_keys'\n\n\n\n\nThis will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace \n10.40.1.26\n with your respective machine's IP.\n\n\nEnable IPv4 Forwarding\n\n\nOn all nodes\n\n\nssh ubuntu@10.40.1.26\nsudo su\nvim /etc/sysctl.conf\n\n\n\n\nEnalbe IPv4 forwarding by uncommenting the following line\n\n\nnet.ipv4.ip_forward=1\n\n\n\n\nStop and Disable UFW Service\n\n\nOn all nodes\n\n\nExecute the following commands to stop and disable ufw service.\n\n\nsystemctl stop ufw.service\nsystemctl disable ufw.service\n\n\n\n\nInstall Python\n\n\nOn all nodes\n\n\nAnsible needs python to be installed on all the machines.\n\n\nsudo apt update\nsudo apt install python\n\n\n\n\nConfiguring Ansible Control node and Kubespray\n\n\nOn control node\n\n\nKubespray is hosted on GitHub. Let us the clone the \nofficial repository\n.\n\n\ngit clone https://github.com/kubernetes-incubator/kubespray.git\ncd kubespray\n\n\n\n\nSet Remote User for Ansible\n\n\nOn control node\n\n\nAdd the following section in ansible.cfg file\n\n\nremote_user=ubuntu\n\n\n\n\nYour \nansible.cfg\n file should look like this.\n\n\n[ssh_connection]\npipelining=True\nssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\n#control_path = ~/.ssh/ansible-%%r@%%h:%%p\n[defaults]\nhost_key_checking=False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp\nstdout_callback = skippy\nlibrary = ./library\ncallback_whitelist = profile_tasks\nroles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles\ndeprecation_warnings=False\nremote_user=ubuntu\n\n\n\n\nDownload Inventory Builder\n\n\nOn control node\n\n\nInventory builder (a python script) helps us to create inventory file. \nInventory\n file is something with which we specify the groups of masters and nodes of our cluster.\n\n\ncd inventory\nwget https://raw.githubusercontent.com/kubernetes-incubator/kubespray/master/contrib/inventory_builder/inventory.py\n\n\n\n\nTo build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments\n\n\npython inventory.py 10.40.1.26 10.40.1.25 10.40.1.20 10.40.1.11\n\n\n\n\nThese IPs are different for you. Please replace them with your corresponding IPs.\nThis step will result in the creation of a new file \ninventory.cfg\n. This is our inventory file.\n\n\ninventory.cfg\n\n\n[all]\nnode1    ansible_host=10.40.1.26 ip=10.40.1.26\nnode2    ansible_host=10.40.1.25 ip=10.40.1.25\nnode3    ansible_host=10.40.1.20 ip=10.40.1.20\nnode4    ansible_host=10.40.1.11 ip=10.40.1.11\n\n[kube-master]\nnode1\nnode2\n\n[kube-node]\nnode1\nnode2\nnode3\nnode4\n\n[etcd]\nnode1\nnode2\nnode3\n\n[k8s-cluster:children]\nkube-node\nkube-master\n\n[calico-rr]\n\n[vault]\nnode1\nnode2\nnode3\n\n\n\n\nProvisioning  kubernetes cluster with kubespray\n\n\nOn control node\n\n\nWe are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster.\n\n\nansible-playbook -i inventory/inventory.cfg cluster.yml -b -v\n\n\n\n\nOption -i = Inventory file path\nOption -b = Become as root user\nOption -v = Give verbose output\n\n\nThis Ansible run will take around 30 mins to complete.\n\n\nInstall Kubectl\n\n\nOn control node\n\n\nBefore we proceed further, we will need to install \nkubectl\n binary in our control node. Read installation procedure from this \nlink\n.\n\n\nGetting the Kubernetes Configuration File\n\n\nOn control node\n\n\nOnce the cluster setup is done, we have to copy over the cluster config file from the master machine. We will discuss about this file extensively in the next chapter.\n\n\nssh ubuntu@10.40.1.26\nsudo su\ncp /etc/kubernetes/admin.conf /home/ubuntu\nchown ubuntu:ubuntu /home/ubuntu/admin.conf\nexit\nexit\nscp ubuntu@10.40.1.26:~/admin.conf .\ncd\nmkdir .kube\nmv admin.conf .kube/config\n\n\n\n\nCheck the State of the Cluster\n\n\nOn control node\n\n\nLet us check the state of the cluster by running,\n\n\nkubectl cluster-info\n\nKubernetes master is running at https://10.40.1.26:6443\nKubeDNS is running at https://10.40.1.26:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n\n\n\nkubectl get nodes\n\nNAME      STATUS    ROLES         AGE       VERSION\nnode1     Ready     master,node   21h       v1.9.0+coreos.0\nnode2     Ready     master,node   21h       v1.9.0+coreos.0\nnode3     Ready     node          21h       v1.9.0+coreos.0\nnode4     Ready     node          21h       v1.9.0+coreos.0\n\n\n\n\nIf you are able to see this, your cluster has been set up successfully.\n\n\n\n\n1\n You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.", 
            "title": "Production grade setup with Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#kubernetes-cluster-setup-using-kubespray", 
            "text": "Kubespray is an  Ansible  based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.", 
            "title": "Kubernetes - Cluster Setup using Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#prerequisites", 
            "text": "Hardware Pre requisites\n  * 4 Nodes: Virtual/Physical Machines\n  * Memory: 2GB each\n  * CPU: 2 cores recommended\n  * Hard disk: 20GB available  Software Pre Requisites\n  * Ubuntu 16.04 Operating System\n  * Python", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#architecture-of-a-high-available-kubernetes-cluster", 
            "text": "", 
            "title": "Architecture of a high available kubernetes cluster"
        }, 
        {
            "location": "/cluster_setup_kubespray/#preparing-the-kubernetes-nodes", 
            "text": "On control node  Ansible is the base provisioner in our cluster. But installing Ansible is out of the scope of this training. You can learn about installing and configuring Ansible from  here.", 
            "title": "Preparing the kubernetes nodes"
        }, 
        {
            "location": "/cluster_setup_kubespray/#setup-passwordless-ssh-between-control-and-kubernetes-nodes", 
            "text": "On control node  Ansible uses passwordless ssh 1  to create the cluster. Let us see how to set it up from your  control node .  ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   E    ..       |\n|  . o +..        |\n| . +o*+o         |\n|. .o+Bo+         |\n|. .++.X S        |\n|+ +ooX .         |\n|.=.OB.+ .        |\n| .=o*= . .       |\n|  .o.   .        |\n+----[SHA256]-----+  Just leave the fields to defaults. This command will generate a public key and private key for you.  cat ~/.ssh/id_rsa.pub | ssh ubuntu@10.40.1.26 'cat   ~/.ssh/authorized_keys'  This will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace  10.40.1.26  with your respective machine's IP.", 
            "title": "Setup passwordless SSH between control and kubernetes nodes"
        }, 
        {
            "location": "/cluster_setup_kubespray/#enable-ipv4-forwarding", 
            "text": "On all nodes  ssh ubuntu@10.40.1.26\nsudo su\nvim /etc/sysctl.conf  Enalbe IPv4 forwarding by uncommenting the following line  net.ipv4.ip_forward=1", 
            "title": "Enable IPv4 Forwarding"
        }, 
        {
            "location": "/cluster_setup_kubespray/#stop-and-disable-ufw-service", 
            "text": "On all nodes  Execute the following commands to stop and disable ufw service.  systemctl stop ufw.service\nsystemctl disable ufw.service", 
            "title": "Stop and Disable UFW Service"
        }, 
        {
            "location": "/cluster_setup_kubespray/#install-python", 
            "text": "On all nodes  Ansible needs python to be installed on all the machines.  sudo apt update\nsudo apt install python", 
            "title": "Install Python"
        }, 
        {
            "location": "/cluster_setup_kubespray/#configuring-ansible-control-node-and-kubespray", 
            "text": "On control node  Kubespray is hosted on GitHub. Let us the clone the  official repository .  git clone https://github.com/kubernetes-incubator/kubespray.git\ncd kubespray", 
            "title": "Configuring Ansible Control node and Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#set-remote-user-for-ansible", 
            "text": "On control node  Add the following section in ansible.cfg file  remote_user=ubuntu  Your  ansible.cfg  file should look like this.  [ssh_connection]\npipelining=True\nssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\n#control_path = ~/.ssh/ansible-%%r@%%h:%%p\n[defaults]\nhost_key_checking=False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp\nstdout_callback = skippy\nlibrary = ./library\ncallback_whitelist = profile_tasks\nroles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles\ndeprecation_warnings=False\nremote_user=ubuntu", 
            "title": "Set Remote User for Ansible"
        }, 
        {
            "location": "/cluster_setup_kubespray/#download-inventory-builder", 
            "text": "On control node  Inventory builder (a python script) helps us to create inventory file.  Inventory  file is something with which we specify the groups of masters and nodes of our cluster.  cd inventory\nwget https://raw.githubusercontent.com/kubernetes-incubator/kubespray/master/contrib/inventory_builder/inventory.py  To build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments  python inventory.py 10.40.1.26 10.40.1.25 10.40.1.20 10.40.1.11  These IPs are different for you. Please replace them with your corresponding IPs.\nThis step will result in the creation of a new file  inventory.cfg . This is our inventory file.  inventory.cfg  [all]\nnode1    ansible_host=10.40.1.26 ip=10.40.1.26\nnode2    ansible_host=10.40.1.25 ip=10.40.1.25\nnode3    ansible_host=10.40.1.20 ip=10.40.1.20\nnode4    ansible_host=10.40.1.11 ip=10.40.1.11\n\n[kube-master]\nnode1\nnode2\n\n[kube-node]\nnode1\nnode2\nnode3\nnode4\n\n[etcd]\nnode1\nnode2\nnode3\n\n[k8s-cluster:children]\nkube-node\nkube-master\n\n[calico-rr]\n\n[vault]\nnode1\nnode2\nnode3", 
            "title": "Download Inventory Builder"
        }, 
        {
            "location": "/cluster_setup_kubespray/#provisioning-kubernetes-cluster-with-kubespray", 
            "text": "On control node  We are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster.  ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v  Option -i = Inventory file path\nOption -b = Become as root user\nOption -v = Give verbose output  This Ansible run will take around 30 mins to complete.", 
            "title": "Provisioning  kubernetes cluster with kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#install-kubectl", 
            "text": "On control node  Before we proceed further, we will need to install  kubectl  binary in our control node. Read installation procedure from this  link .", 
            "title": "Install Kubectl"
        }, 
        {
            "location": "/cluster_setup_kubespray/#getting-the-kubernetes-configuration-file", 
            "text": "On control node  Once the cluster setup is done, we have to copy over the cluster config file from the master machine. We will discuss about this file extensively in the next chapter.  ssh ubuntu@10.40.1.26\nsudo su\ncp /etc/kubernetes/admin.conf /home/ubuntu\nchown ubuntu:ubuntu /home/ubuntu/admin.conf\nexit\nexit\nscp ubuntu@10.40.1.26:~/admin.conf .\ncd\nmkdir .kube\nmv admin.conf .kube/config", 
            "title": "Getting the Kubernetes Configuration File"
        }, 
        {
            "location": "/cluster_setup_kubespray/#check-the-state-of-the-cluster", 
            "text": "On control node  Let us check the state of the cluster by running,  kubectl cluster-info\n\nKubernetes master is running at https://10.40.1.26:6443\nKubeDNS is running at https://10.40.1.26:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  kubectl get nodes\n\nNAME      STATUS    ROLES         AGE       VERSION\nnode1     Ready     master,node   21h       v1.9.0+coreos.0\nnode2     Ready     master,node   21h       v1.9.0+coreos.0\nnode3     Ready     node          21h       v1.9.0+coreos.0\nnode4     Ready     node          21h       v1.9.0+coreos.0  If you are able to see this, your cluster has been set up successfully.   1  You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.", 
            "title": "Check the State of the Cluster"
        }, 
        {
            "location": "/12_troubleshooting/", 
            "text": "Troubleshooting the Kubernetes cluster\n\n\nIn this chapter we will learn about how to trouble shoot our Kubernetes cluster at \ncontrol plane\n level and at \napplication level\n.\n\n\nTroubleshooting the control plane\n\n\nListing the nodes in a cluster\n\n\nFirst thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.\n\n\nkubectl get nodes\n\n\n\n\nMake sure that all nodes are in \nReady\n state.\n\n\nList the control plane pods\n\n\nIf your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,\n\n\nkubectl get pods -n kube-system\n\n\n\n\nIf any of the pod is \nrestarting or crashing\n, look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster \nkube-dns\n is crashing. In order to fix this first check the deployment for errors.\n\n\nkubectl describe deployment -n kube-system kube-dns\n\n\n\n\nLog files\n\n\nMaster\n\n\nIf your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...\n\n\n/var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs\n\n\n\n\nIf your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the \nactual pod's name may differ\n from cluster to cluster...\n\n\nkubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1\n\n\n\n\nWorker Nodes\n\n\nIn your worker, you will need to check for errors in kubelet's log...\n\n\nsudo journalctl -u  kubelet\n\n\n\n\nTroubleshooting the application\n\n\nSometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.\n\n\nGetting detailed status of an object (pods, deployments)\n\n\nobject.status\n shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.\n\n\nExample\n\n\nkubectl get pod vote -o yaml\n\n\n\n\n\nexample output snippet when a wrong image was used to create a pod. \n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nChecking the status of Deployment\n\n\nFor this example I have a sample deployment called nginx.\n\n\nFILE: nginx-deployment.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80\n\n\n\n\nList the deployment to check for the \navailability of pods\n\n\nkubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h\n\n\n\n\nIt is clear that my pod is unavailable. Lets dig further.\n\n\nCheck the \nevents\n of your deployment.\n\n\nkubectl describe deployment nginx\n\n\n\n\nList the pods to check for any \nregistry related error\n\n\nkubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m\n\n\n\n\nAs we can see, we are not able to pull the image(\nImagePullBackOff\n). Let's investigate further.\n\n\nkubectl describe pod nginx-57c88d7bb8-c6kpc\n\n\n\n\nCheck the events of the pod's description.\n\n\nEvents:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \ndefault-token-8cwn4\n\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \nngnix\n\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \nngnix\n: rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \nngnix\n\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod\n\n\n\n\nBingo! The name of the image is \nngnix\n instead of \nnginx\n. So \nfix the typo in your deployment file and redo the deployment\n.\n\n\nSometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.\n\n\nkubectl logs -f nginx-57c88d7bb8-c6kpc\n\n\n\n\nIf you have any errors it will get populated in your logs.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-kubernetes-cluster", 
            "text": "In this chapter we will learn about how to trouble shoot our Kubernetes cluster at  control plane  level and at  application level .", 
            "title": "Troubleshooting the Kubernetes cluster"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-control-plane", 
            "text": "", 
            "title": "Troubleshooting the control plane"
        }, 
        {
            "location": "/12_troubleshooting/#listing-the-nodes-in-a-cluster", 
            "text": "First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.  kubectl get nodes  Make sure that all nodes are in  Ready  state.", 
            "title": "Listing the nodes in a cluster"
        }, 
        {
            "location": "/12_troubleshooting/#list-the-control-plane-pods", 
            "text": "If your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,  kubectl get pods -n kube-system  If any of the pod is  restarting or crashing , look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster  kube-dns  is crashing. In order to fix this first check the deployment for errors.  kubectl describe deployment -n kube-system kube-dns", 
            "title": "List the control plane pods"
        }, 
        {
            "location": "/12_troubleshooting/#log-files", 
            "text": "", 
            "title": "Log files"
        }, 
        {
            "location": "/12_troubleshooting/#master", 
            "text": "If your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...  /var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs  If your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the  actual pod's name may differ  from cluster to cluster...  kubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1", 
            "title": "Master"
        }, 
        {
            "location": "/12_troubleshooting/#worker-nodes", 
            "text": "In your worker, you will need to check for errors in kubelet's log...  sudo journalctl -u  kubelet", 
            "title": "Worker Nodes"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-application", 
            "text": "Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.", 
            "title": "Troubleshooting the application"
        }, 
        {
            "location": "/12_troubleshooting/#getting-detailed-status-of-an-object-pods-deployments", 
            "text": "object.status  shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.  Example  kubectl get pod vote -o yaml  example output snippet when a wrong image was used to create a pod.   status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248", 
            "title": "Getting detailed status of an object (pods, deployments)"
        }, 
        {
            "location": "/12_troubleshooting/#checking-the-status-of-deployment", 
            "text": "For this example I have a sample deployment called nginx.  FILE: nginx-deployment.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80  List the deployment to check for the  availability of pods  kubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h  It is clear that my pod is unavailable. Lets dig further.  Check the  events  of your deployment.  kubectl describe deployment nginx  List the pods to check for any  registry related error  kubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m  As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further.  kubectl describe pod nginx-57c88d7bb8-c6kpc  Check the events of the pod's description.  Events:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume  default-token-8cwn4 \n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image  ngnix \n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image  ngnix : rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image  ngnix \n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod  Bingo! The name of the image is  ngnix  instead of  nginx . So  fix the typo in your deployment file and redo the deployment .  Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.  kubectl logs -f nginx-57c88d7bb8-c6kpc  If you have any errors it will get populated in your logs.", 
            "title": "Checking the status of Deployment"
        }
    ]
}